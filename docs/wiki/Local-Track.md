# Local Track: Feast + MLFlow

The Local Track provides a complete ML pipeline execution environment using **Feast** for feature store management and **MLFlow** for experiment tracking.

## Overview

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   FEAST     │    │   MLFlow    │    │  Pipeline   │
│ Feature     │    │ Experiment  │    │  Execution  │
│   Store     │    │  Tracking   │    │  (pandas)   │
└─────────────┘    └─────────────┘    └─────────────┘
       │                  │                  │
       ▼                  ▼                  ▼
• Feature views    • Metrics logged   • Bronze → Silver → Gold
• Online serving   • Model artifacts  • Parallel bronze execution
• Point-in-time    • Experiment runs  • Local parquet output
  joins            • Model registry
```

## Quick Start: Generate a Pipeline

```python
from customer_retention.generators.pipeline_generator import PipelineGenerator

# After running exploration notebooks and saving findings...
generator = PipelineGenerator(
    findings_dir="./experiments/findings",     # Where your findings are saved
    output_dir="./orchestration",              # Where to generate pipeline
    pipeline_name="churn_prediction"           # Name of your pipeline
)

# Generate all pipeline files
generated_files = generator.generate()

print(f"Generated {len(generated_files)} files:")
for f in generated_files:
    print(f"  - {f}")
```

**Generated output:**
```
orchestration/churn_prediction/
├── config.py                    # Pipeline configuration
├── bronze/
│   ├── bronze_customers.py      # Customer data cleaning
│   └── bronze_orders.py         # Orders data cleaning
├── silver/
│   └── silver_merge.py          # Join customers + orders
├── gold/
│   └── gold_features.py         # Feature engineering
├── training/
│   └── ml_experiment.py         # Model training with MLFlow
├── pipeline_runner.py           # Run the complete pipeline locally
└── workflow.json                # (optional) Databricks workflow
```

## MLFlow Integration

### Setting Up MLFlow

```bash
# Install MLFlow
pip install mlflow

# Start local tracking server (optional)
mlflow ui --port 5000
```

### Using the MLFlow Adapter

```python
from customer_retention.integrations.adapters.mlflow import get_mlflow, ExperimentTracker

# Auto-detect environment
mlflow_client = get_mlflow(tracking_uri="./experiments/mlruns")

# High-level experiment tracker
tracker = ExperimentTracker(mlflow_client)

# Log exploration findings
from customer_retention.analysis.auto_explorer import ExplorationFindings
findings = ExplorationFindings.load("./experiments/findings/customers_findings.yaml")
tracker.log_exploration(findings, experiment_name="churn_exploration")

# Log pipeline execution
tracker.log_pipeline_execution(
    pipeline_name="churn_prediction",
    stage="gold",
    metrics={"rows_processed": 10000, "features_created": 25}
)

# Search for best runs
best_runs = tracker.get_best_run(
    experiment_name="churn_prediction",
    metric="auc",
    ascending=False
)
```

### Generated Training Code

The `training/ml_experiment.py` generated by the pipeline includes MLFlow integration:

```python
# Generated code in training/ml_experiment.py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, precision_score, recall_score
from config import get_gold_path, TARGET_COLUMN

def load_gold() -> pd.DataFrame:
    return pd.read_parquet(get_gold_path())

def run_experiment():
    gold = load_gold()
    X = gold.drop(columns=[TARGET_COLUMN])
    y = gold[TARGET_COLUMN]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # Metrics logged automatically when run through pipeline_runner.py
    print(f"AUC: {roc_auc_score(y_test, y_proba):.4f}")
    print(f"Precision: {precision_score(y_test, y_pred):.4f}")
    print(f"Recall: {recall_score(y_test, y_pred):.4f}")

    return model

if __name__ == "__main__":
    run_experiment()
```

## Running the Pipeline

### Command Line

```bash
# Navigate to generated pipeline
cd orchestration/churn_prediction

# Run the complete pipeline
python pipeline_runner.py
```

### Programmatically

```python
# Import and run from the generated pipeline
import sys
sys.path.insert(0, "orchestration/churn_prediction")

from pipeline_runner import run_pipeline
run_pipeline()
```

**Output:**
```
Starting pipeline: churn_prediction
Bronze complete
Silver complete
Gold complete
Training complete
AUC: 0.8234
Precision: 0.7456
Recall: 0.6892
```

## Complete End-to-End Example

```python
# ============================================
# STEP 1: Explore your data (Notebooks 01-04)
# ============================================
from customer_retention.analysis.auto_explorer import DataExplorer

# Explore each dataset
explorer = DataExplorer(save_findings=True, output_dir="./experiments/findings")

# Explore customers (entity-level)
customers_findings = explorer.explore("data/customers.csv")
print(f"Customers: {customers_findings.row_count} rows, target={customers_findings.target_column}")

# Explore orders (event-level)
orders_findings = explorer.explore("data/orders.parquet")
print(f"Orders: {orders_findings.row_count} rows, is_event_level={orders_findings.is_time_series}")

# ============================================
# STEP 2: Combine datasets (Notebook 05)
# ============================================
from customer_retention.analysis.auto_explorer import ExplorationManager

manager = ExplorationManager(explorations_dir="./experiments/findings")

# Discover all explored datasets
datasets = manager.list_datasets()
print(f"Found datasets: {datasets}")

# Create multi-dataset findings
multi = manager.create_multi_dataset_findings()

# Define relationships
multi.add_relationship(
    left_dataset="customers",
    right_dataset="orders",
    left_column="customer_id",
    right_column="customer_id",
    relationship_type="one_to_many"
)

# Save multi-dataset findings
multi.save("./experiments/findings/multi_dataset_findings.yaml")

# ============================================
# STEP 3: Generate pipeline
# ============================================
from customer_retention.generators.pipeline_generator import PipelineGenerator

generator = PipelineGenerator(
    findings_dir="./experiments/findings",
    output_dir="./orchestration",
    pipeline_name="churn_prediction"
)

files = generator.generate()
print(f"Generated {len(files)} pipeline files")

# ============================================
# STEP 4: Run pipeline locally
# ============================================
import sys
sys.path.insert(0, "./orchestration/churn_prediction")
from pipeline_runner import run_pipeline

run_pipeline()

# ============================================
# STEP 5: Register features with Feast
# ============================================
from customer_retention.integrations.adapters.feature_store import get_feature_store, FeatureViewConfig
import pandas as pd

feature_store = get_feature_store(repo_path="./experiments/feature_store/feature_repo")

gold_df = pd.read_parquet("./orchestration/churn_prediction/data/gold/features.parquet")

config = FeatureViewConfig(
    name="churn_features",
    entity_key="customer_id",
    features=[c for c in gold_df.columns if c != "churn"],
    ttl_days=30
)

feature_store.register_feature_view(config, gold_df)
print("Features registered with Feast!")

# ============================================
# STEP 6: Track experiment with MLFlow
# ============================================
from customer_retention.integrations.adapters.mlflow import ExperimentTracker, get_mlflow

mlflow_client = get_mlflow(tracking_uri="./experiments/mlruns")
tracker = ExperimentTracker(mlflow_client)

# Log the exploration
tracker.log_exploration(customers_findings, experiment_name="churn_analysis")

print("Experiment tracked with MLFlow!")
print("View at: http://localhost:5000 (run 'mlflow ui' first)")
```

## Best Practices

| Practice | Description |
|----------|-------------|
| **Version findings** | Commit `*_findings.yaml` files to git for reproducibility |
| **Review generated code** | Check generated transformations before running |
| **Test locally first** | Run `pipeline_runner.py` locally before deployment |
| **Use feature store** | Register gold features in Feast for reuse across models |
| **Track experiments** | Use MLFlow to compare model versions and hyperparameters |
| **Parameterize paths** | Update `config.py` paths for your environment |

## Next Steps

- [[Databricks Track]] - Deploy to Databricks
- [[Feature Store]] - Deep dive into feature management
- [[Tutorial: Bank Customer Churn]] - Hands-on example
