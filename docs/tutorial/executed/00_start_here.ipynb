{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.002063,
     "end_time": "2026-01-29T23:46:03.645447",
     "exception": false,
     "start_time": "2026-01-29T23:46:03.643384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Here: Prerequisites\n",
    "\n",
    "> **No sample data required!** This framework works directly with your own CSV, Parquet, or Delta files. The datasets below are internal examples for learning - skip to **01_data_discovery.ipynb** if you have your own data.\n",
    "\n",
    "**Purpose:** Set up your environment and optionally download sample datasets for learning.\n",
    "\n",
    "**What you'll do:**\n",
    "- Verify your Python environment\n",
    "- (Optional) Set up Kaggle API credentials\n",
    "- (Optional) Download sample churn datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {
    "papermill": {
     "duration": 0.00121,
     "end_time": "2026-01-29T23:46:03.648388",
     "exception": false,
     "start_time": "2026-01-29T23:46:03.647178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.1 Verify Environment\n",
    "\n",
    "First, let's make sure the customer_retention package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:46:03.651581Z",
     "iopub.status.busy": "2026-01-29T23:46:03.651454Z",
     "iopub.status.idle": "2026-01-29T23:46:04.878306Z",
     "shell.execute_reply": "2026-01-29T23:46:04.877757Z"
    },
    "papermill": {
     "duration": 1.229729,
     "end_time": "2026-01-29T23:46:04.879262",
     "exception": false,
     "start_time": "2026-01-29T23:46:03.649533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_retention is installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import customer_retention\n",
    "    from customer_retention.core.config.experiments import FINDINGS_DIR, EXPERIMENTS_DIR, OUTPUT_DIR, setup_experiments_structure\n",
    "    print(f\"customer_retention is installed\")\n",
    "except ImportError:\n",
    "    print(\"customer_retention not found. Install with:\")\n",
    "    print(\"  uv sync\")\n",
    "    print(\"  # or: pip install -e .\")\n",
    "from customer_retention.stages.temporal import TEMPORAL_METADATA_COLS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {
    "papermill": {
     "duration": 0.001378,
     "end_time": "2026-01-29T23:46:04.882062",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.880684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.2 Available Datasets\n",
    "\n",
    "This framework includes several internal datasets for testing and learning. **You do not need any of these to use the framework with your own data.**\n",
    "\n",
    "### Entity-Level Datasets (one row per customer)\n",
    "Use these with the standard exploration flow (notebooks 02, 03, 04).\n",
    "\n",
    "| Dataset | Status | Description |\n",
    "|---------|--------|-------------|\n",
    "| `customer_retention_retail.csv` | Included | Retail customer retention (~31K rows) |\n",
    "| `bank_customer_churn.csv` | Download | Bank customer churn (~10K rows) |\n",
    "| `netflix_customer_churn.csv` | Download | Netflix subscription churn (~10K rows) |\n",
    "\n",
    "### Event-Level Datasets (multiple rows per customer)\n",
    "Use these with the Event Bronze Track (notebooks 01a, 01b, 01c, 01d).\n",
    "\n",
    "| Dataset | Status | Description |\n",
    "|---------|--------|-------------|\n",
    "| `customer_transactions.csv` | Included | Transaction events (~5K rows) |\n",
    "| `customer_emails.csv` | Included | Email engagement events (large) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:46:04.885378Z",
     "iopub.status.busy": "2026-01-29T23:46:04.885203Z",
     "iopub.status.idle": "2026-01-29T23:46:04.889175Z",
     "shell.execute_reply": "2026-01-29T23:46:04.888452Z"
    },
    "papermill": {
     "duration": 0.006649,
     "end_time": "2026-01-29T23:46:04.890103",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.883454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-Level Datasets:\n",
      "--------------------------------------------------\n",
      "  [x] customer_retention_retail.csv (2.4 MB)\n",
      "  [x] bank_customer_churn.csv (0.6 MB)\n",
      "  [x] netflix_customer_churn.csv (0.5 MB)\n",
      "\n",
      "Event-Level Datasets:\n",
      "--------------------------------------------------\n",
      "  [x] customer_transactions.csv (4.8 MB)\n",
      "  [x] customer_emails.csv (5.6 MB)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "FIXTURES_DIR = Path(\"../tests/fixtures\")\n",
    "\n",
    "# Entity-level datasets\n",
    "entity_datasets = {\n",
    "    \"customer_retention_retail.csv\": \"Included\",\n",
    "    \"bank_customer_churn.csv\": \"Download from Kaggle\",\n",
    "    \"netflix_customer_churn.csv\": \"Download from Kaggle\",\n",
    "}\n",
    "\n",
    "# Event-level datasets (internal)\n",
    "event_datasets = {\n",
    "    \"customer_transactions.csv\": \"Included\",\n",
    "    \"customer_emails.csv\": \"Included\",\n",
    "}\n",
    "\n",
    "print(\"Entity-Level Datasets:\")\n",
    "print(\"-\" * 50)\n",
    "for name, source in entity_datasets.items():\n",
    "    path = FIXTURES_DIR / name\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  [x] {name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  [ ] {name} - {source}\")\n",
    "\n",
    "print(\"\\nEvent-Level Datasets:\")\n",
    "print(\"-\" * 50)\n",
    "for name, source in event_datasets.items():\n",
    "    path = FIXTURES_DIR / name\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  [x] {name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  [ ] {name} - {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {
    "papermill": {
     "duration": 0.001013,
     "end_time": "2026-01-29T23:46:04.892403",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.891390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.3 Kaggle API Setup\n",
    "\n",
    "To download datasets from Kaggle, you need to set up API credentials:\n",
    "\n",
    "1. Create a Kaggle account at https://www.kaggle.com\n",
    "2. Go to **Account Settings** → **API** → **Create New Token**\n",
    "3. This downloads `kaggle.json` - move it to `~/.kaggle/kaggle.json`\n",
    "4. Set permissions: `chmod 600 ~/.kaggle/kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:46:04.895825Z",
     "iopub.status.busy": "2026-01-29T23:46:04.895706Z",
     "iopub.status.idle": "2026-01-29T23:46:04.898300Z",
     "shell.execute_reply": "2026-01-29T23:46:04.897866Z"
    },
    "papermill": {
     "duration": 0.005276,
     "end_time": "2026-01-29T23:46:04.899020",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.893744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials not found.\n",
      "\n",
      "To set up:\n",
      "1. Go to https://www.kaggle.com/settings\n",
      "2. Scroll to 'API' section and click 'Create New Token'\n",
      "3. Move downloaded file to /Users/Vital/.kaggle/kaggle.json\n",
      "4. Run: chmod 600 /Users/Vital/.kaggle/kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Check if Kaggle credentials exist\n",
    "kaggle_config = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "\n",
    "if kaggle_config.exists():\n",
    "    print(f\"Kaggle credentials found at {kaggle_config}\")\n",
    "else:\n",
    "    print(\"Kaggle credentials not found.\")\n",
    "    print(\"\\nTo set up:\")\n",
    "    print(\"1. Go to https://www.kaggle.com/settings\")\n",
    "    print(\"2. Scroll to 'API' section and click 'Create New Token'\")\n",
    "    print(f\"3. Move downloaded file to {kaggle_config}\")\n",
    "    print(f\"4. Run: chmod 600 {kaggle_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {
    "papermill": {
     "duration": 0.001065,
     "end_time": "2026-01-29T23:46:04.901424",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.900359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.4 Download Kaggle Datasets\n",
    "\n",
    "Run the cells below to download each dataset. You only need to do this once.\n",
    "\n",
    "### Bank Customer Churn Dataset\n",
    "Source: https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:46:04.904564Z",
     "iopub.status.busy": "2026-01-29T23:46:04.904459Z",
     "iopub.status.idle": "2026-01-29T23:46:04.907767Z",
     "shell.execute_reply": "2026-01-29T23:46:04.907224Z"
    },
    "papermill": {
     "duration": 0.005723,
     "end_time": "2026-01-29T23:46:04.908244",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.902521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: ../tests/fixtures/bank_customer_churn.csv\n"
     ]
    }
   ],
   "source": [
    "# Download Bank Customer Churn dataset\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "FIXTURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "bank_churn_path = FIXTURES_DIR / \"bank_customer_churn.csv\"\n",
    "\n",
    "if bank_churn_path.exists():\n",
    "    print(f\"Already exists: {bank_churn_path}\")\n",
    "else:\n",
    "    print(\"Downloading Bank Customer Churn dataset...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\", \"-d\", \"gauravtopre/bank-customer-churn-dataset\",\n",
    "            \"-p\", str(FIXTURES_DIR), \"--unzip\"\n",
    "        ], check=True)\n",
    "        # Rename to consistent name\n",
    "        downloaded = FIXTURES_DIR / \"Bank_Churn.csv\"\n",
    "        if downloaded.exists():\n",
    "            shutil.move(downloaded, bank_churn_path)\n",
    "        print(f\"Downloaded to: {bank_churn_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: kaggle CLI not found. Install with: pip install kaggle\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {
    "papermill": {
     "duration": 0.001222,
     "end_time": "2026-01-29T23:46:04.910842",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.909620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Netflix Customer Churn Dataset\n",
    "Source: https://www.kaggle.com/datasets/vasifasad/netflix-customer-churn-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:46:04.913819Z",
     "iopub.status.busy": "2026-01-29T23:46:04.913715Z",
     "iopub.status.idle": "2026-01-29T23:46:04.917078Z",
     "shell.execute_reply": "2026-01-29T23:46:04.916471Z"
    },
    "papermill": {
     "duration": 0.005719,
     "end_time": "2026-01-29T23:46:04.917705",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.911986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists: ../tests/fixtures/netflix_customer_churn.csv\n"
     ]
    }
   ],
   "source": [
    "# Download Netflix Customer Churn dataset\n",
    "netflix_churn_path = FIXTURES_DIR / \"netflix_customer_churn.csv\"\n",
    "\n",
    "if netflix_churn_path.exists():\n",
    "    print(f\"Already exists: {netflix_churn_path}\")\n",
    "else:\n",
    "    print(\"Downloading Netflix Customer Churn dataset...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\", \"-d\", \"vasifasad/netflix-customer-churn-prediction\",\n",
    "            \"-p\", str(FIXTURES_DIR), \"--unzip\"\n",
    "        ], check=True)\n",
    "        print(f\"Downloaded to: {netflix_churn_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: kaggle CLI not found. Install with: pip install kaggle\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {
    "papermill": {
     "duration": 0.001268,
     "end_time": "2026-01-29T23:46:04.920721",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.919453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 0.5 Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:46:04.924539Z",
     "iopub.status.busy": "2026-01-29T23:46:04.924419Z",
     "iopub.status.idle": "2026-01-29T23:46:05.108058Z",
     "shell.execute_reply": "2026-01-29T23:46:05.107539Z"
    },
    "papermill": {
     "duration": 0.186443,
     "end_time": "2026-01-29T23:46:05.108657",
     "exception": false,
     "start_time": "2026-01-29T23:46:04.922214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "============================================================\n",
      "\n",
      "customer_retention_retail.csv:\n",
      "  Rows: 30,801\n",
      "  Columns: 15\n",
      "  Columns: custid, retained, created, firstorder, lastorder...\n",
      "\n",
      "bank_customer_churn.csv:\n",
      "  Rows: 10,000\n",
      "  Columns: 13\n",
      "  Columns: CustomerId, Surname, CreditScore, Geography, Gender...\n",
      "\n",
      "netflix_customer_churn.csv:\n",
      "  Rows: 5,000\n",
      "  Columns: 14\n",
      "  Columns: customer_id, age, gender, subscription_type, watch_hours...\n",
      "\n",
      "customer_transactions.csv:\n",
      "  Rows: 50,000\n",
      "  Columns: 15\n",
      "  Columns: transaction_id, customer_id, transaction_date, amount, product_category...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "customer_emails.csv:\n",
      "  Rows: 83,198\n",
      "  Columns: 13\n",
      "  Columns: email_id, customer_id, sent_date, campaign_type, opened...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_datasets = {**entity_datasets, **event_datasets}\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in all_datasets.keys():\n",
    "    path = FIXTURES_DIR / name\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Rows: {len(df):,}\")\n",
    "        print(f\"  Columns: {len(df.columns)}\")\n",
    "        print(f\"  Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"\\n{name}: Not downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {
    "papermill": {
     "duration": 0.001362,
     "end_time": "2026-01-29T23:46:05.111849",
     "exception": false,
     "start_time": "2026-01-29T23:46:05.110487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Temporal Framework Overview\n",
    "\n",
    "This framework includes a **leakage-safe temporal infrastructure** for preventing data leakage in ML pipelines:\n",
    "\n",
    "- **Timestamp Management**: Automatic detection and handling of `feature_timestamp` and `label_timestamp`\n",
    "- **Versioned Snapshots**: Point-in-time training snapshots with integrity hashing\n",
    "- **Scenario Detection**: Automatic detection of production vs Kaggle-style datasets\n",
    "- **Leakage Detection**: Multi-probe validation (correlation, separation, temporal logic)\n",
    "\n",
    "The temporal framework ensures that:\n",
    "1. Features are only computed using data available at prediction time\n",
    "2. Training data is versioned and reproducible\n",
    "3. Temporal leakage is detected before model training\n",
    "\n",
    "---\n",
    "\n",
    "## 0.6 Using the Temporal Framework\n",
    "\n",
    "### Loading Data with Snapshot Manager\n",
    "\n",
    "For production use, load data through the snapshot system to ensure reproducibility:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from customer_retention.stages.temporal import SnapshotManager, UnifiedDataPreparer, ScenarioDetector\n",
    "\n",
    "output_path = Path(\"../experiments/findings\")\n",
    "snapshot_manager = SnapshotManager(output_path)\n",
    "\n",
    "snapshots = snapshot_manager.list_snapshots()\n",
    "if snapshots:\n",
    "    latest = snapshot_manager.get_latest_snapshot()\n",
    "    df, metadata = snapshot_manager.load_snapshot(latest)\n",
    "    print(f\"Loaded {latest}: {df.shape}, created {metadata['created_at']}\")\n",
    "```\n",
    "\n",
    "### Auto-Detecting Dataset Scenario\n",
    "\n",
    "The framework automatically detects whether your data is production (with timestamps) or Kaggle-style:\n",
    "\n",
    "```python\n",
    "from customer_retention.stages.temporal import ScenarioDetector\n",
    "\n",
    "detector = ScenarioDetector()\n",
    "scenario, config, discovery_result = detector.detect(df, target_column=\"churned\")\n",
    "\n",
    "print(f\"Scenario: {scenario}\")\n",
    "print(f\"Feature timestamp: {config.feature_timestamp_column}\")\n",
    "print(f\"Label timestamp: {config.label_timestamp_column}\")\n",
    "print(f\"Strategy: {config.strategy.value}\")\n",
    "```\n",
    "\n",
    "### Manual Override (When Auto-Detection Fails)\n",
    "\n",
    "If auto-detection picks wrong columns or an unsuitable strategy, bypass it entirely by creating `TimestampConfig` directly:\n",
    "\n",
    "```python\n",
    "from customer_retention.stages.temporal import TimestampManager, TimestampConfig, TimestampStrategy\n",
    "\n",
    "config = TimestampConfig(\n",
    "    strategy=TimestampStrategy.PRODUCTION,\n",
    "    feature_timestamp_column=\"my_observation_date\",\n",
    "    label_timestamp_column=\"my_outcome_date\",\n",
    "    observation_window_days=90,\n",
    ")\n",
    "manager = TimestampManager(config)\n",
    "df_with_timestamps = manager.ensure_timestamps(df)\n",
    "```\n",
    "\n",
    "**Available strategies:**\n",
    "\n",
    "| Strategy | When to Use |\n",
    "|----------|-------------|\n",
    "| `PRODUCTION` | Data has explicit timestamp columns |\n",
    "| `DERIVED` | Timestamps can be computed from other columns (e.g., tenure) |\n",
    "| `SYNTHETIC_FIXED` | No temporal info - use fixed date for all rows |\n",
    "| `SYNTHETIC_RANDOM` | No temporal info - generate random dates within range |\n",
    "| `SYNTHETIC_INDEX` | No temporal info - generate dates based on row order |\n",
    "\n",
    "**Force synthetic timestamps (Kaggle-style data):**\n",
    "\n",
    "```python\n",
    "config = TimestampConfig(\n",
    "    strategy=TimestampStrategy.SYNTHETIC_FIXED,\n",
    "    synthetic_base_date=\"2024-01-01\",\n",
    "    observation_window_days=90,\n",
    ")\n",
    "```\n",
    "\n",
    "**Derive timestamps from tenure column:**\n",
    "\n",
    "```python\n",
    "config = TimestampConfig(\n",
    "    strategy=TimestampStrategy.DERIVED,\n",
    "    derivation_config={\n",
    "        \"feature_derivation\": {\n",
    "            \"formula\": \"reference_date - tenure_months\",\n",
    "            \"sources\": [\"tenure_months\"],\n",
    "        }\n",
    "    },\n",
    "    observation_window_days=90,\n",
    ")\n",
    "```\n",
    "\n",
    "### Creating a Training Snapshot from Raw Data\n",
    "\n",
    "```python\n",
    "from customer_retention.stages.temporal import UnifiedDataPreparer\n",
    "from customer_retention.core.config import TemporalConfig\n",
    "\n",
    "config = TemporalConfig(\n",
    "    feature_timestamp_column=\"feature_timestamp\",\n",
    "    label_timestamp_column=\"label_timestamp\",\n",
    ")\n",
    "\n",
    "preparer = UnifiedDataPreparer(output_path=Path(\"../experiments/findings\"), timestamp_config=config)\n",
    "\n",
    "unified_df = preparer.prepare_from_raw(\n",
    "    df=raw_df, target_column=\"churned\", entity_column=\"customer_id\"\n",
    ")\n",
    "\n",
    "snapshot_df, metadata = preparer.create_training_snapshot(df=unified_df, snapshot_name=\"training\")\n",
    "print(f\"Created snapshot: {metadata['snapshot_id']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You're ready to start exploring! Continue to **01_data_discovery.ipynb**.\n",
    "\n",
    "**Using your own data?** Just set `DATA_PATH` to your file:\n",
    "```python\n",
    "DATA_PATH = \"/path/to/your/data.csv\"\n",
    "```\n",
    "\n",
    "**Using sample datasets?** Choose one based on your learning goal:\n",
    "```python\n",
    "# Entity-level (standard flow)\n",
    "DATA_PATH = \"../tests/fixtures/customer_retention_retail.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/bank_customer_churn.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/netflix_customer_churn.csv\"\n",
    "\n",
    "# Event-level (time series flow)\n",
    "DATA_PATH = \"../tests/fixtures/customer_transactions.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/customer_emails.csv\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.705753,
   "end_time": "2026-01-29T23:46:05.532201",
   "environment_variables": {},
   "exception": null,
   "input_path": "exploration_notebooks/00_start_here.ipynb",
   "output_path": "docs/tutorial/executed/00_start_here.ipynb",
   "parameters": {},
   "start_time": "2026-01-29T23:46:02.826448",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}