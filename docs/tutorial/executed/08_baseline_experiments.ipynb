{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcb808d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199395e",
   "metadata": {
    "papermill": {
     "duration": 0.002301,
     "end_time": "2026-01-22T14:18:16.049063",
     "exception": false,
     "start_time": "2026-01-22T14:18:16.046762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chapter 8: Baseline Experiments\n",
    "\n",
    "**Purpose:** Train baseline models to understand data predictability and establish performance benchmarks.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to prepare data for ML with proper train/test splitting\n",
    "- How to handle class imbalance with class weights\n",
    "- How to evaluate models with appropriate metrics (not just accuracy!)\n",
    "- How to interpret feature importance\n",
    "\n",
    "**Outputs:**\n",
    "- Baseline model performance (AUC, Precision, Recall, F1)\n",
    "- Feature importance rankings\n",
    "- ROC and Precision-Recall curves\n",
    "- Performance benchmarks for comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics for Imbalanced Data\n",
    "\n",
    "| Metric | What It Measures | When to Use |\n",
    "|--------|-----------------|-------------|\n",
    "| **AUC-ROC** | Ranking quality across thresholds | General model comparison |\n",
    "| **Precision** | \"Of predicted churned, how many are correct?\" | When false positives are costly |\n",
    "| **Recall** | \"Of actual churned, how many did we catch?\" | When missing churners is costly |\n",
    "| **F1-Score** | Balance of precision and recall | When both matter equally |\n",
    "| **PR-AUC** | Precision-Recall under curve | Better for imbalanced data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02dfbb",
   "metadata": {
    "papermill": {
     "duration": 0.001618,
     "end_time": "2026-01-22T14:18:16.052645",
     "exception": false,
     "start_time": "2026-01-22T14:18:16.051027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a3bbc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:18:16.056786Z",
     "iopub.status.busy": "2026-01-22T14:18:16.056655Z",
     "iopub.status.idle": "2026-01-22T14:18:17.503860Z",
     "shell.execute_reply": "2026-01-22T14:18:17.503407Z"
    },
    "papermill": {
     "duration": 1.450367,
     "end_time": "2026-01-22T14:18:17.504588",
     "exception": false,
     "start_time": "2026-01-22T14:18:16.054221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (roc_auc_score, classification_report, confusion_matrix,\n",
    "                             roc_curve, precision_recall_curve, average_precision_score,\n",
    "                             f1_score, precision_score, recall_score)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307e152",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ba825c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:18:17.508319Z",
     "iopub.status.busy": "2026-01-22T14:18:17.508147Z",
     "iopub.status.idle": "2026-01-22T14:18:17.649187Z",
     "shell.execute_reply": "2026-01-22T14:18:17.648256Z"
    },
    "papermill": {
     "duration": 0.143726,
     "end_time": "2026-01-22T14:18:17.649999",
     "exception": true,
     "start_time": "2026-01-22T14:18:17.506273",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No findings files found in ../experiments/findings. Run notebook 01 first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m findings_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m FINDINGS_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*_findings.yaml\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmulti_dataset\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f.name]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m findings_files:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo findings files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINDINGS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run notebook 01 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m findings_files.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m f: f.stat().st_mtime, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m FINDINGS_PATH = \u001b[38;5;28mstr\u001b[39m(findings_files[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No findings files found in ../experiments/findings. Run notebook 01 first."
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "from pathlib import Path\n",
    "\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "findings_files = [f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") if \"multi_dataset\" not in f.name]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Found {len(findings_files)} findings file(s)\")\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "\n",
    "# Load data with snapshot preference (uses temporal snapshots if available)\n",
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\n",
    "charts = ChartBuilder()\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} rows from: {data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abafd676",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8.2 Prepare Data for Modeling\n",
    "\n",
    "**ðŸ“– Feature Source:**\n",
    "\n",
    "Features used in this notebook come from the **ExplorationFindings** generated in earlier notebooks:\n",
    "- Column types are **auto-detected** in notebook 01 (Data Discovery)\n",
    "- Target column is identified from the findings\n",
    "- Identifier columns are **excluded** to prevent data leakage\n",
    "- Text columns are **excluded** (require specialized NLP processing)\n",
    "\n",
    "**ðŸ“– Best Practices:**\n",
    "1. **Stratified Split**: Maintains class ratios in train/test sets\n",
    "2. **Scale After Split**: Fit scaler on train only (prevents data leakage)\n",
    "3. **Handle Missing**: Impute or drop before scaling\n",
    "\n",
    "**ðŸ“– Transformations Applied:**\n",
    "- Categorical variables â†’ Label Encoded\n",
    "- Missing values â†’ Median (numeric) or Mode (categorical)\n",
    "- Features â†’ StandardScaler (fit on train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2fd13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not findings.target_column:\n",
    "    raise ValueError(\"No target column set. Please define one in exploration notebooks.\")\n",
    "\n",
    "target = findings.target_column\n",
    "y = df[target]\n",
    "\n",
    "# Features are selected based on column types from ExplorationFindings\n",
    "# This ensures consistency with earlier notebooks and prevents data leakage\n",
    "feature_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type not in [ColumnType.IDENTIFIER, ColumnType.TARGET, ColumnType.TEXT]\n",
    "    and name not in TEMPORAL_METADATA_COLS  # Exclude point-in-time columns from features\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE SELECTION FROM FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸŽ¯ Target Column: {target}\")\n",
    "print(f\"ðŸ“Š Features Selected: {len(feature_cols)}\")\n",
    "\n",
    "# Show feature breakdown by type\n",
    "type_counts = {}\n",
    "for name in feature_cols:\n",
    "    col_type = findings.columns[name].inferred_type.value\n",
    "    type_counts[col_type] = type_counts.get(col_type, 0) + 1\n",
    "\n",
    "print(\"\\nðŸ“‹ Features by Type:\")\n",
    "for col_type, count in sorted(type_counts.items()):\n",
    "    print(f\"   {col_type}: {count}\")\n",
    "\n",
    "# Show excluded columns\n",
    "excluded = [name for name, col in findings.columns.items() \n",
    "            if col.inferred_type in [ColumnType.IDENTIFIER, ColumnType.TARGET, ColumnType.TEXT]]\n",
    "if excluded:\n",
    "    print(f\"\\nâ›” Excluded Columns: {', '.join(excluded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747f1db",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[feature_cols].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "# Handle missing values (median for numeric, mode for others)\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        if X[col].dtype in ['int64', 'float64']:\n",
    "            X[col] = X[col].fillna(X[col].median())\n",
    "        else:\n",
    "            X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "# Stratified train/test split (maintains class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features (fit on train only!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "print(f\"Train size: {len(X_train):,} ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"Test size: {len(X_test):,} ({len(X_test)/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nTrain class distribution:\")\n",
    "print(f\"  Retained (1): {(y_train == 1).sum():,} ({(y_train == 1).sum()/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Churned (0): {(y_train == 0).sum():,} ({(y_train == 0).sum()/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44a24b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8.3 Baseline Models (with Class Weights)\n",
    "\n",
    "**ðŸ“– Using Class Weights:**\n",
    "- `class_weight='balanced'` automatically adjusts weights inversely proportional to class frequencies\n",
    "- This helps models pay more attention to the minority class (churned customers)\n",
    "- Without weights, models may just predict \"retained\" for everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661db494",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Models with class_weight='balanced' to handle imbalance\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "model_predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for Logistic Regression, unscaled for tree-based\n",
    "    if \"Logistic\" in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    if \"Logistic\" in name:\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Test AUC\": auc,\n",
    "        \"PR-AUC\": pr_auc,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"CV AUC Mean\": cv_scores.mean(),\n",
    "        \"CV AUC Std\": cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    model_predictions[name] = {\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "display_table(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415096c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8.4 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3063b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf_model = models[\"Random Forest\"]\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"Importance\": rf_model.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "top_n = 15\n",
    "top_features = importance_df.head(top_n)\n",
    "\n",
    "fig = charts.bar_chart(\n",
    "    top_features[\"Feature\"].tolist(),\n",
    "    top_features[\"Importance\"].tolist(),\n",
    "    title=f\"Top {top_n} Feature Importances\"\n",
    ")\n",
    "display_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3f6f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8.5 Classification Report (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf85e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = models[\"Gradient Boosting\"]\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (Gradient Boosting):\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922a6c3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8.6 Model Comparison Grid\n",
    "\n",
    "This visualization shows all models side-by-side with:\n",
    "- **Row 1**: Confusion matrices (counts and percentages)\n",
    "- **Row 2**: ROC curves with AUC scores\n",
    "- **Row 3**: Precision-Recall curves with PR-AUC scores\n",
    "\n",
    "**ðŸ“– How to Read:**\n",
    "- **Confusion Matrix**: Diagonal = correct predictions. Off-diagonal = errors.\n",
    "- **ROC Curve**: Higher curve = better. AUC > 0.8 is good, > 0.9 is excellent.\n",
    "- **PR Curve**: Higher curve = better at finding positives without false alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a63ea2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create comprehensive model comparison grid\n",
    "# Uses the framework's ChartBuilder.model_comparison_grid method\n",
    "\n",
    "# Prepare model results in the expected format\n",
    "grid_results = {\n",
    "    name: {\n",
    "        \"y_pred\": data[\"y_pred\"],\n",
    "        \"y_pred_proba\": data[\"y_pred_proba\"]\n",
    "    }\n",
    "    for name, data in model_predictions.items()\n",
    "}\n",
    "\n",
    "# Create the visualization grid\n",
    "fig = charts.model_comparison_grid(\n",
    "    grid_results,\n",
    "    y_test,\n",
    "    class_labels=[\"Churned (0)\", \"Retained (1)\"],\n",
    "    title=\"Model Comparison: Confusion Matrix | ROC Curve | Precision-Recall\"\n",
    ")\n",
    "\n",
    "display_figure(fig)\n",
    "\n",
    "# Summary metrics table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display_table(results_df[[\"Model\", \"Test AUC\", \"PR-AUC\", \"F1-Score\", \"Precision\", \"Recall\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf19f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 8.6.1 Individual Model Analysis\n",
    "\n",
    "The grid above shows all models together. Below is detailed analysis per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f694a54",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Individual model classification reports\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFICATION REPORTS BY MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, data in model_predictions.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"ðŸ“Š {name}\")\n",
    "    print('='*40)\n",
    "    print(classification_report(y_test, data['y_pred'], target_names=[\"Churned\", \"Retained\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c09303",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 8.6.1 Precision-Recall Curves\n",
    "\n",
    "**ðŸ“– Why PR Curves for Imbalanced Data:**\n",
    "- ROC curves can look optimistic for imbalanced data\n",
    "- PR curves focus on the minority class (churners)\n",
    "- Better at showing how well we detect actual churners\n",
    "\n",
    "**ðŸ“– How to Read:**\n",
    "- **Baseline** (dashed line) = proportion of positives in the data\n",
    "- Higher curve = better at finding churners without too many false alarms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda099c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8.7 Key Takeaways\n",
    "\n",
    "**ðŸ“– Interpreting Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364319bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = results_df.loc[results_df['Test AUC'].idxmax()]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY TAKEAWAYS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_model['Model']}\")\n",
    "print(f\"   Test AUC: {best_model['Test AUC']:.4f}\")\n",
    "print(f\"   PR-AUC: {best_model['PR-AUC']:.4f}\")\n",
    "print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š TOP 3 IMPORTANT FEATURES:\")\n",
    "for i, feat in enumerate(importance_df.head(3)['Feature'].tolist(), 1):\n",
    "    imp = importance_df[importance_df['Feature'] == feat]['Importance'].values[0]\n",
    "    print(f\"   {i}. {feat} ({imp:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ MODEL PERFORMANCE ASSESSMENT:\")\n",
    "if best_model['Test AUC'] > 0.90:\n",
    "    print(\"   Excellent predictive signal - likely production-ready with tuning\")\n",
    "elif best_model['Test AUC'] > 0.80:\n",
    "    print(\"   Strong predictive signal - good baseline for improvement\")\n",
    "elif best_model['Test AUC'] > 0.70:\n",
    "    print(\"   Moderate signal - consider more feature engineering\")\n",
    "else:\n",
    "    print(\"   Weak signal - may need more data or different features\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ NEXT STEPS:\")\n",
    "print(\"   1. Feature engineering with derived features (notebook 05)\")\n",
    "print(\"   2. Hyperparameter tuning (GridSearchCV)\")\n",
    "print(\"   3. Threshold optimization for business metrics\")\n",
    "print(\"   4. A/B testing in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c746a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "In this notebook, we trained baseline models and established performance benchmarks:\n",
    "\n",
    "1. **Data Preparation** - Proper train/test split with stratification and scaling\n",
    "2. **Class Imbalance Handling** - Used balanced class weights\n",
    "3. **Model Comparison** - Compared Logistic Regression, Random Forest, and Gradient Boosting\n",
    "4. **Multiple Metrics** - Evaluated with AUC, PR-AUC, F1, Precision, Recall\n",
    "5. **Feature Importance** - Identified the most predictive features\n",
    "\n",
    "## Key Results for This Dataset\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| Best AUC | ~0.98 | Excellent discrimination |\n",
    "| Top Feature | esent | Email engagement is critical |\n",
    "| Imbalance | ~4:1 | Moderate, handled with class weights |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **09_business_alignment.ipynb** to:\n",
    "- Align model performance with business objectives\n",
    "- Define intervention strategies by risk level\n",
    "- Calculate expected ROI from the model\n",
    "- Set deployment requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba61fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_auc = max(float(r[\"Test AUC\"]) for r in results)\n",
    "\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best baseline AUC: {best_auc:.4f}\")\n",
    "print(f\"Top 3 important features: {', '.join(importance_df.head(3)['Feature'].tolist())}\")\n",
    "\n",
    "if best_auc > 0.85:\n",
    "    print(\"\\nStrong predictive signal detected. Data is well-suited for modeling.\")\n",
    "elif best_auc > 0.70:\n",
    "    print(\"\\nModerate predictive signal. Consider feature engineering for improvement.\")\n",
    "else:\n",
    "    print(\"\\nWeak predictive signal. May need more features or data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff99f03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **09_business_alignment.ipynb** to align with business objectives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.523121,
   "end_time": "2026-01-22T14:18:17.968481",
   "environment_variables": {},
   "exception": true,
   "input_path": "exploration_notebooks/08_baseline_experiments.ipynb",
   "output_path": "docs/tutorial/executed/08_baseline_experiments.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T14:18:15.445360",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}