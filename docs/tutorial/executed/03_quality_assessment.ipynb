{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ae3bdb",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e04ac5",
   "metadata": {
    "papermill": {
     "duration": 0.002782,
     "end_time": "2026-01-22T14:18:03.746443",
     "exception": false,
     "start_time": "2026-01-22T14:18:03.743661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chapter 3: Quality Assessment\n",
    "\n",
    "**Purpose:** Deep dive into data quality issues with actionable remediation strategies.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to analyze missing value patterns (MCAR vs MAR vs MNAR)\n",
    "- How to detect and handle outliers using IQR method\n",
    "- How to validate date sequences and binary fields\n",
    "- How to implement data cleanup strategies\n",
    "\n",
    "**Outputs:**\n",
    "- Missing value analysis with correlation patterns\n",
    "- Outlier detection with visualization\n",
    "- Date logic validation results\n",
    "- Binary field validation\n",
    "- Cleanup code examples ready to use\n",
    "\n",
    "---\n",
    "\n",
    "## Quality Assessment Framework\n",
    "\n",
    "| Issue Type | Detection Method | Common Solutions |\n",
    "|------------|-----------------|------------------|\n",
    "| Missing Values | Null counts, pattern analysis | Impute (mean/median/mode), drop, flag |\n",
    "| Outliers | IQR, Z-score, isolation forest | Cap/clip, winsorize, transform, keep (if valid) |\n",
    "| Date Logic | Sequence validation | Set placeholders to NULL, exclude invalid |\n",
    "| Duplicates | Key uniqueness | Drop exact, keep most recent |\n",
    "| Invalid Values | Range/domain checks | Correct, flag, exclude |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ccd74d",
   "metadata": {
    "papermill": {
     "duration": 0.003214,
     "end_time": "2026-01-22T14:18:03.751748",
     "exception": false,
     "start_time": "2026-01-22T14:18:03.748534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4710a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:18:03.757519Z",
     "iopub.status.busy": "2026-01-22T14:18:03.757398Z",
     "iopub.status.idle": "2026-01-22T14:18:05.171434Z",
     "shell.execute_reply": "2026-01-22T14:18:05.170922Z"
    },
    "papermill": {
     "duration": 1.417818,
     "end_time": "2026-01-22T14:18:05.172223",
     "exception": false,
     "start_time": "2026-01-22T14:18:03.754405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings, RecommendationRegistry\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6fe1a",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcae8382",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:18:05.177364Z",
     "iopub.status.busy": "2026-01-22T14:18:05.177200Z",
     "iopub.status.idle": "2026-01-22T14:18:05.313788Z",
     "shell.execute_reply": "2026-01-22T14:18:05.313281Z"
    },
    "papermill": {
     "duration": 0.139975,
     "end_time": "2026-01-22T14:18:05.314643",
     "exception": true,
     "start_time": "2026-01-22T14:18:05.174668",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No findings files found in ../experiments/findings. Run notebook 01 first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m findings_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m FINDINGS_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*_findings.yaml\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmulti_dataset\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f.name]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m findings_files:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo findings files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINDINGS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run notebook 01 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m findings_files.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m f: f.stat().st_mtime, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m FINDINGS_PATH = \u001b[38;5;28mstr\u001b[39m(findings_files[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No findings files found in ../experiments/findings. Run notebook 01 first."
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Option 1: Set the exact path from notebook 01 output\n",
    "# FINDINGS_PATH = \"../experiments/findings/customer_retention_retail_abc123_findings.yaml\"\n",
    "\n",
    "# Option 2: Auto-discover the most recent findings file\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "findings_files = [f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") if \"multi_dataset\" not in f.name]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Found {len(findings_files)} findings file(s)\")\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "\n",
    "# Load data with snapshot preference (uses snapshot if available, falls back to source)\n",
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\n",
    "print(f\"Loaded data from: {data_source}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "charts = ChartBuilder()\n",
    "\n",
    "# Load or initialize recommendation registry\n",
    "RECOMMENDATIONS_PATH = FINDINGS_PATH.replace(\"_findings.yaml\", \"_recommendations.yaml\")\n",
    "if Path(RECOMMENDATIONS_PATH).exists():\n",
    "    with open(RECOMMENDATIONS_PATH, \"r\") as f:\n",
    "        registry = RecommendationRegistry.from_dict(yaml.safe_load(f))\n",
    "    print(f\"Loaded existing recommendations from: {RECOMMENDATIONS_PATH}\")\n",
    "else:\n",
    "    registry = RecommendationRegistry()\n",
    "    registry.init_bronze(findings.source_path)\n",
    "    if findings.target_column:\n",
    "        registry.init_gold(findings.target_column)\n",
    "    entity_col = next((name for name, col in findings.columns.items() if col.inferred_type == ColumnType.IDENTIFIER), None)\n",
    "    if entity_col:\n",
    "        registry.init_silver(entity_col)\n",
    "    print(f\"Initialized new recommendation registry\")\n",
    "\n",
    "print(f\"\\nLoaded findings for {findings.column_count} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87622e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Duplicate Analysis\n",
    "\n",
    "**üìñ Why This Matters:**\n",
    "- Duplicate records can skew statistics and model training\n",
    "- Key column duplicates may indicate data quality issues or event-level data\n",
    "- Value conflicts (same key, different values) require investigation\n",
    "\n",
    "**What to Watch For:**\n",
    "- **Exact duplicates**: Identical rows that should be deduplicated\n",
    "- **Key duplicates**: Same identifier with different values (may indicate updates or errors)\n",
    "- **Value conflicts**: Same key with conflicting values in important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce99206",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.stages.validation import DataValidator\n",
    "\n",
    "validator = DataValidator()\n",
    "\n",
    "# Auto-detect potential key columns\n",
    "potential_keys = [name for name, col in findings.columns.items() \n",
    "                  if col.inferred_type.value in ('identifier', 'id') or 'id' in name.lower()]\n",
    "KEY_COLUMN = potential_keys[0] if potential_keys else None\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if KEY_COLUMN:\n",
    "    dup_result = validator.check_duplicates(df, key_column=KEY_COLUMN, check_value_conflicts=True)\n",
    "    print(f\"\\nKey Column: {KEY_COLUMN}\")\n",
    "    print(f\"Total Rows: {dup_result.total_rows:,}\")\n",
    "    print(f\"Unique Keys: {dup_result.unique_keys:,}\")\n",
    "    print(f\"Duplicate Keys: {dup_result.duplicate_keys:,} ({dup_result.duplicate_percentage:.2f}%)\")\n",
    "    \n",
    "    # Exact duplicates\n",
    "    if dup_result.exact_duplicate_rows > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Exact duplicate rows: {dup_result.exact_duplicate_rows:,}\")\n",
    "        dup_mask = df.duplicated(keep=False)\n",
    "        dup_examples = df[dup_mask].head(6)\n",
    "        if len(dup_examples) > 0:\n",
    "            print(\"\\nExample duplicate rows:\")\n",
    "            display(dup_examples)\n",
    "        \n",
    "        # Add deduplication recommendation for exact duplicates\n",
    "        registry.add_bronze_deduplication(\n",
    "            key_column=KEY_COLUMN, strategy=\"drop_exact_duplicates\",\n",
    "            rationale=f\"{dup_result.exact_duplicate_rows} exact duplicate rows detected\",\n",
    "            source_notebook=\"03_quality_assessment\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n‚úì No exact duplicate rows\")\n",
    "    \n",
    "    # Value conflicts\n",
    "    if dup_result.has_value_conflicts:\n",
    "        print(f\"\\n‚ö†Ô∏è Value conflicts detected in: {', '.join(dup_result.conflict_columns[:5])}\")\n",
    "        if findings.target_column and findings.target_column in dup_result.conflict_columns:\n",
    "            print(f\"   üî¥ CRITICAL: Target '{findings.target_column}' has conflicting values!\")\n",
    "        \n",
    "        # Show examples of conflicting records\n",
    "        key_counts = df[KEY_COLUMN].value_counts()\n",
    "        dup_keys = key_counts[key_counts > 1].head(3).index.tolist()\n",
    "        if dup_keys:\n",
    "            print(\"\\nExample records with duplicate keys:\")\n",
    "            conflict_examples = df[df[KEY_COLUMN].isin(dup_keys)].sort_values(KEY_COLUMN).head(10)\n",
    "            display(conflict_examples)\n",
    "        \n",
    "        # Add deduplication recommendation for value conflicts\n",
    "        registry.add_bronze_deduplication(\n",
    "            key_column=KEY_COLUMN, strategy=\"keep_first\",\n",
    "            rationale=f\"Value conflicts in {len(dup_result.conflict_columns)} columns\",\n",
    "            source_notebook=\"03_quality_assessment\",\n",
    "            conflict_columns=dup_result.conflict_columns[:5]\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n‚úì No value conflicts\")\n",
    "    \n",
    "    # Duplicate frequency distribution\n",
    "    if dup_result.duplicate_keys > 0:\n",
    "        key_counts = df[KEY_COLUMN].value_counts()\n",
    "        dup_distribution = key_counts[key_counts > 1].value_counts().sort_index()\n",
    "        if len(dup_distribution) > 0:\n",
    "            print(\"\\nDuplicate frequency distribution:\")\n",
    "            for count, num_keys in dup_distribution.head(5).items():\n",
    "                print(f\"   Keys appearing {count}x: {num_keys:,}\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "        if dup_result.exact_duplicate_rows > 0:\n",
    "            print(\"   ‚Ä¢ Remove exact duplicates: df.drop_duplicates()\")\n",
    "        if dup_result.has_value_conflicts:\n",
    "            print(\"   ‚Ä¢ For value conflicts, decide strategy:\")\n",
    "            print(\"     - Keep most recent (if you have a timestamp)\")\n",
    "            print(\"     - Keep first occurrence: df.drop_duplicates(subset=[KEY_COLUMN], keep='first')\")\n",
    "            print(\"     - Aggregate values (for numeric columns)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No key column detected.\")\n",
    "    print(\"   Set KEY_COLUMN above to enable duplicate analysis.\")\n",
    "    print(f\"   Available columns: {list(findings.columns.keys())[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46423e3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.3 Overall Quality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0657a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Overall Quality Score: {findings.overall_quality_score:.1f}/100\")\n",
    "\n",
    "if findings.overall_quality_score >= 90:\n",
    "    print(\"Excellent: Data is high quality and ready for modeling.\")\n",
    "elif findings.overall_quality_score >= 70:\n",
    "    print(\"Good: Minor quality issues that should be addressed.\")\n",
    "elif findings.overall_quality_score >= 50:\n",
    "    print(\"Fair: Significant quality issues require attention.\")\n",
    "else:\n",
    "    print(\"Poor: Major quality issues must be resolved before modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65608c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.4 Target Variable Analysis\n",
    "\n",
    "Understanding target distribution is critical for:\n",
    "- **Class imbalance** affects model training and evaluation metrics\n",
    "- **Business context** helps interpret what we're trying to predict\n",
    "- **Sampling strategies** depend on imbalance severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207dfce6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"TARGET VARIABLE DISTRIBUTION: {findings.target_column}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if findings.target_column and findings.target_column in df.columns:\n",
    "    target_series = df[findings.target_column]\n",
    "    target_counts = target_series.value_counts().sort_index()\n",
    "    \n",
    "    # Create distribution table\n",
    "    dist_data = []\n",
    "    for val, count in target_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        dist_data.append({\n",
    "            findings.target_column: val,\n",
    "            \"count\": count,\n",
    "            \"percentage\": f\"{pct:.3f}\"\n",
    "        })\n",
    "    \n",
    "    dist_df = pd.DataFrame(dist_data)\n",
    "    display(dist_df)\n",
    "    \n",
    "    # Calculate imbalance metrics\n",
    "    if len(target_counts) == 2:\n",
    "        majority = target_counts.max()\n",
    "        minority = target_counts.min()\n",
    "        minority_class = target_counts.idxmin()\n",
    "        imbalance_ratio = majority / minority\n",
    "        retention_rate = target_counts.get(1, 0) / len(df) * 100\n",
    "        \n",
    "        print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1 (minority class: {minority_class})\")\n",
    "        print(f\"Retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        # Business context\n",
    "        if retention_rate > 70:\n",
    "            print(f\"\\nüìä Business Context: {retention_rate:.0f}% retention is healthy!\")\n",
    "            print(\"   Churned customers are the minority class we want to predict.\")\n",
    "        elif retention_rate > 50:\n",
    "            print(f\"\\nüìä Business Context: {retention_rate:.0f}% retention is moderate.\")\n",
    "            print(\"   Balanced focus on both retention and churn prediction.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Business Context: {retention_rate:.0f}% retention is concerning!\")\n",
    "            print(\"   High churn rate requires urgent attention.\")\n",
    "        \n",
    "        # Modeling recommendations based on imbalance\n",
    "        print(\"\\n‚ö†Ô∏è  Class imbalance considerations for modeling:\")\n",
    "        print(\"   - Use stratified sampling for train/test splits\")\n",
    "        print(\"   - Consider class weights in model training\")\n",
    "        print(\"   - Evaluate with Precision-Recall AUC (not just ROC-AUC)\")\n",
    "        print(\"   - Focus on recall for churned class (catch at-risk customers)\")\n",
    "        \n",
    "        # Add imbalance strategy recommendation\n",
    "        if imbalance_ratio < 3:\n",
    "            strategy = \"stratified_sampling\"\n",
    "            rationale = f\"Mild imbalance ({imbalance_ratio:.2f}:1) - stratified sampling sufficient\"\n",
    "            print(\"   - SMOTE not needed (imbalance is mild)\")\n",
    "        elif imbalance_ratio < 5:\n",
    "            strategy = \"class_weights\"\n",
    "            rationale = f\"Moderate imbalance ({imbalance_ratio:.2f}:1) - use class weights\"\n",
    "            print(\"   - SMOTE may not be necessary (imbalance is moderate)\")\n",
    "        else:\n",
    "            strategy = \"smote\"\n",
    "            rationale = f\"Severe imbalance ({imbalance_ratio:.2f}:1) - consider SMOTE\"\n",
    "            print(\"   - Consider SMOTE or undersampling (imbalance is severe)\")\n",
    "        \n",
    "        registry.add_bronze_imbalance_strategy(\n",
    "            target_column=findings.target_column,\n",
    "            imbalance_ratio=imbalance_ratio,\n",
    "            minority_class=minority_class,\n",
    "            strategy=strategy,\n",
    "            rationale=rationale,\n",
    "            source_notebook=\"03_quality_assessment\"\n",
    "        )\n",
    "        \n",
    "        # Visualization\n",
    "        fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]],\n",
    "                           subplot_titles=[\"Class Distribution\", \"Count Comparison\"])\n",
    "        \n",
    "        labels = [f\"{'Retained' if v == 1 else 'Churned'} ({v})\" for v in target_counts.index]\n",
    "        fig.add_trace(go.Pie(labels=labels, values=target_counts.values, hole=0.4,\n",
    "                            marker_colors=[\"#2ecc71\", \"#e74c3c\"]), row=1, col=1)\n",
    "        fig.add_trace(go.Bar(x=labels, y=target_counts.values,\n",
    "                            marker_color=[\"#e74c3c\", \"#2ecc71\"]), row=1, col=2)\n",
    "        \n",
    "        fig.update_layout(height=350, title_text=\"Target Variable Distribution\",\n",
    "                         showlegend=False, template=\"plotly_white\")\n",
    "        display_figure(fig)\n",
    "    else:\n",
    "        print(f\"\\nMulticlass target with {len(target_counts)} classes\")\n",
    "        \n",
    "        fig = go.Figure(go.Bar(x=[str(v) for v in target_counts.index], y=target_counts.values,\n",
    "                               marker_color=px.colors.qualitative.Set2[:len(target_counts)]))\n",
    "        fig.update_layout(height=350, title_text=\"Target Variable Distribution\",\n",
    "                         xaxis_title=findings.target_column, yaxis_title=\"Count\",\n",
    "                         template=\"plotly_white\")\n",
    "        display_figure(fig)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No target column detected or specified.\")\n",
    "    print(\"   Set target_hint parameter in DataExplorer.explore() or\")\n",
    "    print(\"   manually specify in findings.target_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1327b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.5 Missing Value Analysis\n",
    "\n",
    "**üìñ Interpretation Guide:**\n",
    "- **MCAR (Missing Completely at Random)**: Missing values have no pattern - safe to impute with mean/median\n",
    "- **MAR (Missing at Random)**: Missingness depends on other observed variables - use regression imputation\n",
    "- **MNAR (Missing Not at Random)**: Missingness depends on the missing value itself - create missing indicator\n",
    "\n",
    "**‚ö†Ô∏è What to Watch For:**\n",
    "- Columns with >50% missing may need to be dropped\n",
    "- Highly correlated missing patterns suggest MAR\n",
    "- ID columns with missing values indicate data integrity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5e6d7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_data = []\n",
    "for col_name, col_info in findings.columns.items():\n",
    "    null_count = col_info.universal_metrics.get(\"null_count\", 0)\n",
    "    null_pct = col_info.universal_metrics.get(\"null_percentage\", 0)\n",
    "    if null_count > 0:\n",
    "        missing_data.append({\n",
    "            \"Column\": col_name,\n",
    "            \"Missing Count\": null_count,\n",
    "            \"Missing %\": f\"{null_pct:.2f}%\"\n",
    "        })\n",
    "\n",
    "if missing_data:\n",
    "    missing_df = pd.DataFrame(missing_data).sort_values(\"Missing Count\", ascending=False)\n",
    "    print(\"Columns with Missing Values:\")\n",
    "    display(missing_df)\n",
    "    \n",
    "    fig = charts.bar_chart(\n",
    "        missing_df[\"Column\"].tolist(),\n",
    "        [float(x.replace(\"%\", \"\")) for x in missing_df[\"Missing %\"].tolist()],\n",
    "        title=\"Missing Value Percentage by Column\"\n",
    "    )\n",
    "    display_figure(fig)\n",
    "else:\n",
    "    print(\"No missing values detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8b02a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.6 Missing Value Patterns\n",
    "\n",
    "**üìñ How to Read the Correlation Heatmap:**\n",
    "- **Correlation = 1.0**: Columns always missing together (same rows)\n",
    "- **Correlation > 0.5**: Strong pattern - investigate the relationship\n",
    "- **Correlation ‚âà 0**: Independent missing patterns (MCAR likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9acde6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_matrix = df.isnull()\n",
    "missing_correlations = missing_matrix.corr()\n",
    "\n",
    "cols_with_missing = [col for col in df.columns if df[col].isnull().any()]\n",
    "if len(cols_with_missing) > 1:\n",
    "    print(\"Missing Value Correlations (MCAR vs MAR analysis):\")\n",
    "    fig = charts.heatmap(\n",
    "        missing_correlations.loc[cols_with_missing, cols_with_missing].values,\n",
    "        x_labels=cols_with_missing,\n",
    "        y_labels=cols_with_missing,\n",
    "        title=\"Missing Value Pattern Correlation\"\n",
    "    )\n",
    "    display_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ade42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.7 Segment-Aware Outlier Analysis\n",
    "\n",
    "**üìñ Why Segment Before Detecting Outliers?**\n",
    "\n",
    "Global outlier detection can produce **false positives** when data contains natural segments:\n",
    "- **Retail vs Enterprise customers**: Order values of $5K may be outliers for retail but normal for enterprise\n",
    "- **New vs Established accounts**: Activity patterns differ dramatically by customer tenure\n",
    "- **Geographic segments**: Regional price differences can appear as outliers globally\n",
    "\n",
    "**‚ö†Ô∏è The Risk:**\n",
    "If you remove \"outliers\" that are actually valid data from a different segment, you lose critical patterns needed for accurate modeling.\n",
    "\n",
    "**üìä What This Analysis Does:**\n",
    "1. Detects natural data segments (using clustering or explicit segment columns)\n",
    "2. Compares global outliers vs segment-specific outliers\n",
    "3. Identifies \"false outliers\" - values flagged globally but normal within their segment\n",
    "4. Recommends whether segment-specific outlier treatment is beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accf2aa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.stages.profiling import SegmentAwareOutlierAnalyzer\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEGMENT-AWARE OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get numeric columns for analysis\n",
    "numeric_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]\n",
    "    and name not in TEMPORAL_METADATA_COLS\n",
    "]\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Option 1: Specify an explicit segment column if you have one (e.g., customer_type, region)\n",
    "SEGMENT_COL = None  # e.g., \"customer_segment\", \"account_type\"\n",
    "\n",
    "# Option 2: Load from findings metadata if saved in previous notebook\n",
    "if SEGMENT_COL is None and \"segment_column\" in findings.metadata:\n",
    "    SEGMENT_COL = findings.metadata[\"segment_column\"]\n",
    "    print(f\"Using segment column from findings: {SEGMENT_COL}\")\n",
    "\n",
    "if numeric_cols:\n",
    "    analyzer = SegmentAwareOutlierAnalyzer(max_segments=5)\n",
    "    \n",
    "    # Run segment-aware analysis\n",
    "    segment_result = analyzer.analyze(\n",
    "        df,\n",
    "        feature_cols=numeric_cols,\n",
    "        segment_col=SEGMENT_COL,\n",
    "        target_col=findings.target_column\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä SEGMENTATION RESULTS:\")\n",
    "    print(f\"   Segments detected: {segment_result.n_segments}\")\n",
    "    \n",
    "    if segment_result.n_segments > 1:\n",
    "        print(f\"\\nüìà GLOBAL VS SEGMENT OUTLIER COMPARISON:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        comparison_data = []\n",
    "        for col in numeric_cols:\n",
    "            global_outliers = segment_result.global_analysis[col].outliers_detected\n",
    "            segment_outliers = sum(\n",
    "                seg[col].outliers_detected \n",
    "                for seg in segment_result.segment_analysis.values()\n",
    "                if col in seg\n",
    "            )\n",
    "            false_outliers = segment_result.false_outliers.get(col, 0)\n",
    "            \n",
    "            if global_outliers > 0:\n",
    "                reduction_pct = (global_outliers - segment_outliers) / global_outliers * 100\n",
    "                false_pct = false_outliers / global_outliers * 100\n",
    "            else:\n",
    "                reduction_pct = 0\n",
    "                false_pct = 0\n",
    "            \n",
    "            comparison_data.append({\n",
    "                \"Feature\": col,\n",
    "                \"Global Outliers\": global_outliers,\n",
    "                \"Segment Outliers\": segment_outliers,\n",
    "                \"False Outliers\": false_outliers,\n",
    "                \"Reduction\": f\"{reduction_pct:.1f}%\"\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        display(comparison_df)\n",
    "        \n",
    "        # Show false outlier analysis\n",
    "        has_false_outliers = any(segment_result.false_outliers.get(col, 0) > 0 for col in numeric_cols)\n",
    "        \n",
    "        if has_false_outliers:\n",
    "            print(\"\\n‚ö†Ô∏è FALSE OUTLIERS DETECTED:\")\n",
    "            print(\"   (Global outliers that are normal within their segment)\")\n",
    "            for col, count in segment_result.false_outliers.items():\n",
    "                if count > 0:\n",
    "                    global_count = segment_result.global_analysis[col].outliers_detected\n",
    "                    pct = count / global_count * 100 if global_count > 0 else 0\n",
    "                    print(f\"   ‚Ä¢ {col}: {count} false outliers ({pct:.1f}% of global)\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "        if segment_result.segmentation_recommended:\n",
    "            print(\"   ‚úÖ SEGMENT-SPECIFIC OUTLIER TREATMENT RECOMMENDED\")\n",
    "            for rec in segment_result.recommendations:\n",
    "                print(f\"      ‚Ä¢ {rec}\")\n",
    "            \n",
    "            # Add outlier recommendations for columns with high false outlier rate\n",
    "            for col, count in segment_result.false_outliers.items():\n",
    "                if count > 0:\n",
    "                    global_count = segment_result.global_analysis[col].outliers_detected\n",
    "                    false_pct = count / global_count * 100 if global_count > 0 else 0\n",
    "                    if false_pct > 50:  # High false outlier rate\n",
    "                        registry.add_bronze_outlier(\n",
    "                            column=col, action=\"segment_aware_cap\",\n",
    "                            parameters={\"method\": \"segment_iqr\", \"n_segments\": segment_result.n_segments},\n",
    "                            rationale=f\"{false_pct:.0f}% of global outliers are segment-normal\",\n",
    "                            source_notebook=\"03_quality_assessment\"\n",
    "                        )\n",
    "        else:\n",
    "            print(\"   ‚ÑπÔ∏è Global outlier treatment is appropriate for this data\")\n",
    "        \n",
    "        # Rationale\n",
    "        print(\"\\nüìã RATIONALE:\")\n",
    "        for rationale in segment_result.rationale:\n",
    "            print(f\"   ‚Ä¢ {rationale}\")\n",
    "        \n",
    "        # Visualization: Compare outlier counts\n",
    "        cols_with_diff = [\n",
    "            row[\"Feature\"] for _, row in comparison_df.iterrows()\n",
    "            if row[\"Global Outliers\"] > 0 and row[\"Global Outliers\"] != row[\"Segment Outliers\"]\n",
    "        ]\n",
    "        \n",
    "        if cols_with_diff and len(cols_with_diff) <= 8:\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            global_counts = [comparison_df[comparison_df[\"Feature\"] == c][\"Global Outliers\"].values[0] for c in cols_with_diff]\n",
    "            segment_counts = [comparison_df[comparison_df[\"Feature\"] == c][\"Segment Outliers\"].values[0] for c in cols_with_diff]\n",
    "            \n",
    "            fig.add_trace(go.Bar(name=\"Global Outliers\", x=cols_with_diff, y=global_counts, marker_color=\"#e74c3c\"))\n",
    "            fig.add_trace(go.Bar(name=\"Segment Outliers\", x=cols_with_diff, y=segment_counts, marker_color=\"#2ecc71\"))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                barmode=\"group\",\n",
    "                title=\"Global vs Segment-Specific Outlier Detection\",\n",
    "                xaxis_title=\"Feature\",\n",
    "                yaxis_title=\"Outlier Count\",\n",
    "                template=\"plotly_white\",\n",
    "                height=400\n",
    "            )\n",
    "            display_figure(fig)\n",
    "    else:\n",
    "        print(\"\\n   ‚ÑπÔ∏è Data appears homogeneous (single segment)\")\n",
    "        print(\"   ‚Üí Proceeding with standard global outlier detection\")\n",
    "    \n",
    "    # Store result in findings metadata for use in later notebooks\n",
    "    findings.metadata[\"segment_aware_analysis\"] = {\n",
    "        \"n_segments\": segment_result.n_segments,\n",
    "        \"segmentation_recommended\": segment_result.segmentation_recommended,\n",
    "        \"recommendations\": segment_result.recommendations\n",
    "    }\n",
    "else:\n",
    "    print(\"\\nNo numeric columns to analyze for outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54072e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.8 Global Outlier Detection\n",
    "\n",
    "**üìñ IQR Method Explained:**\n",
    "- **Q1** = 25th percentile, **Q3** = 75th percentile\n",
    "- **IQR** = Q3 - Q1 (the middle 50% of data)\n",
    "- **Lower Bound** = Q1 - 1.5 √ó IQR\n",
    "- **Upper Bound** = Q3 + 1.5 √ó IQR\n",
    "- Values outside these bounds are considered outliers\n",
    "\n",
    "**‚ö†Ô∏è Important Considerations:**\n",
    "- Review section 3.7 above to determine if global or segment-specific outlier treatment is appropriate\n",
    "- Outliers in rate fields (>100%) are likely errors ‚Üí Cap at 100\n",
    "- Outliers in amount fields may be valid high-value customers ‚Üí Keep but consider capping for modeling\n",
    "- High outlier % (>10%) suggests heavy-tailed distribution ‚Üí Consider log transform instead of capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bd1af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER DETECTION (IQR Method)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numeric_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]\n",
    "    and name not in TEMPORAL_METADATA_COLS\n",
    "]\n",
    "\n",
    "# Build comprehensive outlier table\n",
    "outlier_data = []\n",
    "for col_name in numeric_cols:\n",
    "    series = df[col_name].dropna()\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    outliers_low = (series < lower_bound).sum()\n",
    "    outliers_high = (series > upper_bound).sum()\n",
    "    total_outliers = outliers_low + outliers_high\n",
    "    \n",
    "    outlier_data.append({\n",
    "        \"feature\": col_name,\n",
    "        \"Q1\": q1,\n",
    "        \"Q3\": q3,\n",
    "        \"IQR\": iqr,\n",
    "        \"lower_bound\": lower_bound,\n",
    "        \"upper_bound\": upper_bound,\n",
    "        \"outliers_low\": outliers_low,\n",
    "        \"outliers_high\": outliers_high,\n",
    "        \"total_outliers\": total_outliers,\n",
    "        \"outlier_pct\": total_outliers / len(series) * 100\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_data)\n",
    "\n",
    "# Display IQR bounds table\n",
    "print(\"\\nüìä IQR BOUNDS TABLE:\")\n",
    "bounds_display = outlier_df[[\"feature\", \"Q1\", \"Q3\", \"IQR\", \"lower_bound\", \"upper_bound\", \n",
    "                              \"outliers_low\", \"outliers_high\"]].copy()\n",
    "for col in [\"Q1\", \"Q3\", \"IQR\", \"lower_bound\", \"upper_bound\"]:\n",
    "    bounds_display[col] = bounds_display[col].apply(lambda x: f\"{x:.2f}\")\n",
    "display(bounds_display)\n",
    "\n",
    "# Outlier summary for columns with issues\n",
    "cols_with_outliers = outlier_df[outlier_df[\"total_outliers\"] > 0].copy()\n",
    "if len(cols_with_outliers) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è COLUMNS WITH OUTLIERS:\")\n",
    "    for _, row in cols_with_outliers.iterrows():\n",
    "        severity = \"üî¥ HIGH\" if row[\"outlier_pct\"] > 10 else \"üü° MEDIUM\" if row[\"outlier_pct\"] > 5 else \"üü¢ LOW\"\n",
    "        print(f\"\\n  {row['feature']}: {row['total_outliers']:,} outliers ({row['outlier_pct']:.2f}%) {severity}\")\n",
    "        print(f\"     Lower bound: {row['lower_bound']:.2f} | Upper bound: {row['upper_bound']:.2f}\")\n",
    "        if row[\"outliers_low\"] > 0:\n",
    "            print(f\"     Below lower: {row['outliers_low']:,}\")\n",
    "        if row[\"outliers_high\"] > 0:\n",
    "            print(f\"     Above upper: {row['outliers_high']:,}\")\n",
    "        \n",
    "        # Determine action and add recommendation (skip if segment-aware already added)\n",
    "        col_name = row['feature']\n",
    "        existing_outlier_recs = [r for r in registry.bronze.outlier_handling if r.target_column == col_name]\n",
    "        \n",
    "        if not existing_outlier_recs and row[\"outlier_pct\"] > 5:  # Only add if significant and not already handled\n",
    "            if row[\"outlier_pct\"] > 10:\n",
    "                action = \"log_transform\"\n",
    "                rationale = f\"{row['outlier_pct']:.1f}% outliers - heavy tails require log transform\"\n",
    "                print(\"     ‚Üí Consider log transform or RobustScaler\")\n",
    "            else:\n",
    "                action = \"winsorize\"\n",
    "                rationale = f\"{row['outlier_pct']:.1f}% outliers - winsorize to 1st/99th percentile\"\n",
    "                print(\"     ‚Üí Consider Winsorization (clip to 1st/99th percentile)\")\n",
    "            \n",
    "            registry.add_bronze_outlier(\n",
    "                column=col_name, action=action,\n",
    "                parameters={\"method\": \"iqr\", \"lower_bound\": row[\"lower_bound\"], \"upper_bound\": row[\"upper_bound\"]},\n",
    "                rationale=rationale,\n",
    "                source_notebook=\"03_quality_assessment\"\n",
    "            )\n",
    "        elif row[\"outlier_pct\"] <= 5:\n",
    "            print(\"     ‚Üí Minor issue, can cap at IQR bounds if needed\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No significant outliers detected in numeric columns\")\n",
    "\n",
    "# Box plots for columns with outliers\n",
    "if len(cols_with_outliers) > 0 and len(cols_with_outliers) <= 6:\n",
    "    outlier_cols = cols_with_outliers[\"feature\"].tolist()\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=len(outlier_cols), subplot_titles=outlier_cols)\n",
    "    \n",
    "    for i, col in enumerate(outlier_cols, 1):\n",
    "        fig.add_trace(\n",
    "            go.Box(y=df[col].dropna(), name=col, boxpoints=\"outliers\",\n",
    "                  marker_color=\"#3498db\", showlegend=False),\n",
    "            row=1, col=i\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=400, title_text=\"Outlier Distribution (Box Plots)\",\n",
    "                     template=\"plotly_white\")\n",
    "    display_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3eb31",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.9 Date Logic Validation\n",
    "\n",
    "**üìñ What This Checks:**\n",
    "- Date ranges and suspicious placeholder dates (e.g., 1/1/1900, 1/1/2004)\n",
    "- Date sequence violations if `DATE_SEQUENCE` is configured below\n",
    "\n",
    "**‚ö†Ô∏è Common Issues:**\n",
    "- Very old dates (pre-2005): Often placeholder values ‚Üí Set to NULL\n",
    "- Sequence violations (e.g., `last_purchase < first_purchase`): Data entry errors ‚Üí Flag for review\n",
    "\n",
    "**üí° Configuration:**\n",
    "Set `DATE_SEQUENCE` below to validate that dates occur in expected chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ee2ab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === DATE SEQUENCE CONFIGURATION ===\n",
    "# Define expected chronological order of date columns (earliest to latest)\n",
    "# Example: [\"account_created\", \"first_purchase\", \"last_purchase\"]\n",
    "# Set to None or empty list to skip sequence validation\n",
    "\n",
    "# Option 1: Override here\n",
    "DATE_SEQUENCE = None  # e.g., [\"created\", \"firstorder\", \"lastorder\"]\n",
    "\n",
    "# Option 2: Load from findings (saved in notebook 01)\n",
    "if DATE_SEQUENCE is None and \"date_sequence\" in findings.metadata:\n",
    "    DATE_SEQUENCE = findings.metadata[\"date_sequence\"]\n",
    "    print(f\"Loaded date sequence from findings: {DATE_SEQUENCE}\")\n",
    "\n",
    "# Detect date columns from findings\n",
    "date_cols = [name for name, col in findings.columns.items() \n",
    "             if col.inferred_type == ColumnType.DATETIME]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATE LOGIC VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDetected date columns: {date_cols}\")\n",
    "\n",
    "if date_cols:\n",
    "    df_dates = df.copy()\n",
    "    for col in date_cols:\n",
    "        df_dates[col] = pd.to_datetime(df_dates[col], errors='coerce', format='mixed')\n",
    "    \n",
    "    # Date ranges\n",
    "    print(\"\\nüìÖ DATE RANGES:\")\n",
    "    for col in date_cols:\n",
    "        print(f\"  {col}: {df_dates[col].min()} to {df_dates[col].max()}\")\n",
    "    \n",
    "    # Placeholder detection\n",
    "    print(\"\\nüïµÔ∏è PLACEHOLDER DATE DETECTION:\")\n",
    "    for col in date_cols:\n",
    "        old_dates = (df_dates[col] < '2005-01-01').sum()\n",
    "        if old_dates > 0:\n",
    "            print(f\"  {col}: {old_dates:,} dates before 2005 (possible placeholders)\")\n",
    "        else:\n",
    "            print(f\"  {col}: No suspicious early dates\")\n",
    "    \n",
    "    # Sequence validation\n",
    "    if DATE_SEQUENCE and len(DATE_SEQUENCE) >= 2:\n",
    "        valid_sequence_cols = [c for c in DATE_SEQUENCE if c in date_cols]\n",
    "        if len(valid_sequence_cols) >= 2:\n",
    "            print(f\"\\nüîó DATE SEQUENCE VALIDATION:\")\n",
    "            print(f\"  Expected order: {' ‚â§ '.join(valid_sequence_cols)}\")\n",
    "            \n",
    "            total_violations = 0\n",
    "            for i in range(len(valid_sequence_cols) - 1):\n",
    "                col1, col2 = valid_sequence_cols[i], valid_sequence_cols[i + 1]\n",
    "                # Check where col2 < col1 (violation)\n",
    "                mask = df_dates[col1].notna() & df_dates[col2].notna()\n",
    "                violations = (df_dates.loc[mask, col2] < df_dates.loc[mask, col1]).sum()\n",
    "                total_violations += violations\n",
    "                \n",
    "                if violations > 0:\n",
    "                    pct = violations / mask.sum() * 100\n",
    "                    print(f\"  ‚ö†Ô∏è {col2} < {col1}: {violations:,} violations ({pct:.2f}%)\")\n",
    "                else:\n",
    "                    print(f\"  ‚úì {col1} ‚â§ {col2}: No violations\")\n",
    "            \n",
    "            if total_violations == 0:\n",
    "                print(\"\\n  ‚úÖ All date sequences valid\")\n",
    "            else:\n",
    "                print(f\"\\n  ‚ö†Ô∏è Total sequence violations: {total_violations:,}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è DATE_SEQUENCE columns not found in data: {DATE_SEQUENCE}\")\n",
    "            print(f\"   Available date columns: {date_cols}\")\n",
    "    else:\n",
    "        print(\"\\nüí° TIP: Set DATE_SEQUENCE above or in notebook 01 to enable sequence validation\")\n",
    "        if len(date_cols) >= 2:\n",
    "            print(f\"   Available date columns: {date_cols}\")\n",
    "else:\n",
    "    print(\"\\nNo date columns detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1ae90",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.10 Binary Field Validation\n",
    "\n",
    "Binary fields should contain only 0 and 1 values. Any other values indicate data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f86d8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "binary_cols = [name for name, col in findings.columns.items() \n",
    "               if col.inferred_type == ColumnType.BINARY\n",
    "               and name not in TEMPORAL_METADATA_COLS]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BINARY FIELD VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDetected binary columns: {binary_cols}\")\n",
    "\n",
    "if binary_cols:\n",
    "    binary_results = []\n",
    "    for col in binary_cols:\n",
    "        unique_vals = sorted(df[col].dropna().unique())\n",
    "        is_valid = set(unique_vals).issubset({0, 1, 0.0, 1.0})\n",
    "        count_0 = (df[col] == 0).sum()\n",
    "        count_1 = (df[col] == 1).sum()\n",
    "        total = count_0 + count_1\n",
    "        \n",
    "        binary_results.append({\n",
    "            'column': col,\n",
    "            'unique_values': unique_vals,\n",
    "            'is_valid': is_valid,\n",
    "            'count_0': count_0,\n",
    "            'count_1': count_1,\n",
    "            'pct_1': count_1 / total * 100 if total > 0 else 0\n",
    "        })\n",
    "        \n",
    "        status = \"‚úì\" if is_valid else \"‚ö†Ô∏è\"\n",
    "        print(f\"\\n{status} {col}:\")\n",
    "        print(f\"   Unique values: {unique_vals}\")\n",
    "        print(f\"   0 (No): {count_0:,} ({count_0/total*100:.1f}%)\")\n",
    "        print(f\"   1 (Yes): {count_1:,} ({count_1/total*100:.1f}%)\")\n",
    "        \n",
    "        if not is_valid:\n",
    "            invalid_vals = [v for v in unique_vals if v not in [0, 1, 0.0, 1.0]]\n",
    "            print(f\"   ‚ö†Ô∏è Invalid values found: {invalid_vals}\")\n",
    "\n",
    "    if len(binary_cols) <= 6:\n",
    "        n_cols = len(binary_cols)\n",
    "        fig = make_subplots(rows=1, cols=n_cols, subplot_titles=binary_cols)\n",
    "        \n",
    "        for i, col in enumerate(binary_cols, 1):\n",
    "            counts = df[col].value_counts().sort_index()\n",
    "            fig.add_trace(\n",
    "                go.Bar(x=['No (0)', 'Yes (1)'], y=[counts.get(0, 0), counts.get(1, 0)],\n",
    "                       marker_color=['#d62728', '#2ca02c'], showlegend=False),\n",
    "                row=1, col=i\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(height=350, title_text=\"Binary Field Distributions\",\n",
    "                          template='plotly_white')\n",
    "        display_figure(fig)\n",
    "else:\n",
    "    print(\"\\nNo binary columns detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da2a3b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.11 Data Consistency Checks\n",
    "\n",
    "Check for case variants, leading/trailing spaces, and other string inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ac094",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "consistency_issues = []\n",
    "\n",
    "for col_name in df.select_dtypes(include=['object']).columns:\n",
    "    if col_name in TEMPORAL_METADATA_COLS:\n",
    "        continue\n",
    "    unique_vals = df[col_name].dropna().unique()\n",
    "    case_variants = {}\n",
    "    for val in unique_vals:\n",
    "        lower_val = str(val).lower().strip()\n",
    "        if lower_val not in case_variants:\n",
    "            case_variants[lower_val] = []\n",
    "        case_variants[lower_val].append(val)\n",
    "    \n",
    "    for lower_val, variants in case_variants.items():\n",
    "        if len(variants) > 1:\n",
    "            consistency_issues.append({\n",
    "                \"Column\": col_name,\n",
    "                \"Issue\": \"Case/Spacing Variants\",\n",
    "                \"Details\": str(variants[:5]),\n",
    "                \"variants\": variants\n",
    "            })\n",
    "\n",
    "if consistency_issues:\n",
    "    print(\"Data Consistency Issues:\")\n",
    "    display(pd.DataFrame([{k: v for k, v in issue.items() if k != \"variants\"} for issue in consistency_issues]))\n",
    "    \n",
    "    # Add consistency recommendations\n",
    "    for issue in consistency_issues:\n",
    "        registry.add_bronze_consistency(\n",
    "            column=issue[\"Column\"],\n",
    "            issue_type=\"case_variants\",\n",
    "            action=\"normalize_lower\",\n",
    "            variants=issue[\"variants\"][:5],\n",
    "            rationale=f\"{len(issue['variants'])} case/spacing variants detected\",\n",
    "            source_notebook=\"03_quality_assessment\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No consistency issues detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e03f70",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.12 Quality Improvement Recommendations\n",
    "\n",
    "Automated recommendations based on the issues detected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be853b9d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import RecommendationEngine\n",
    "\n",
    "recommender = RecommendationEngine()\n",
    "cleaning_recs = recommender.recommend_cleaning(findings)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUALITY IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if cleaning_recs:\n",
    "    # Group by severity\n",
    "    high_severity = [r for r in cleaning_recs if r.severity == \"high\"]\n",
    "    medium_severity = [r for r in cleaning_recs if r.severity == \"medium\"]\n",
    "    low_severity = [r for r in cleaning_recs if r.severity == \"low\"]\n",
    "    \n",
    "    if high_severity:\n",
    "        print(\"\\nüî¥ HIGH PRIORITY (must fix before modeling):\")\n",
    "        print(\"-\" * 60)\n",
    "        for rec in high_severity:\n",
    "            print(f\"\\n  üìå {rec.column_name}\")\n",
    "            print(f\"     Issue: {rec.description}\")\n",
    "            print(f\"     Strategy: {rec.strategy_label}\")\n",
    "            print(f\"     Impact: {rec.problem_impact}\")\n",
    "            if rec.action_steps:\n",
    "                print(\"     Action Steps:\")\n",
    "                for step in rec.action_steps:\n",
    "                    print(f\"       ‚Ä¢ {step}\")\n",
    "    \n",
    "    if medium_severity:\n",
    "        print(\"\\nüü° MEDIUM PRIORITY (recommended fixes):\")\n",
    "        print(\"-\" * 60)\n",
    "        for rec in medium_severity:\n",
    "            print(f\"\\n  üìå {rec.column_name}\")\n",
    "            print(f\"     Issue: {rec.description}\")\n",
    "            print(f\"     Strategy: {rec.strategy_label}\")\n",
    "            print(f\"     Impact: {rec.problem_impact}\")\n",
    "            if rec.action_steps:\n",
    "                print(\"     Action Steps:\")\n",
    "                for step in rec.action_steps:\n",
    "                    print(f\"       ‚Ä¢ {step}\")\n",
    "    \n",
    "    if low_severity:\n",
    "        print(\"\\nüü¢ LOW PRIORITY (nice to have):\")\n",
    "        print(\"-\" * 60)\n",
    "        for rec in low_severity:\n",
    "            print(f\"\\n  üìå {rec.column_name}\")\n",
    "            print(f\"     Issue: {rec.description}\")\n",
    "            print(f\"     Strategy: {rec.strategy_label}\")\n",
    "            print(f\"     Impact: {rec.problem_impact}\")\n",
    "    \n",
    "    # Persist cleaning recommendations to registry\n",
    "    for rec in cleaning_recs:\n",
    "        # Check if not already added by previous sections\n",
    "        existing_null = [r for r in registry.bronze.null_handling if r.target_column == rec.column_name]\n",
    "        existing_outlier = [r for r in registry.bronze.outlier_handling if r.target_column == rec.column_name]\n",
    "        \n",
    "        if rec.issue_type in [\"null_values\", \"missing_values\"] and not existing_null:\n",
    "            strategy = \"median\" if \"median\" in rec.strategy.lower() else \"mode\" if \"mode\" in rec.strategy.lower() else \"drop\"\n",
    "            registry.add_bronze_null(\n",
    "                column=rec.column_name,\n",
    "                strategy=strategy,\n",
    "                rationale=rec.description,\n",
    "                source_notebook=\"03_quality_assessment\"\n",
    "            )\n",
    "        elif rec.issue_type == \"outliers\" and not existing_outlier:\n",
    "            registry.add_bronze_outlier(\n",
    "                column=rec.column_name,\n",
    "                action=\"winsorize\" if \"winsor\" in rec.strategy.lower() else \"cap\",\n",
    "                parameters={\"severity\": rec.severity, \"affected_rows\": rec.affected_rows},\n",
    "                rationale=rec.description,\n",
    "                source_notebook=\"03_quality_assessment\"\n",
    "            )\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CLEANUP SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_data = []\n",
    "    for rec in cleaning_recs:\n",
    "        summary_data.append({\n",
    "            \"Column\": rec.column_name,\n",
    "            \"Issue\": rec.issue_type.replace(\"_\", \" \").title(),\n",
    "            \"Severity\": rec.severity.upper(),\n",
    "            \"Recommended Action\": rec.strategy_label,\n",
    "            \"Affected Rows\": f\"{rec.affected_rows:,}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Total impact\n",
    "    total_affected = sum(r.affected_rows for r in cleaning_recs)\n",
    "    unique_affected = min(total_affected, len(df))  # Can't exceed total rows\n",
    "    print(f\"\\nTotal potentially affected: {total_affected:,} cell values\")\n",
    "    print(f\"Columns needing attention: {len(cleaning_recs)}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No cleaning recommendations - data quality is excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4be59e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3.13 Save Updated Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c482b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save updated findings back to the same file\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"Updated findings saved to: {FINDINGS_PATH}\")\n",
    "\n",
    "# Save recommendations registry\n",
    "with open(RECOMMENDATIONS_PATH, \"w\") as f:\n",
    "    yaml.dump(registry.to_dict(), f, default_flow_style=False, sort_keys=False)\n",
    "print(f\"Recommendations saved to: {RECOMMENDATIONS_PATH}\")\n",
    "\n",
    "# Summary of recommendations\n",
    "all_recs = registry.all_recommendations\n",
    "print(f\"\\nüìã Recommendations Summary:\")\n",
    "print(f\"   Bronze layer: {len(registry.get_by_layer('bronze'))} recommendations\")\n",
    "if registry.silver:\n",
    "    print(f\"   Silver layer: {len(registry.get_by_layer('silver'))} recommendations\")\n",
    "if registry.gold:\n",
    "    print(f\"   Gold layer: {len(registry.get_by_layer('gold'))} recommendations\")\n",
    "print(f\"   Total: {len(all_recs)} recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf981d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "In this notebook, we performed a comprehensive quality assessment:\n",
    "\n",
    "1. **Duplicate Analysis** - Identified key-based duplicates, exact duplicates, and value conflicts\n",
    "2. **Target Variable** - Analyzed class distribution and imbalance for modeling guidance\n",
    "3. **Missing Values** - Analyzed patterns (MCAR/MAR/MNAR) and correlations\n",
    "4. **Segment-Aware Outliers** - Detected natural data segments to avoid false positive outliers\n",
    "5. **Global Outliers** - Detected using IQR method with bounds and percentages\n",
    "6. **Date Logic** - Validated temporal sequences and detected placeholders\n",
    "7. **Binary Fields** - Verified 0/1 encoding and distributions\n",
    "8. **Consistency** - Checked for case variants and spacing issues\n",
    "9. **Recommendations** - Generated automated cleaning strategies\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "**Duplicate Analysis:**\n",
    "- Exact duplicates should be removed before modeling\n",
    "- Key duplicates with value conflicts require investigation and a resolution strategy\n",
    "- High duplicate percentages may indicate event-level data requiring aggregation\n",
    "\n",
    "**Segment-Aware Analysis:**\n",
    "- If segments were detected, some global \"outliers\" may actually be valid data from different customer segments\n",
    "- Enterprise vs retail customers, new vs established accounts often have legitimately different value ranges\n",
    "- Use segment-specific outlier treatment when recommended to preserve important patterns\n",
    "\n",
    "## Key Cleanup Actions for This Dataset\n",
    "\n",
    "Based on the analysis above:\n",
    "- **Duplicates**: Review key duplicates and resolve value conflicts\n",
    "- **Missing Values**: Low (0.06%) - can drop or impute with mode\n",
    "- **Outliers**: Check segment-aware analysis results - some may be false positives\n",
    "- **Date Issues**: Check for placeholder dates before lastorder\n",
    "- **Binary Fields**: All valid with 0/1 encoding\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **04_relationship_analysis.ipynb** to:\n",
    "- Explore correlations between features\n",
    "- Analyze feature-target relationships\n",
    "- Identify potential feature interactions\n",
    "- Detect multicollinearity issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.442262,
   "end_time": "2026-01-22T14:18:05.634196",
   "environment_variables": {},
   "exception": true,
   "input_path": "exploration_notebooks/03_quality_assessment.ipynb",
   "output_path": "docs/tutorial/executed/03_quality_assessment.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T14:18:03.191934",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}