{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ba4a92",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [3]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.00269,
     "end_time": "2026-01-22T14:17:38.145645",
     "exception": false,
     "start_time": "2026-01-22T14:17:38.142955",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Start Here: Data Discovery\n",
    "\n",
    "**Purpose:** Create a point-in-time snapshot and understand your dataset's structure through automatic profiling.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to create temporally-safe training snapshots\n",
    "- How automatic type inference works and when to override it\n",
    "- How to identify entity-level vs event-level data\n",
    "- How to set up your target column for downstream analysis\n",
    "\n",
    "**Outputs:**\n",
    "- Point-in-time training snapshot (Parquet)\n",
    "- Dataset overview (rows, columns, memory, format, structure)\n",
    "- Automatic column type inference with confidence scores\n",
    "- Saved exploration findings (YAML)\n",
    "\n",
    "---\n",
    "\n",
    "## How to Read This Notebook\n",
    "\n",
    "Each section includes:\n",
    "- **ðŸ“Š Charts** - Interactive Plotly visualizations\n",
    "- **ðŸ“– Interpretation Guide** - How to read and understand the output\n",
    "- **âœ… Actions** - What to do based on the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {
    "papermill": {
     "duration": 0.001525,
     "end_time": "2026-01-22T14:17:38.148939",
     "exception": false,
     "start_time": "2026-01-22T14:17:38.147414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Configuration\n",
    "\n",
    "Configure your data source and target column **before** running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:17:38.152819Z",
     "iopub.status.busy": "2026-01-22T14:17:38.152705Z",
     "iopub.status.idle": "2026-01-22T14:17:39.785431Z",
     "shell.execute_reply": "2026-01-22T14:17:39.784945Z"
    },
    "papermill": {
     "duration": 1.635894,
     "end_time": "2026-01-22T14:17:39.786346",
     "exception": false,
     "start_time": "2026-01-22T14:17:38.150452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import DataExplorer\n",
    "from customer_retention.analysis.auto_explorer.findings import TimeSeriesMetadata\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table, console\n",
    "from customer_retention.stages.validation import TimeSeriesDetector\n",
    "from customer_retention.core.config.column_config import DatasetGranularity, ColumnType\n",
    "from customer_retention.stages.profiling import TypeDetector\n",
    "from customer_retention.stages.temporal import (\n",
    "    ScenarioDetector, UnifiedDataPreparer, SnapshotManager,\n",
    "    TimestampConfig, TimestampStrategy, PointInTimeRegistry, CutoffAnalyzer\n",
    ")\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:17:39.790448Z",
     "iopub.status.busy": "2026-01-22T14:17:39.790258Z",
     "iopub.status.idle": "2026-01-22T14:17:39.792965Z",
     "shell.execute_reply": "2026-01-22T14:17:39.792631Z"
    },
    "papermill": {
     "duration": 0.005525,
     "end_time": "2026-01-22T14:17:39.793599",
     "exception": false,
     "start_time": "2026-01-22T14:17:39.788074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Set these before running\n",
    "# =============================================================================\n",
    "\n",
    "# DATA_PATH: Path to your data file (CSV, Parquet, or Delta)\n",
    "DATA_PATH = \"../tests/fixtures/customer_retention_retail.csv\"\n",
    "\n",
    "# TARGET_COLUMN: Your prediction target (set to None for auto-detection)\n",
    "TARGET_COLUMN = \"retained\"\n",
    "\n",
    "# ENTITY_COLUMN: Customer/user ID column (set to None for auto-detection)\n",
    "ENTITY_COLUMN = None\n",
    "\n",
    "# LABEL_WINDOW_DAYS: Days after last activity to derive label timestamp\n",
    "# Used when no explicit label timestamp column exists (e.g., churn_date)\n",
    "# Default: 180 days (6 months observation window)\n",
    "LABEL_WINDOW_DAYS = 180\n",
    "\n",
    "# TIMESTAMP_CONFIG: Override auto-detection if needed (set to None for auto-detection)\n",
    "# Example manual override:\n",
    "# TIMESTAMP_CONFIG = TimestampConfig(\n",
    "#     strategy=TimestampStrategy.PRODUCTION,\n",
    "#     feature_timestamp_column=\"observation_date\",\n",
    "#     label_timestamp_column=\"churn_date\",\n",
    "# )\n",
    "TIMESTAMP_CONFIG = None\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLE DATASETS (for learning/testing only)\n",
    "# =============================================================================\n",
    "# ENTITY-LEVEL (one row per customer):\n",
    "# DATA_PATH = \"../tests/fixtures/customer_retention_retail.csv\"\n",
    "# DATA_PATH = \"../tests/fixtures/bank_customer_churn.csv\"\n",
    "# DATA_PATH = \"../tests/fixtures/netflix_customer_churn.csv\"\n",
    "#\n",
    "# EVENT-LEVEL (multiple rows per customer):\n",
    "# DATA_PATH = \"../tests/fixtures/customer_transactions.csv\"\n",
    "# DATA_PATH = \"../tests/fixtures/customer_emails.csv\"\n",
    "# =============================================================================\n",
    "\n",
    "# OUTPUT_DIR: All outputs go here (gitignored)\n",
    "OUTPUT_DIR = Path(\"../experiments/findings\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {
    "papermill": {
     "duration": 0.001477,
     "end_time": "2026-01-22T14:17:39.796709",
     "exception": false,
     "start_time": "2026-01-22T14:17:39.795232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Load Data & Create Point-in-Time Snapshot\n",
    "\n",
    "**This is the critical first step.** We:\n",
    "1. Load raw data\n",
    "2. Detect temporal scenario (production timestamps, derived, or synthetic)\n",
    "3. Create a versioned snapshot with `feature_timestamp` and `label_timestamp`\n",
    "4. All subsequent analysis uses the snapshot data\n",
    "\n",
    "This ensures temporal integrity and prevents data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8de124",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:17:39.800678Z",
     "iopub.status.busy": "2026-01-22T14:17:39.800568Z",
     "iopub.status.idle": "2026-01-22T14:17:40.114776Z",
     "shell.execute_reply": "2026-01-22T14:17:40.114288Z"
    },
    "papermill": {
     "duration": 0.317464,
     "end_time": "2026-01-22T14:17:40.115804",
     "exception": true,
     "start_time": "2026-01-22T14:17:39.798340",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../tests/fixtures/customer_retention_retail.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load raw data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m raw_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m DATA_PATH.endswith(\u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m pd.read_parquet(DATA_PATH)\n\u001b[32m      4\u001b[39m console.start_section()\n\u001b[32m      5\u001b[39m console.header(\u001b[33m\"\u001b[39m\u001b[33mRaw Data Loaded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/CustomerRetention/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/CustomerRetention/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/CustomerRetention/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/CustomerRetention/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python/CustomerRetention/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../tests/fixtures/customer_retention_retail.csv'"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "raw_df = pd.read_csv(DATA_PATH) if DATA_PATH.endswith('.csv') else pd.read_parquet(DATA_PATH)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Raw Data Loaded\")\n",
    "console.metric(\"Source\", DATA_PATH)\n",
    "console.metric(\"Rows\", f\"{len(raw_df):,}\")\n",
    "console.metric(\"Columns\", len(raw_df.columns))\n",
    "console.end_section()\n",
    "\n",
    "# Detect granularity and entity column\n",
    "type_detector = TypeDetector()\n",
    "granularity_result = type_detector.detect_granularity(raw_df)\n",
    "entity_column = ENTITY_COLUMN or granularity_result.entity_column\n",
    "\n",
    "# Detect or use provided timestamp configuration\n",
    "if TIMESTAMP_CONFIG:\n",
    "    ts_config = TIMESTAMP_CONFIG\n",
    "    scenario = \"MANUAL_OVERRIDE\"\n",
    "    discovery_result = None\n",
    "    console.info(f\"Using manual timestamp config: {ts_config.strategy.value}\")\n",
    "else:\n",
    "    detector = ScenarioDetector(label_window_days=LABEL_WINDOW_DAYS)\n",
    "    scenario, ts_config, discovery_result = detector.detect(raw_df, TARGET_COLUMN)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Temporal Scenario Detection\")\n",
    "console.metric(\"Scenario\", scenario)\n",
    "console.metric(\"Strategy\", ts_config.strategy.value)\n",
    "console.metric(\"Label Window\", f\"{LABEL_WINDOW_DAYS} days\")\n",
    "\n",
    "if discovery_result:\n",
    "    if discovery_result.feature_timestamp:\n",
    "        source_col = discovery_result.feature_timestamp.column_name\n",
    "        if discovery_result.feature_timestamp.is_derived:\n",
    "            console.metric(\"Feature Timestamp\", f\"derived from {discovery_result.feature_timestamp.source_columns}\")\n",
    "        else:\n",
    "            was_promoted = \"promoted\" in discovery_result.feature_timestamp.notes.lower()\n",
    "            if was_promoted:\n",
    "                console.metric(\"Feature Timestamp\", f\"{source_col} (auto-selected as latest activity)\")\n",
    "            else:\n",
    "                console.metric(\"Feature Timestamp\", f\"{source_col} (explicit match)\")\n",
    "    \n",
    "    if discovery_result.label_timestamp:\n",
    "        if discovery_result.label_timestamp.is_derived:\n",
    "            console.metric(\"Label Timestamp\", f\"derived: {discovery_result.label_timestamp.derivation_formula}\")\n",
    "        else:\n",
    "            console.metric(\"Label Timestamp\", f\"{discovery_result.label_timestamp.column_name} (explicit match)\")\n",
    "    \n",
    "    if \"datetime_ordering\" in discovery_result.discovery_report:\n",
    "        ordering = discovery_result.discovery_report[\"datetime_ordering\"]\n",
    "        if ordering:\n",
    "            console.info(f\"Datetime column ordering: {' â†’ '.join(ordering)}\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlzs6bfy2z",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Cutoff Date Selection\n",
    "\n",
    "The chart below shows the temporal distribution of your data. Use it to select an appropriate cutoff date:\n",
    "\n",
    "- **Top chart**: Records per time bin and cumulative count\n",
    "- **Bottom chart**: Train/Score split percentage at each potential cutoff date\n",
    "- **Suggested cutoff** (blue dashed): Achieves ~90% train / 10% score split\n",
    "\n",
    "**Final data allocation:**\n",
    "- Cutoff: 90% train, 10% score (holdout for final evaluation)\n",
    "- Train/Test split: 89% train, 11% test (from the 90%)\n",
    "- **Result: ~80% training, ~10% test, ~10% score**\n",
    "\n",
    "Adjust `CUTOFF_DATE` below if the suggested date doesn't fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lfuwm8yytw",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze temporal distribution for cutoff selection\n",
    "cutoff_analyzer = CutoffAnalyzer()\n",
    "cutoff_analysis = None\n",
    "\n",
    "# Get timestamp column from discovery result (reuse existing analysis)\n",
    "timestamp_col = None\n",
    "if discovery_result:\n",
    "    if \"datetime_ordering\" in discovery_result.discovery_report:\n",
    "        ordering = discovery_result.discovery_report[\"datetime_ordering\"]\n",
    "        if ordering:\n",
    "            timestamp_col = ordering[-1]  # Latest datetime column\n",
    "    if not timestamp_col and discovery_result.feature_timestamp:\n",
    "        if not discovery_result.feature_timestamp.is_derived:\n",
    "            timestamp_col = discovery_result.feature_timestamp.column_name\n",
    "\n",
    "# Check registry for existing cutoff\n",
    "pit_registry = PointInTimeRegistry(OUTPUT_DIR)\n",
    "registry_cutoff = pit_registry.check_consistency().reference_cutoff\n",
    "\n",
    "if timestamp_col:\n",
    "    cutoff_analysis = cutoff_analyzer.analyze(raw_df, timestamp_column=timestamp_col, n_bins=50)\n",
    "    data_suggested_cutoff = cutoff_analysis.suggest_cutoff(train_ratio=0.9)\n",
    "    \n",
    "    console.start_section()\n",
    "    console.header(\"Cutoff Date Analysis\")\n",
    "    console.metric(\"Timestamp Column\", timestamp_col)\n",
    "    console.metric(\"Date Range\", f\"{cutoff_analysis.date_range[0].strftime('%Y-%m-%d')} to {cutoff_analysis.date_range[1].strftime('%Y-%m-%d')}\")\n",
    "    console.metric(\"Data-Suggested Cutoff\", data_suggested_cutoff.strftime(\"%Y-%m-%d\"))\n",
    "    split = cutoff_analysis.get_split_at_date(data_suggested_cutoff)\n",
    "    console.metric(\"At Suggested Split\", f\"{split['train_pct']:.0f}% train / {split['score_pct']:.0f}% score\")\n",
    "    \n",
    "    if registry_cutoff:\n",
    "        console.warning(f\"Registry has cutoff: {registry_cutoff.date()} (may be stale)\")\n",
    "        console.info(\"To clear: pit_registry.clear_registry()\")\n",
    "    \n",
    "    # Show milestones for reference\n",
    "    milestones = cutoff_analysis.get_percentage_milestones(step=10)\n",
    "    if milestones:\n",
    "        console.subheader(\"Reference Dates (10% intervals)\")\n",
    "        for m in milestones:\n",
    "            console.info(f\"  {m['train_pct']:.0f}% train: {m['date'].strftime('%Y-%m-%d')}\")\n",
    "    console.end_section()\n",
    "else:\n",
    "    data_suggested_cutoff = datetime.now()\n",
    "    console.start_section()\n",
    "    console.header(\"Cutoff Date Analysis\")\n",
    "    console.warning(\"No timestamp column detected\")\n",
    "    console.metric(\"Default Cutoff\", data_suggested_cutoff.strftime(\"%Y-%m-%d\"))\n",
    "    if registry_cutoff:\n",
    "        console.info(f\"Registry cutoff: {registry_cutoff.date()}\")\n",
    "    console.end_section()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ls2ezi1t5ag",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUTOFF DATE SELECTION - Set your preferred cutoff date\n",
    "# =============================================================================\n",
    "# Options:\n",
    "#   None = use data-suggested cutoff (~90/10 split)\n",
    "#   datetime(YYYY, M, D) = use specific date\n",
    "#\n",
    "# To clear stale registry: pit_registry.clear_registry()\n",
    "# =============================================================================\n",
    "CUTOFF_DATE = None  # e.g., datetime(2017, 7, 1)\n",
    "\n",
    "# Compute final selected cutoff\n",
    "selected_cutoff = CUTOFF_DATE or data_suggested_cutoff\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Selected Cutoff Date\")\n",
    "if CUTOFF_DATE:\n",
    "    console.info(f\"Manual override: {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    console.info(f\"Using data-suggested: {selected_cutoff.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "if cutoff_analysis:\n",
    "    split = cutoff_analysis.get_split_at_date(selected_cutoff)\n",
    "    console.metric(\"Train/Score Split\", f\"{split['train_pct']:.0f}% / {split['score_pct']:.0f}%\")\n",
    "    console.metric(\"Train Records\", f\"{split['train_count']:,}\")\n",
    "    console.metric(\"Score Records\", f\"{split['score_count']:,}\")\n",
    "console.end_section()\n",
    "\n",
    "# Display chart with selected cutoff\n",
    "if cutoff_analysis:\n",
    "    chart_builder = ChartBuilder()\n",
    "    display_figure(chart_builder.cutoff_selection_chart(\n",
    "        cutoff_analysis, \n",
    "        suggested_cutoff=selected_cutoff,\n",
    "        current_cutoff=registry_cutoff\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pit_registry already initialized in cutoff analysis cell\n",
    "dataset_name = Path(DATA_PATH).stem\n",
    "\n",
    "# Use the user's selected cutoff (not forced by registry)\n",
    "cutoff_date = selected_cutoff\n",
    "\n",
    "# Warn if overriding registry\n",
    "if registry_cutoff and registry_cutoff.date() != selected_cutoff.date():\n",
    "    console.start_section()\n",
    "    console.header(\"Registry Update\")\n",
    "    console.warning(f\"Overriding registry cutoff ({registry_cutoff.date()}) with {selected_cutoff.date()}\")\n",
    "    console.info(\"All datasets in this project should use the same cutoff date\")\n",
    "    console.end_section()\n",
    "\n",
    "preparer = UnifiedDataPreparer(OUTPUT_DIR, ts_config)\n",
    "df = preparer.prepare_from_raw(raw_df, target_column=TARGET_COLUMN, entity_column=entity_column or \"entity_id\")\n",
    "snapshot_df, snapshot_metadata = preparer.create_training_snapshot(df, cutoff_date)\n",
    "\n",
    "pit_registry.register_snapshot(\n",
    "    dataset_name=dataset_name,\n",
    "    snapshot_id=snapshot_metadata['snapshot_id'],\n",
    "    cutoff_date=cutoff_date,\n",
    "    source_path=DATA_PATH,\n",
    "    row_count=snapshot_metadata['row_count']\n",
    ")\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Point-in-Time Snapshot Created\")\n",
    "console.metric(\"Dataset\", dataset_name)\n",
    "console.metric(\"Snapshot ID\", snapshot_metadata['snapshot_id'])\n",
    "console.metric(\"Rows\", f\"{snapshot_metadata['row_count']:,}\")\n",
    "console.metric(\"Features\", len(snapshot_metadata['feature_columns']))\n",
    "console.metric(\"Cutoff Date\", str(cutoff_date.date()))\n",
    "console.metric(\"Data Hash\", snapshot_metadata['data_hash'][:16] + \"...\")\n",
    "\n",
    "if \"feature_timestamp\" in df.columns:\n",
    "    console.success(\"Temporal columns added: feature_timestamp, label_timestamp\")\n",
    "else:\n",
    "    console.warning(\"No temporal columns added (synthetic strategy)\")\n",
    "\n",
    "updated_report = pit_registry.check_consistency()\n",
    "if updated_report.is_consistent:\n",
    "    console.success(f\"All {len(pit_registry.snapshots)} datasets use cutoff: {cutoff_date.date()}\")\n",
    "else:\n",
    "    console.error(\"INCONSISTENT CUTOFF DATES DETECTED\")\n",
    "    console.warning(f\"Out of sync: {', '.join(updated_report.inconsistent_datasets)}\")\n",
    "    console.info(\"Re-run notebook 01 for out-of-sync datasets to align cutoff dates\")\n",
    "\n",
    "console.end_section()\n",
    "\n",
    "df = snapshot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Dataset Exploration\n",
    "\n",
    "Now we explore the **snapshot data** (not raw data). This ensures all visualizations and metrics reflect the actual training data with temporal integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explore the snapshot data\n",
    "# Note: UnifiedDataPreparer renames the target column to \"target\" in the snapshot\n",
    "# So we use \"target\" as the hint, not the original TARGET_COLUMN name\n",
    "explorer = DataExplorer(visualize=False, save_findings=True, output_dir=str(OUTPUT_DIR))\n",
    "findings = explorer.explore(df, target_hint=\"target\", name=dataset_name)\n",
    "findings.source_path = DATA_PATH\n",
    "\n",
    "# Store snapshot info in findings\n",
    "findings.snapshot_id = snapshot_metadata['snapshot_id']\n",
    "findings.snapshot_path = str(OUTPUT_DIR / \"snapshots\" / f\"{snapshot_metadata['snapshot_id']}.parquet\")\n",
    "findings.timestamp_scenario = scenario\n",
    "findings.timestamp_strategy = ts_config.strategy.value\n",
    "\n",
    "# Also store the original target column name for reference\n",
    "findings.metadata[\"original_target_column\"] = TARGET_COLUMN\n",
    "\n",
    "granularity = \"event\" if granularity_result.granularity == DatasetGranularity.EVENT_LEVEL else \"entity\"\n",
    "\n",
    "# Display dataset overview\n",
    "chart_builder = ChartBuilder()\n",
    "display_figure(chart_builder.dataset_at_a_glance(\n",
    "    df, findings,\n",
    "    source_path=f\"Snapshot: {snapshot_metadata['snapshot_id']}\",\n",
    "    granularity=granularity,\n",
    "    max_columns=15,\n",
    "    columns_per_row=5\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.4 Column Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exclude temporal metadata columns from summary\n",
    "TEMPORAL_METADATA_COLS = {\"feature_timestamp\", \"label_timestamp\", \"label_available_flag\"}\n",
    "\n",
    "summary_data = []\n",
    "for name, col in findings.columns.items():\n",
    "    if name in TEMPORAL_METADATA_COLS:\n",
    "        continue\n",
    "    null_pct = col.universal_metrics.get(\"null_percentage\", 0)\n",
    "    distinct = col.universal_metrics.get(\"distinct_count\", \"N/A\")\n",
    "    summary_data.append({\n",
    "        \"Column\": name,\n",
    "        \"Type\": col.inferred_type.value,\n",
    "        \"Confidence\": f\"{col.confidence:.0%}\",\n",
    "        \"Nulls %\": f\"{null_pct:.1f}%\",\n",
    "        \"Distinct\": distinct,\n",
    "        \"Evidence\": col.evidence[0] if col.evidence else \"\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display_table(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.5 Target Column Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "console.start_section()\n",
    "console.header(\"Target Column\")\n",
    "\n",
    "if findings.target_column and findings.target_column in df.columns:\n",
    "    console.success(f\"Target: {findings.target_column}\")\n",
    "    target_counts = df[findings.target_column].value_counts()\n",
    "    for val, count in target_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        console.metric(f\"Class {val}\", f\"{count:,} ({pct:.1f}%)\")\n",
    "else:\n",
    "    console.warning(\"No target column configured\")\n",
    "    console.info(\"Set TARGET_COLUMN in the configuration cell above\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.6 Dataset Structure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_detector = TimeSeriesDetector()\n",
    "ts_characteristics = ts_detector.detect(df, entity_column=entity_column)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Dataset Structure\")\n",
    "console.metric(\"Type\", ts_characteristics.dataset_type.value.upper())\n",
    "console.metric(\"Granularity\", granularity_result.granularity.value.upper())\n",
    "console.metric(\"Entity Column\", entity_column or \"N/A\")\n",
    "\n",
    "if granularity_result.unique_entities:\n",
    "    console.metric(\"Unique Entities\", f\"{granularity_result.unique_entities:,}\")\n",
    "if granularity_result.avg_events_per_entity:\n",
    "    console.metric(\"Avg Events/Entity\", f\"{granularity_result.avg_events_per_entity:.1f}\")\n",
    "\n",
    "is_event_level = granularity_result.granularity == DatasetGranularity.EVENT_LEVEL\n",
    "if is_event_level:\n",
    "    console.info(\"EVENT-LEVEL DATA - Use Event Bronze Track:\")\n",
    "    console.info(\"  -> 01a_temporal_deep_dive.ipynb\")\n",
    "    console.info(\"  -> 01b_temporal_quality.ipynb\")\n",
    "    console.info(\"  -> 01c_temporal_patterns.ipynb\")\n",
    "    console.info(\"  -> 01d_event_aggregation.ipynb\")\n",
    "else:\n",
    "    console.info(\"ENTITY-LEVEL DATA - Use standard flow:\")\n",
    "    console.info(\"  -> 02_column_deep_dive.ipynb\")\n",
    "    console.info(\"  -> 03_quality_assessment.ipynb\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.7 Type Override (Optional)\n",
    "\n",
    "Override any incorrectly inferred column types before saving findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === TYPE OVERRIDES ===\n",
    "TYPE_OVERRIDES = {\n",
    "    # \"column_name\": ColumnType.NEW_TYPE,\n",
    "}\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Type Override Review\")\n",
    "\n",
    "low_conf = [(name, col.inferred_type.value, col.confidence) \n",
    "            for name, col in findings.columns.items() \n",
    "            if col.confidence < 0.8 and name not in TEMPORAL_METADATA_COLS]\n",
    "if low_conf:\n",
    "    console.subheader(\"Low Confidence Detections\")\n",
    "    for col_name, col_type, conf in sorted(low_conf, key=lambda x: x[2]):\n",
    "        console.warning(f\"{col_name}: {col_type} ({conf:.0%})\")\n",
    "else:\n",
    "    console.success(\"All type detections have high confidence (>=80%)\")\n",
    "\n",
    "if TYPE_OVERRIDES:\n",
    "    console.subheader(\"Applying Overrides\")\n",
    "    for col_name, new_type in TYPE_OVERRIDES.items():\n",
    "        if col_name in findings.columns:\n",
    "            old_type = findings.columns[col_name].inferred_type.value\n",
    "            findings.columns[col_name].inferred_type = new_type\n",
    "            findings.columns[col_name].confidence = 1.0\n",
    "            console.success(f\"{col_name}: {old_type} -> {new_type.value}\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.8 Save Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Populate time series metadata if event-level\n",
    "if is_event_level:\n",
    "    findings.time_series_metadata = TimeSeriesMetadata(\n",
    "        granularity=DatasetGranularity.EVENT_LEVEL,\n",
    "        entity_column=entity_column,\n",
    "        time_column=granularity_result.time_column or ts_characteristics.timestamp_column,\n",
    "        avg_events_per_entity=granularity_result.avg_events_per_entity,\n",
    "        time_span_days=int(ts_characteristics.time_span_days) if ts_characteristics.time_span_days else None,\n",
    "        unique_entities=granularity_result.unique_entities,\n",
    "        suggested_aggregations=[\"24h\", \"7d\", \"30d\", \"90d\", \"all_time\"]\n",
    "    )\n",
    "\n",
    "FINDINGS_PATH = explorer.last_findings_path\n",
    "findings.save(FINDINGS_PATH)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Findings Saved\")\n",
    "console.success(f\"Findings: {FINDINGS_PATH}\")\n",
    "console.success(f\"Snapshot: {findings.snapshot_path}\")\n",
    "console.metric(\"Columns\", findings.column_count)\n",
    "console.metric(\"Target\", findings.target_column or \"Not set\")\n",
    "console.metric(\"Snapshot ID\", findings.snapshot_id)\n",
    "console.metric(\"Timestamp Strategy\", findings.timestamp_strategy)\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1.9 Summary\n",
    "\n",
    "**What was created:**\n",
    "- Point-in-time snapshot with `feature_timestamp` and `label_timestamp`\n",
    "- Exploration findings with column types and metrics\n",
    "\n",
    "**All downstream notebooks load the snapshot**, ensuring:\n",
    "- Temporal integrity (no data leakage)\n",
    "- Reproducibility (SHA256 hash verification)\n",
    "- Consistency (same data across all analysis)\n",
    "\n",
    "**Next steps:**\n",
    "- Entity-level data: `02_column_deep_dive.ipynb`\n",
    "- Event-level data: `01a_temporal_deep_dive.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.815449,
   "end_time": "2026-01-22T14:17:40.435103",
   "environment_variables": {},
   "exception": true,
   "input_path": "exploration_notebooks/01_data_discovery.ipynb",
   "output_path": "docs/tutorial/executed/01_data_discovery.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T14:17:37.619654",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}