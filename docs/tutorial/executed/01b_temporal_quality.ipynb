{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "011eb771",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f02889",
   "metadata": {
    "papermill": {
     "duration": 0.003948,
     "end_time": "2026-01-22T14:17:46.131859",
     "exception": false,
     "start_time": "2026-01-22T14:17:46.127911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chapter 1b: Temporal Quality Assessment (Event Bronze Track)\n",
    "\n",
    "**Purpose:** Run quality checks specific to event-level (time series) datasets to identify data issues before feature engineering.\n",
    "\n",
    "**When to use this notebook:**\n",
    "- After completing 01a_temporal_deep_dive.ipynb\n",
    "- Your dataset is EVENT_LEVEL granularity\n",
    "- You want to validate temporal data integrity\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to detect duplicate events (same entity + timestamp)\n",
    "- How to find unexpected temporal gaps\n",
    "- How to identify future dates (data quality issue)\n",
    "- How to check for event ordering ambiguities\n",
    "\n",
    "**Quality Checks Performed:**\n",
    "\n",
    "| Check ID | Name | Severity | Description |\n",
    "|----------|------|----------|-------------|\n",
    "| TQ001 | Duplicate Events | MEDIUM | Same entity with identical timestamp |\n",
    "| TQ002 | Temporal Gaps | MEDIUM | Unexpected missing time periods |\n",
    "| TQ003 | Future Dates | HIGH | Dates beyond reference date |\n",
    "| TQ004 | Event Ordering | LOW | Ambiguous ordering (timestamp collisions) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f38fb2",
   "metadata": {
    "papermill": {
     "duration": 0.002691,
     "end_time": "2026-01-22T14:17:46.137952",
     "exception": false,
     "start_time": "2026-01-22T14:17:46.135261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1b.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f703a3e6",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65830dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:17:46.143450Z",
     "iopub.status.busy": "2026-01-22T14:17:46.143321Z",
     "iopub.status.idle": "2026-01-22T14:17:47.637080Z",
     "shell.execute_reply": "2026-01-22T14:17:47.636488Z"
    },
    "papermill": {
     "duration": 1.497572,
     "end_time": "2026-01-22T14:17:47.637879",
     "exception": true,
     "start_time": "2026-01-22T14:17:46.140307",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": "from customer_retention.analysis.auto_explorer import ExplorationFindings, RecommendationEngine\nfrom customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\nfrom customer_retention.core.config.column_config import ColumnType, DatasetGranularity\nfrom customer_retention.core.components.enums import Severity\nfrom customer_retention.stages.profiling import (\n    TemporalQualityCheck, TemporalQualityResult,\n    DuplicateEventCheck, TemporalGapCheck, FutureDateCheck, EventOrderCheck,\n    NumericProfiler, CategoricalProfiler,\n    SegmentAwareOutlierAnalyzer\n)\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71848354",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "from pathlib import Path\n",
    "\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "findings_files = [f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") if \"multi_dataset\" not in f.name]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Loaded findings for {findings.column_count} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556df30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify time series dataset and get column names\n",
    "if not findings.is_time_series:\n",
    "    print(\"\\u26a0\\ufe0f Warning: This dataset was not detected as time series.\")\n",
    "    print(\"   Use 03_quality_assessment.ipynb for entity-level datasets.\")\n",
    "\n",
    "ts_meta = findings.time_series_metadata\n",
    "ENTITY_COLUMN = ts_meta.entity_column if ts_meta else None\n",
    "TIME_COLUMN = ts_meta.time_column if ts_meta else None\n",
    "\n",
    "print(f\"Entity column: {ENTITY_COLUMN}\")\n",
    "print(f\"Time column: {TIME_COLUMN}\")\n",
    "\n",
    "if not ENTITY_COLUMN or not TIME_COLUMN:\n",
    "    raise ValueError(\"Please run 01a_temporal_deep_dive.ipynb first to set entity/time columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2643e9f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\n",
    "charts = ChartBuilder()\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows x {len(df.columns)} columns\")\n",
    "print(f\"Data source: {data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db83397",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.2 Configure Quality Checks\n",
    "\n",
    "You can customize the check parameters based on your data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6062e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === QUALITY CHECK CONFIGURATION ===\n",
    "\n",
    "# Reference date for future date check (default: now)\n",
    "REFERENCE_DATE = pd.Timestamp.now()\n",
    "# REFERENCE_DATE = pd.Timestamp(\"2024-01-01\")  # Use fixed date if needed\n",
    "\n",
    "# Expected data frequency for gap detection\n",
    "# Options: \"D\" (daily), \"W\" (weekly), \"M\" (monthly), \"H\" (hourly)\n",
    "EXPECTED_FREQUENCY = \"D\"\n",
    "\n",
    "# Maximum gap multiplier (gaps > expected * multiplier are flagged)\n",
    "MAX_GAP_MULTIPLE = 3.0\n",
    "\n",
    "print(\"Quality Check Configuration:\")\n",
    "print(f\"   Reference date: {REFERENCE_DATE}\")\n",
    "print(f\"   Expected frequency: {EXPECTED_FREQUENCY}\")\n",
    "print(f\"   Max gap multiple: {MAX_GAP_MULTIPLE}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0184b144",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.3 Run All Temporal Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b08fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize all checks\n",
    "checks = [\n",
    "    DuplicateEventCheck(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN),\n",
    "    TemporalGapCheck(time_column=TIME_COLUMN, expected_frequency=EXPECTED_FREQUENCY, max_gap_multiple=MAX_GAP_MULTIPLE),\n",
    "    FutureDateCheck(time_column=TIME_COLUMN, reference_date=REFERENCE_DATE),\n",
    "    EventOrderCheck(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN),\n",
    "]\n",
    "\n",
    "# Run all checks\n",
    "results = []\n",
    "for check in checks:\n",
    "    result = check.run(df)\n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEMPORAL QUALITY CHECK RESULTS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26d5f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Display summary\npassed = sum(1 for r in results if r.passed)\nfailed = len(results) - passed\n\nseverity_colors = {\n    Severity.HIGH: \"\\U0001f534\",\n    Severity.MEDIUM: \"\\U0001f7e0\",\n    Severity.LOW: \"\\U0001f7e1\",\n    Severity.INFO: \"\\U0001f535\",\n}\n\nprint(f\"\\n\\U0001f4cb Summary: {passed}/{len(results)} checks passed\\n\")\n\nfor result in results:\n    status = \"\\u2705\" if result.passed else \"\\u274c\"\n    severity_icon = severity_colors.get(result.severity, \"\\u26aa\")\n    print(f\"{status} [{result.check_id}] {result.check_name}\")\n    print(f\"   {severity_icon} Severity: {result.severity.value}\")\n    print(f\"   Message: {result.message}\")\n    if result.recommendation:\n        print(f\"   \\U0001f4a1 {result.recommendation}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "d0e44975",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.4 Detailed Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640631ca",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Consolidated quality summary with context\ntotal_rows = len(df)\n\ncheck_data = []\nfor r in results:\n    issue_count = r.duplicate_count or r.gap_count or r.future_count or r.ambiguous_count or 0\n    issue_pct = (issue_count / total_rows * 100) if total_rows > 0 else 0\n    \n    check_data.append({\n        \"Check\": r.check_name,\n        \"Status\": \"‚úÖ PASS\" if r.passed else \"‚ùå FAIL\",\n        \"Severity\": r.severity.value.upper(),\n        \"Issues\": f\"{issue_count:,}\",\n        \"% of Data\": f\"{issue_pct:.2f}%\",\n        \"Impact\": \"None\" if r.passed else (\"Critical\" if r.severity == Severity.HIGH else \"Moderate\" if r.severity == Severity.MEDIUM else \"Minor\")\n    })\n\nsummary_df = pd.DataFrame(check_data)\ndisplay(summary_df)\n\nprint(f\"\\nüìä Context: Dataset has {total_rows:,} total rows\")"
  },
  {
   "cell_type": "markdown",
   "id": "b2b28caa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### TQ001: Duplicate Events Analysis\n",
    "\n",
    "**üìñ Why Duplicates Matter for ML:**\n",
    "\n",
    "Duplicate events (same entity + timestamp) can distort your model in subtle ways:\n",
    "\n",
    "| Impact Area | Problem |\n",
    "|-------------|---------|\n",
    "| **Event counts** | Inflated activity metrics (\"sent 10 emails\" when actually 5) |\n",
    "| **Aggregations** | Sum/mean calculations skewed by duplicate values |\n",
    "| **Sequence modeling** | Artificial patterns introduced in event sequences |\n",
    "| **Class balance** | If duplicates correlate with target, creates sampling bias |\n",
    "\n",
    "**Common causes:**\n",
    "- System retries logging the same event multiple times\n",
    "- ETL pipeline re-processing data without deduplication\n",
    "- Intentional (e.g., multiple items in one transaction logged separately)\n",
    "\n",
    "**‚ö†Ô∏è Key question:** Are duplicates a **data quality issue** or **valid business events**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328b40c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dup_result = results[0]  # DuplicateEventCheck\n",
    "\n",
    "print(\"\\U0001f50d Duplicate Events Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if dup_result.duplicate_count > 0:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Found {dup_result.duplicate_count} duplicate events\")\n",
    "    print(f\"   Affected entities: {dup_result.details.get('affected_entities', 'N/A')}\")\n",
    "    \n",
    "    if \"duplicate_examples\" in dup_result.details:\n",
    "        print(\"\\n   Example duplicates:\")\n",
    "        examples = dup_result.details[\"duplicate_examples\"][:5]\n",
    "        for ex in examples:\n",
    "            print(f\"      Entity: {ex[ENTITY_COLUMN]}, Time: {ex[TIME_COLUMN]}\")\n",
    "    \n",
    "    print(\"\\n   \\U0001f6e0\\ufe0f Recommended Actions:\")\n",
    "    print(\"      1. Investigate why duplicates exist (system issue? intentional?)\")\n",
    "    print(\"      2. If unintentional: deduplicate by keeping first/last occurrence\")\n",
    "    print(\"      3. If intentional: add sequence column to differentiate\")\n",
    "else:\n",
    "    print(\"\\n\\u2705 No duplicate events found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda4367",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### TQ002: Temporal Gap Analysis\n",
    "\n",
    "**üìñ Why Gaps Matter for ML:**\n",
    "\n",
    "Temporal gaps (missing time periods) can silently corrupt your analysis and model:\n",
    "\n",
    "| Impact Area | Problem |\n",
    "|-------------|---------|\n",
    "| **Rolling features** | \"Events in last 30 days\" becomes artificially low during/after gaps |\n",
    "| **Recency features** | \"Days since last event\" inflated by data gaps, not actual inactivity |\n",
    "| **Seasonality detection** | Missing months distort seasonal patterns (e.g., missing all Decembers) |\n",
    "| **Train/test splits** | Gap at split boundary can cause data leakage or unfair evaluation |\n",
    "| **Aggregation bias** | Time windows spanning gaps have incomplete data, biasing metrics |\n",
    "| **Trend analysis** | Gaps create artificial trend breaks or distort slope calculations |\n",
    "\n",
    "**‚ö†Ô∏è Key insight:** A gap doesn't mean customers were inactive‚Äîit means **we don't know** what happened. Models can't distinguish \"no events\" from \"no data.\"\n",
    "\n",
    "**Recommended actions if gaps exist:**\n",
    "1. Document gap periods for downstream users\n",
    "2. Exclude gap-affected time windows from training\n",
    "3. Add a `data_available` flag to features\n",
    "4. Consider imputation only if gap cause is known (e.g., planned maintenance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696667eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gap_result = results[1]  # TemporalGapCheck\n",
    "\n",
    "print(\"\\U0001f50d Temporal Gap Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n   Expected frequency: {EXPECTED_FREQUENCY}\")\n",
    "print(f\"   Max gap detected: {gap_result.max_gap_days:.1f} days\")\n",
    "\n",
    "if gap_result.gap_count > 0:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Found {gap_result.gap_count} significant gaps\")\n",
    "    print(f\"   Threshold: {gap_result.details.get('threshold_days', 'N/A'):.1f} days\")\n",
    "    \n",
    "    print(\"\\n   \\U0001f6e0\\ufe0f Recommended Actions:\")\n",
    "    print(\"      1. Investigate gaps - were systems down? holidays?\")\n",
    "    print(\"      2. Consider gap locations when designing time windows\")\n",
    "    print(\"      3. Document known gaps for downstream users\")\n",
    "else:\n",
    "    print(\"\\n\\u2705 No significant temporal gaps detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac007c93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize event volume over time\n",
    "df_temp = df.copy()\n",
    "df_temp[TIME_COLUMN] = pd.to_datetime(df_temp[TIME_COLUMN])\n",
    "\n",
    "# Calculate time span to choose appropriate aggregation\n",
    "time_span_days = (df_temp[TIME_COLUMN].max() - df_temp[TIME_COLUMN].min()).days\n",
    "\n",
    "# Choose aggregation based on time span for better visibility\n",
    "if time_span_days <= 90:\n",
    "    freq, freq_label = \"D\", \"Daily\"\n",
    "elif time_span_days <= 365:\n",
    "    freq, freq_label = \"W\", \"Weekly\"\n",
    "else:\n",
    "    freq, freq_label = \"ME\", \"Monthly\"\n",
    "\n",
    "event_counts = df_temp.groupby(pd.Grouper(key=TIME_COLUMN, freq=freq)).size()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=event_counts.index,\n",
    "    y=event_counts.values,\n",
    "    name=f\"{freq_label} Events\",\n",
    "    marker_color=\"#4682B4\"\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=f\"{freq_label} Event Volume (gaps appear as missing bars)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Number of Events\",\n",
    "    template=\"plotly_white\",\n",
    "    height=350,\n",
    "    bargap=0.1\n",
    ")\n",
    "display_figure(fig)\n",
    "\n",
    "# Calendar heatmap for pattern discovery\n",
    "fig_calendar = charts.monthly_calendar_heatmap(\n",
    "    df_temp[TIME_COLUMN],\n",
    "    title=\"Event Patterns by Month and Day of Week\"\n",
    ")\n",
    "display_figure(fig_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4cc7d9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### TQ003: Future Dates Analysis\n",
    "\n",
    "**üìñ Why Future Dates Matter for ML:**\n",
    "\n",
    "Events with timestamps in the future are almost always data quality issues:\n",
    "\n",
    "| Impact Area | Problem |\n",
    "|-------------|---------|\n",
    "| **Data leakage** | Future events can leak into training data, inflating metrics |\n",
    "| **Time-based splits** | Future dates break temporal train/test separation |\n",
    "| **Recency features** | Negative \"days since\" values cause calculation errors |\n",
    "| **Business logic** | Impossible to have events that haven't happened yet |\n",
    "\n",
    "**Common causes:**\n",
    "- Timezone conversion errors (UTC vs local time)\n",
    "- Placeholder dates (e.g., \"9999-12-31\" for \"unknown\")\n",
    "- Scheduled events logged with future execution date\n",
    "- Data entry errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441d713",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_result = results[2]  # FutureDateCheck\n",
    "\n",
    "print(\"\\U0001f50d Future Dates Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n   Reference date: {REFERENCE_DATE}\")\n",
    "\n",
    "if future_result.future_count > 0:\n",
    "    print(f\"\\n\\U0001f6a8 Found {future_result.future_count} events with future dates!\")\n",
    "    \n",
    "    if \"future_date_examples\" in future_result.details:\n",
    "        print(\"\\n   Example future dates:\")\n",
    "        for ex in future_result.details[\"future_date_examples\"][:5]:\n",
    "            print(f\"      {ex}\")\n",
    "    \n",
    "    print(\"\\n   \\U0001f6e0\\ufe0f Recommended Actions:\")\n",
    "    print(\"      1. CRITICAL: Investigate data source - likely data quality issue\")\n",
    "    print(\"      2. Check for timezone issues or date parsing errors\")\n",
    "    print(\"      3. Filter out future dates before modeling\")\n",
    "else:\n",
    "    print(\"\\n\\u2705 No future dates detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8e2b0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### TQ004: Event Ordering Analysis\n",
    "\n",
    "**üìñ Why Event Ordering Matters for ML:**\n",
    "\n",
    "When multiple events share the same timestamp, their order becomes ambiguous:\n",
    "\n",
    "| Impact Area | Problem |\n",
    "|-------------|---------|\n",
    "| **Sequence features** | \"Previous event type\" undefined when order is ambiguous |\n",
    "| **State tracking** | Can't determine correct state transitions |\n",
    "| **Lag calculations** | Which event comes \"before\" the other? |\n",
    "| **Causal inference** | Impossible to establish event causality |\n",
    "\n",
    "**When this is okay:**\n",
    "- Events are independent (order doesn't matter for your features)\n",
    "- You're only using aggregations (counts, sums) not sequences\n",
    "\n",
    "**When this is a problem:**\n",
    "- Building sequence models (RNNs, transformers)\n",
    "- Creating \"previous event\" or \"next event\" features\n",
    "- Tracking customer journey stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fe0d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "order_result = results[3]  # EventOrderCheck\n",
    "\n",
    "print(\"\\U0001f50d Event Ordering Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if order_result.ambiguous_count > 0:\n",
    "    print(f\"\\n\\U0001f7e1 Found {order_result.ambiguous_count} events with ambiguous ordering\")\n",
    "    print(f\"   (Same entity + same timestamp = can't determine order)\")\n",
    "    print(f\"   Collision groups: {order_result.details.get('collision_groups', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n   \\U0001f6e0\\ufe0f Recommended Actions:\")\n",
    "    print(\"      1. If order matters: add a sequence number column\")\n",
    "    print(\"      2. If order doesn't matter: proceed with aggregation\")\n",
    "    print(\"      3. Consider using sub-second timestamps if available\")\n",
    "else:\n",
    "    print(\"\\n\\u2705 Event ordering is unambiguous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04e554",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.5 Quality Score & Recommendations\n",
    "\n",
    "**üìä Proportional Scoring System:**\n",
    "\n",
    "Each of the 4 checks contributes **25%** to the total score. Deductions are proportional to the **% of data affected**:\n",
    "\n",
    "| % Affected | Severity | Score Range |\n",
    "|------------|----------|-------------|\n",
    "| 0% | None | 100% |\n",
    "| < 0.1% | Negligible | ~99% |\n",
    "| 0.1 - 1% | Minor | 90-95% |\n",
    "| 1 - 5% | Moderate | 70-90% |\n",
    "| 5 - 20% | Significant | 30-70% |\n",
    "| > 20% | Severe | 0-30% |\n",
    "\n",
    "**Example:** If a check finds issues affecting 0.5% of rows, that check scores ~92%, contributing ~23/25 points to the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef0516",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Proportional Quality Scoring System\n",
    "# Each check contributes 25% to total score\n",
    "# Deduction proportional to % of data affected\n",
    "\n",
    "total_rows = len(df)\n",
    "\n",
    "def calculate_check_score(issue_count: int, total: int) -> float:\n",
    "    \"\"\"Calculate score (0-100) based on % of data affected.\"\"\"\n",
    "    if total == 0:\n",
    "        return 100.0\n",
    "    pct_affected = (issue_count / total) * 100\n",
    "    \n",
    "    # Graduated severity based on magnitude\n",
    "    if pct_affected == 0:\n",
    "        return 100.0\n",
    "    elif pct_affected < 0.1:\n",
    "        return 99.0  # Negligible\n",
    "    elif pct_affected < 1.0:\n",
    "        return 95.0 - (pct_affected * 5)  # Minor: 90-95\n",
    "    elif pct_affected < 5.0:\n",
    "        return 90.0 - (pct_affected * 4)  # Moderate: 70-90\n",
    "    elif pct_affected < 20.0:\n",
    "        return 70.0 - (pct_affected * 2)  # Significant: 30-70\n",
    "    else:\n",
    "        return max(0, 30.0 - pct_affected)  # Severe: 0-30\n",
    "\n",
    "# Calculate individual check scores\n",
    "check_scores = []\n",
    "for r in results:\n",
    "    issue_count = r.duplicate_count or r.gap_count or r.future_count or r.ambiguous_count or 0\n",
    "    pct_affected = (issue_count / total_rows * 100) if total_rows > 0 else 0\n",
    "    score = calculate_check_score(issue_count, total_rows)\n",
    "    deduction = (100 - score) * 0.25\n",
    "    \n",
    "    check_scores.append({\n",
    "        \"name\": r.check_name,\n",
    "        \"check_id\": r.check_id,\n",
    "        \"issues\": issue_count,\n",
    "        \"pct_affected\": pct_affected,\n",
    "        \"score\": score,\n",
    "        \"max_points\": 25.0,\n",
    "        \"deduction\": deduction,\n",
    "        \"contribution\": score * 0.25,\n",
    "        \"passed\": r.passed\n",
    "    })\n",
    "\n",
    "quality_score = sum(c[\"contribution\"] for c in check_scores)\n",
    "total_deductions = sum(c[\"deduction\"] for c in check_scores)\n",
    "\n",
    "grade, message = (\n",
    "    (\"A\", \"Excellent! Ready for feature engineering.\") if quality_score >= 90 else\n",
    "    (\"B\", \"Good. Minor issues - document and proceed.\") if quality_score >= 75 else\n",
    "    (\"C\", \"Fair. Address issues before proceeding.\") if quality_score >= 60 else\n",
    "    (\"D\", \"Needs attention. Significant issues found.\")\n",
    ")\n",
    "\n",
    "# Stacked horizontal bar - compact visualization\n",
    "def get_color(contribution):\n",
    "    if contribution >= 23:\n",
    "        return \"#2ca02c\"  # Green\n",
    "    elif contribution >= 17.5:\n",
    "        return \"#ffbb00\"  # Yellow\n",
    "    elif contribution >= 12.5:\n",
    "        return \"#ff7f0e\"  # Orange\n",
    "    else:\n",
    "        return \"#d62728\"  # Red\n",
    "\n",
    "fig = go.Figure()\n",
    "for c in check_scores:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=[c[\"contribution\"]],\n",
    "        y=[\"Score\"],\n",
    "        orientation=\"h\",\n",
    "        name=c[\"name\"],\n",
    "        marker_color=get_color(c[\"contribution\"]),\n",
    "        text=f\"+{c['contribution']:.0f}\",\n",
    "        textposition=\"inside\",\n",
    "        textfont=dict(color=\"white\", size=12),\n",
    "        hovertemplate=f\"<b>{c['name']}</b><br>Issues: {c['issues']:,} ({c['pct_affected']:.2f}%)<br>Contribution: {c['contribution']:.1f}/25<extra></extra>\"\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    title=f\"Quality Score: {quality_score:.0f}/100 (Grade {grade})\",\n",
    "    xaxis=dict(range=[0, 105], title=\"\"),\n",
    "    yaxis=dict(visible=False),\n",
    "    template=\"plotly_white\",\n",
    "    height=120,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"center\", x=0.5),\n",
    "    margin=dict(t=60, b=20, l=20, r=40)\n",
    ")\n",
    "fig.add_annotation(x=quality_score, y=\"Score\", text=f\"<b>{quality_score:.0f}</b>\", showarrow=False, xanchor=\"left\", xshift=5, font=dict(size=14))\n",
    "display_figure(fig)\n",
    "\n",
    "# Detailed breakdown table\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"{'Check':<22} {'Issues':>8} {'% Affected':>11} {'Max':>6} {'Deduction':>10} {'Actual':>8}\")\n",
    "print(f\"{'-'*90}\")\n",
    "for c in check_scores:\n",
    "    status = \"‚úì\" if c[\"deduction\"] < 0.5 else \"‚ñ≥\" if c[\"deduction\"] < 5 else \"‚úó\"\n",
    "    ded_str = f\"-{c['deduction']:.1f}\" if c[\"deduction\"] > 0 else \"0\"\n",
    "    print(f\"{status} {c['name']:<20} {c['issues']:>8,} {c['pct_affected']:>10.2f}% {c['max_points']:>6.0f} {ded_str:>10} {c['contribution']:>7.1f}\")\n",
    "print(f\"{'-'*90}\")\n",
    "print(f\"  {'TOTAL':<20} {'':<8} {'':<11} {'100':>6} {f'-{total_deductions:.1f}':>10} {quality_score:>7.1f}\")\n",
    "print(f\"\\nüìä Grade {grade}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c02b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mitigation strategies for checks with issues\n",
    "checks_with_issues = [c for c in check_scores if c[\"issues\"] > 0]\n",
    "\n",
    "if checks_with_issues:\n",
    "    mitigation_map = {\n",
    "        \"TQ001\": (\"Duplicate Events\", [\n",
    "            \"Deduplicate: df.drop_duplicates(subset=[entity_col, time_col], keep='first')\",\n",
    "            \"If intentional: df['seq'] = df.groupby([entity_col, time_col]).cumcount()\"\n",
    "        ]),\n",
    "        \"TQ002\": (\"Temporal Gaps\", [\n",
    "            \"Document gaps for downstream users\",\n",
    "            \"Exclude gap periods from rolling calculations\",\n",
    "            \"Add indicator: df['has_gap'] = df[time_col].diff() > threshold\"\n",
    "        ]),\n",
    "        \"TQ003\": (\"Future Dates\", [\n",
    "            \"Filter: df = df[df[time_col] <= pd.Timestamp.now()]\",\n",
    "            \"Check timezones: df[time_col] = df[time_col].dt.tz_convert('UTC')\"\n",
    "        ]),\n",
    "        \"TQ004\": (\"Event Ordering\", [\n",
    "            \"Add sequence: df['event_seq'] = df.groupby(entity_col).cumcount()\",\n",
    "            \"Stable sort: df.sort_values([entity_col, time_col], kind='stable')\"\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üîß RECOMMENDED MITIGATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for c in checks_with_issues:\n",
    "        if c[\"check_id\"] in mitigation_map:\n",
    "            name, strategies = mitigation_map[c[\"check_id\"]]\n",
    "            severity = \"üü¢ Minor\" if c[\"pct_affected\"] < 1 else \"üü° Moderate\" if c[\"pct_affected\"] < 5 else \"üî¥ Significant\"\n",
    "            \n",
    "            print(f\"\\n{severity}: {name}\")\n",
    "            print(f\"   {c['issues']:,} issues ({c['pct_affected']:.2f}% of data) ‚Üí Score: {c['score']:.0f}%\")\n",
    "            print(f\"   Strategies:\")\n",
    "            for i, s in enumerate(strategies, 1):\n",
    "                print(f\"      {i}. {s}\")\n",
    "else:\n",
    "    print(\"‚úÖ No issues detected - data quality is excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb71353",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.6 Target Variable Analysis\n",
    "\n",
    "Understanding target distribution is critical for:\n",
    "- **Class imbalance** affects model training and evaluation metrics\n",
    "- **Business context** helps interpret what we're trying to predict\n",
    "- **Sampling strategies** depend on imbalance severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd761d06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"TARGET VARIABLE DISTRIBUTION: {findings.target_column}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if findings.target_column and findings.target_column in df.columns:\n",
    "    target_series = df[findings.target_column]\n",
    "    target_counts = target_series.value_counts().sort_index()\n",
    "    \n",
    "    dist_data = []\n",
    "    for val, count in target_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        dist_data.append({findings.target_column: val, \"count\": count, \"percentage\": f\"{pct:.3f}\"})\n",
    "    \n",
    "    display(pd.DataFrame(dist_data))\n",
    "    \n",
    "    if len(target_counts) == 2:\n",
    "        majority, minority = target_counts.max(), target_counts.min()\n",
    "        minority_class = target_counts.idxmin()\n",
    "        imbalance_ratio = majority / minority\n",
    "        retention_rate = target_counts.get(1, 0) / len(df) * 100\n",
    "        \n",
    "        print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1 (minority class: {minority_class})\")\n",
    "        print(f\"Retention rate: {retention_rate:.1f}%\")\n",
    "        \n",
    "        if retention_rate > 70:\n",
    "            print(f\"\\nüìä Business Context: {retention_rate:.0f}% retention is healthy!\")\n",
    "        elif retention_rate > 50:\n",
    "            print(f\"\\nüìä Business Context: {retention_rate:.0f}% retention is moderate.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Business Context: {retention_rate:.0f}% retention is concerning!\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è Class imbalance considerations:\")\n",
    "        print(\"   - Use stratified sampling for train/test splits\")\n",
    "        print(\"   - Consider class weights in model training\")\n",
    "        print(\"   - Evaluate with Precision-Recall AUC (not just ROC-AUC)\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]],\n",
    "                           horizontal_spacing=0.25)\n",
    "        \n",
    "        labels = [f\"{'Retained' if v == 1 else 'Churned'} ({v})\" for v in target_counts.index]\n",
    "        \n",
    "        # Pie chart - percentages inside, legend from this trace\n",
    "        fig.add_trace(go.Pie(\n",
    "            labels=labels, \n",
    "            values=target_counts.values, \n",
    "            hole=0.4,\n",
    "            marker_colors=[\"#e74c3c\", \"#2ecc71\"],\n",
    "            textposition=\"inside\",\n",
    "            textinfo=\"percent\",\n",
    "            textfont=dict(size=12, color=\"white\"),\n",
    "            hoverinfo=\"label+percent+value\"\n",
    "        ), row=1, col=1)\n",
    "        \n",
    "        # Bar chart - counts inside, no legend (uses same colors)\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=labels, \n",
    "            y=target_counts.values,\n",
    "            marker_color=[\"#e74c3c\", \"#2ecc71\"],\n",
    "            text=[f\"{count:,}\" for count in target_counts.values],\n",
    "            textposition=\"inside\",\n",
    "            textfont=dict(color=\"white\", size=12),\n",
    "            showlegend=False\n",
    "        ), row=1, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=420,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            ),\n",
    "            template=\"plotly_white\",\n",
    "            margin=dict(t=100, b=60, l=50, r=50),\n",
    "            title=dict(\n",
    "                text=\"<b>Target Variable Distribution</b>\",\n",
    "                y=0.98,\n",
    "                x=0.5,\n",
    "                xanchor=\"center\",\n",
    "                yanchor=\"top\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        display_figure(fig)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No target column detected. Set target_hint in DataExplorer.explore()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05134ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.7 Numerical Feature Statistics\n",
    "\n",
    "Comprehensive statistical summary including skewness, kurtosis, and distribution shape analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d69f66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_cols = [name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]\n",
    "    and name not in [ENTITY_COLUMN, TIME_COLUMN]]\n",
    "\n",
    "if numeric_cols:\n",
    "    stats_data = []\n",
    "    for col_name in numeric_cols:\n",
    "        series = df[col_name].dropna()\n",
    "        if len(series) > 0:\n",
    "            stats_data.append({\n",
    "                \"feature\": col_name, \"count\": len(series),\n",
    "                \"mean\": series.mean(), \"std\": series.std(),\n",
    "                \"min\": series.min(), \"25%\": series.quantile(0.25),\n",
    "                \"50%\": series.quantile(0.50), \"75%\": series.quantile(0.75),\n",
    "                \"99%\": series.quantile(0.99), \"max\": series.max(),\n",
    "                \"skewness\": stats.skew(series), \"kurtosis\": stats.kurtosis(series)\n",
    "            })\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    display_df = stats_df.copy()\n",
    "    for col in [\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"99%\", \"max\"]:\n",
    "        display_df[col] = display_df[col].apply(lambda x: f\"{x:.3f}\")\n",
    "    display_df[\"skewness\"] = display_df[\"skewness\"].apply(lambda x: f\"{x:.3f}\")\n",
    "    display_df[\"kurtosis\"] = display_df[\"kurtosis\"].apply(lambda x: f\"{x:.3f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"NUMERICAL FEATURE STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    display(display_df)\n",
    "    \n",
    "    print(\"\\nüìä DISTRIBUTION ALERTS:\")\n",
    "    alerts = []\n",
    "    for _, row in stats_df.iterrows():\n",
    "        issues = []\n",
    "        if abs(row[\"skewness\"]) > 2:\n",
    "            issues.append(f\"highly skewed ({row['skewness']:.2f})\")\n",
    "        elif abs(row[\"skewness\"]) > 1:\n",
    "            issues.append(f\"moderately skewed ({row['skewness']:.2f})\")\n",
    "        if row[\"kurtosis\"] > 10:\n",
    "            issues.append(f\"very heavy tails ({row['kurtosis']:.1f})\")\n",
    "        elif row[\"kurtosis\"] > 3:\n",
    "            issues.append(f\"heavy tails ({row['kurtosis']:.1f})\")\n",
    "        if issues:\n",
    "            alerts.append(row[\"feature\"])\n",
    "            print(f\"  ‚ö†Ô∏è {row['feature']}: {', '.join(issues)}\")\n",
    "    if not alerts:\n",
    "        print(\"  ‚úÖ All distributions are approximately normal\")\n",
    "else:\n",
    "    print(\"No numeric columns found (excluding entity/time columns).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca6fd5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.8 Segment-Aware Outlier Analysis\n",
    "\n",
    "Global outlier detection can produce false positives when data contains natural segments (e.g., high-value vs regular customers). This analysis detects segments and compares global vs segment-specific outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f896d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SEGMENT-AWARE OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if numeric_cols:\n",
    "    analyzer = SegmentAwareOutlierAnalyzer(max_segments=5)\n",
    "    segment_result = analyzer.analyze(df, feature_cols=numeric_cols, segment_col=None,\n",
    "                                       target_col=findings.target_column)\n",
    "    \n",
    "    print(f\"\\nüìä Segments detected: {segment_result.n_segments}\")\n",
    "    \n",
    "    if segment_result.n_segments > 1:\n",
    "        print(f\"\\nüìà GLOBAL VS SEGMENT OUTLIER COMPARISON:\")\n",
    "        comparison_data = []\n",
    "        for col in numeric_cols:\n",
    "            global_outliers = segment_result.global_analysis[col].outliers_detected\n",
    "            segment_outliers = sum(seg[col].outliers_detected for seg in segment_result.segment_analysis.values() if col in seg)\n",
    "            false_outliers = segment_result.false_outliers.get(col, 0)\n",
    "            reduction_pct = (global_outliers - segment_outliers) / global_outliers * 100 if global_outliers > 0 else 0\n",
    "            comparison_data.append({\"Feature\": col, \"Global\": global_outliers, \"Segment\": segment_outliers,\n",
    "                                    \"False Outliers\": false_outliers, \"Reduction\": f\"{reduction_pct:.1f}%\"})\n",
    "        display(pd.DataFrame(comparison_data))\n",
    "        \n",
    "        if segment_result.segmentation_recommended:\n",
    "            print(\"\\nüí° SEGMENT-SPECIFIC OUTLIER TREATMENT RECOMMENDED\")\n",
    "            for rec in segment_result.recommendations:\n",
    "                print(f\"   ‚Ä¢ {rec}\")\n",
    "        \n",
    "        # Visualization\n",
    "        cols_with_diff = [r[\"Feature\"] for r in comparison_data if r[\"Global\"] > 0 and r[\"Global\"] != r[\"Segment\"]]\n",
    "        if cols_with_diff:\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Bar(name=\"Global\", x=cols_with_diff,\n",
    "                y=[r[\"Global\"] for r in comparison_data if r[\"Feature\"] in cols_with_diff], marker_color=\"#e74c3c\"))\n",
    "            fig.add_trace(go.Bar(name=\"Segment\", x=cols_with_diff,\n",
    "                y=[r[\"Segment\"] for r in comparison_data if r[\"Feature\"] in cols_with_diff], marker_color=\"#2ecc71\"))\n",
    "            fig.update_layout(barmode=\"group\", title=\"Global vs Segment Outliers\", height=400, template=\"plotly_white\")\n",
    "            display_figure(fig)\n",
    "    else:\n",
    "        print(\"   Data appears homogeneous - using global outlier detection\")\n",
    "else:\n",
    "    print(\"No numeric columns for outlier analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516237d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.9 Binary Field & Data Consistency Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c0ff2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Binary Field Validation\n",
    "binary_cols = [name for name, col in findings.columns.items() if col.inferred_type == ColumnType.BINARY]\n",
    "print(\"=\" * 60)\n",
    "print(\"BINARY FIELD VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if binary_cols:\n",
    "    for col in binary_cols:\n",
    "        unique_vals = sorted(df[col].dropna().unique())\n",
    "        is_valid = set(unique_vals).issubset({0, 1, 0.0, 1.0})\n",
    "        count_0, count_1 = (df[col] == 0).sum(), (df[col] == 1).sum()\n",
    "        total = count_0 + count_1\n",
    "        status = \"‚úì\" if is_valid else \"‚ö†Ô∏è\"\n",
    "        print(f\"\\n{status} {col}: 0={count_0:,} ({count_0/total*100:.1f}%), 1={count_1:,} ({count_1/total*100:.1f}%)\")\n",
    "        if not is_valid:\n",
    "            print(f\"   Invalid values: {[v for v in unique_vals if v not in [0, 1, 0.0, 1.0]]}\")\n",
    "else:\n",
    "    print(\"\\nNo binary columns detected.\")\n",
    "\n",
    "# Data Consistency Checks\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA CONSISTENCY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "consistency_issues = []\n",
    "for col_name in df.select_dtypes(include=['object']).columns:\n",
    "    if col_name in [ENTITY_COLUMN, TIME_COLUMN]:\n",
    "        continue\n",
    "    unique_vals = df[col_name].dropna().unique()\n",
    "    case_variants = {}\n",
    "    for val in unique_vals:\n",
    "        lower_val = str(val).lower().strip()\n",
    "        if lower_val not in case_variants:\n",
    "            case_variants[lower_val] = []\n",
    "        case_variants[lower_val].append(val)\n",
    "    for lower_val, variants in case_variants.items():\n",
    "        if len(variants) > 1:\n",
    "            consistency_issues.append({\"Column\": col_name, \"Issue\": \"Case/Spacing Variants\", \"Details\": str(variants[:3])})\n",
    "\n",
    "if consistency_issues:\n",
    "    print(\"\\n‚ö†Ô∏è Consistency issues found:\")\n",
    "    display(pd.DataFrame(consistency_issues))\n",
    "else:\n",
    "    print(\"\\n‚úÖ No consistency issues detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ffa01",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.10 Quality Improvement Recommendations\n",
    "\n",
    "Using the framework's RecommendationEngine to generate prioritized, actionable recommendations based on detected issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9605d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"QUALITY IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "rec_engine = RecommendationEngine()\n",
    "cleaning_recs = rec_engine.recommend_cleaning(findings)\n",
    "\n",
    "if cleaning_recs:\n",
    "    severity_order = {\"high\": 0, \"medium\": 1, \"low\": 2}\n",
    "    sorted_recs = sorted(cleaning_recs, key=lambda r: severity_order.get(r.severity, 3))\n",
    "    \n",
    "    for rec in sorted_recs:\n",
    "        icon = \"üî¥\" if rec.severity == \"high\" else \"üü°\" if rec.severity == \"medium\" else \"üü¢\"\n",
    "        print(f\"\\n{icon} [{rec.severity.upper()}] {rec.column_name}\")\n",
    "        print(f\"   Issue: {rec.issue_type} - {rec.description}\")\n",
    "        print(f\"   Strategy: {rec.strategy}\")\n",
    "        if rec.problem_impact:\n",
    "            print(f\"   Impact: {rec.problem_impact}\")\n",
    "        if rec.action_steps:\n",
    "            print(f\"   Steps: {', '.join(rec.action_steps[:3])}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No critical cleaning recommendations - data quality is good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5756d0d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1b.11 Save Quality Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927747f8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store quality check results in findings\n",
    "quality_summary = {\n",
    "    \"temporal_quality_score\": quality_score,\n",
    "    \"temporal_quality_grade\": grade,\n",
    "    \"checks_passed\": passed,\n",
    "    \"checks_total\": len(results),\n",
    "    \"issues\": {\n",
    "        \"duplicate_events\": dup_result.duplicate_count,\n",
    "        \"temporal_gaps\": gap_result.gap_count,\n",
    "        \"future_dates\": future_result.future_count,\n",
    "        \"ambiguous_ordering\": order_result.ambiguous_count,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add to findings notes\n",
    "if not findings.metadata:\n",
    "    findings.metadata = {}\n",
    "findings.metadata[\"temporal_quality\"] = quality_summary\n",
    "\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"Quality results saved to: {FINDINGS_PATH}\")\n",
    "print(f\"\\nSummary: {quality_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855d3c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "In this notebook, we performed temporal-specific quality checks:\n",
    "\n",
    "1. **Duplicate Events** - Checked for same entity + timestamp combinations\n",
    "2. **Temporal Gaps** - Identified unexpected missing time periods\n",
    "3. **Future Dates** - Found dates beyond reference (data quality issue)\n",
    "4. **Event Ordering** - Verified events can be uniquely ordered\n",
    "\n",
    "## Quality Score Interpretation\n",
    "\n",
    "| Grade | Score | Meaning |\n",
    "|-------|-------|--------|\n",
    "| A | 90-100 | Excellent - proceed with confidence |\n",
    "| B | 75-89 | Good - minor issues, document and proceed |\n",
    "| C | 60-74 | Fair - address issues before feature engineering |\n",
    "| D | <60 | Poor - significant investigation needed |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue with the **Event Bronze Track**:\n",
    "\n",
    "1. **01c_temporal_patterns.ipynb** - Detect trends, seasonality, cohort analysis\n",
    "2. **01d_event_aggregation.ipynb** - Aggregate events to entity-level (produces new dataset)\n",
    "\n",
    "After completing 01d, continue with the **Entity Bronze Track** (02 ‚Üí 03 ‚Üí 04) on the aggregated data.\n",
    "\n",
    "Or return to fix data quality issues if grade is C or below."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.441716,
   "end_time": "2026-01-22T14:17:47.957266",
   "environment_variables": {},
   "exception": true,
   "input_path": "exploration_notebooks/01b_temporal_quality.ipynb",
   "output_path": "docs/tutorial/executed/01b_temporal_quality.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T14:17:45.515550",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}