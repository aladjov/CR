{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23d605d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46bd605",
   "metadata": {
    "papermill": {
     "duration": 0.002872,
     "end_time": "2026-01-22T14:18:11.295366",
     "exception": false,
     "start_time": "2026-01-22T14:18:11.292494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chapter 6: Feature Opportunities\n",
    "\n",
    "**Purpose:** Identify and implement feature engineering opportunities to improve model performance.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to derive time-based features (tenure, recency, active period)\n",
    "- How to create composite engagement scores\n",
    "- How to segment customers based on behavior patterns\n",
    "- How to encode categorical variables effectively\n",
    "\n",
    "**Outputs:**\n",
    "- Derived feature recommendations with code examples\n",
    "- Composite score formulas (engagement, service adoption)\n",
    "- Customer segmentation rules\n",
    "- Categorical encoding strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Why Feature Engineering Matters\n",
    "\n",
    "| Feature Type | Business Meaning | Predictive Power |\n",
    "|-------------|-----------------|------------------|\n",
    "| **Tenure** | How long customer has been with us | Loyalty indicator |\n",
    "| **Recency** | Days since last order | Engagement/churn signal |\n",
    "| **Engagement Score** | Combined email metrics | Overall engagement level |\n",
    "| **Segments** | High/Low value √ó Frequent/Infrequent | Risk stratification |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3d5b4",
   "metadata": {
    "papermill": {
     "duration": 0.002202,
     "end_time": "2026-01-22T14:18:11.300406",
     "exception": false,
     "start_time": "2026-01-22T14:18:11.298204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6bc2166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:18:11.306260Z",
     "iopub.status.busy": "2026-01-22T14:18:11.306107Z",
     "iopub.status.idle": "2026-01-22T14:18:12.670274Z",
     "shell.execute_reply": "2026-01-22T14:18:12.669706Z"
    },
    "papermill": {
     "duration": 1.368301,
     "end_time": "2026-01-22T14:18:12.670945",
     "exception": false,
     "start_time": "2026-01-22T14:18:11.302644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings, RecommendationEngine, RecommendationRegistry\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType\n",
    "from customer_retention.stages.features import CustomerSegmenter, SegmentationType\n",
    "from customer_retention.stages.profiling import FeatureCapacityAnalyzer\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a4b06",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d757dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:18:12.676281Z",
     "iopub.status.busy": "2026-01-22T14:18:12.676127Z",
     "iopub.status.idle": "2026-01-22T14:18:12.807483Z",
     "shell.execute_reply": "2026-01-22T14:18:12.806972Z"
    },
    "papermill": {
     "duration": 0.135294,
     "end_time": "2026-01-22T14:18:12.808302",
     "exception": true,
     "start_time": "2026-01-22T14:18:12.673008",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No findings files found in ../experiments/findings. Run notebook 01 first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m findings_files = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m FINDINGS_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*_findings.yaml\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmulti_dataset\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f.name]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m findings_files:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo findings files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINDINGS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run notebook 01 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m findings_files.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m f: f.stat().st_mtime, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m FINDINGS_PATH = \u001b[38;5;28mstr\u001b[39m(findings_files[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No findings files found in ../experiments/findings. Run notebook 01 first."
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Option 1: Set the exact path from notebook 01 output\n",
    "# FINDINGS_PATH = \"../experiments/findings/customer_retention_retail_abc123_findings.yaml\"\n",
    "\n",
    "# Option 2: Auto-discover the most recent findings file\n",
    "from pathlib import Path\n",
    "\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "findings_files = [f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") if \"multi_dataset\" not in f.name]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "RECOMMENDATIONS_PATH = FINDINGS_PATH.replace(\"_findings.yaml\", \"_recommendations.yaml\")\n",
    "\n",
    "print(f\"Found {len(findings_files)} findings file(s)\")\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "\n",
    "# Load data with snapshot preference (uses temporal snapshots if available)\n",
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\n",
    "charts = ChartBuilder()\n",
    "\n",
    "if Path(RECOMMENDATIONS_PATH).exists():\n",
    "    with open(RECOMMENDATIONS_PATH, \"r\") as f:\n",
    "        registry = RecommendationRegistry.from_dict(yaml.safe_load(f))\n",
    "    print(f\"Loaded existing recommendations: {len(registry.all_recommendations)} total\")\n",
    "else:\n",
    "    registry = RecommendationRegistry()\n",
    "    print(\"Initialized new recommendation registry\")\n",
    "\n",
    "# Ensure all layers are initialized (even if loaded from file)\n",
    "if not registry.bronze:\n",
    "    registry.init_bronze(findings.source_path)\n",
    "if not registry.silver:\n",
    "    registry.init_silver(findings.entity_column or \"entity_id\")\n",
    "if not registry.gold:\n",
    "    registry.init_gold(findings.target_column or \"target\")\n",
    "    print(\"  Initialized gold layer for feature engineering recommendations\")\n",
    "\n",
    "print(f\"\\nLoaded {len(df):,} rows from: {data_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff0581",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.2 Automated Feature Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf1ed0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommender = RecommendationEngine()\n",
    "feature_recs = recommender.recommend_features(findings)\n",
    "\n",
    "print(f\"Found {len(feature_recs)} feature engineering opportunities:\\n\")\n",
    "\n",
    "for rec in feature_recs:\n",
    "    print(f\"{rec.feature_name}\")\n",
    "    print(f\"  Source: {rec.source_column}\")\n",
    "    print(f\"  Type: {rec.feature_type}\")\n",
    "    print(f\"  Priority: {rec.priority}\")\n",
    "    print(f\"  Description: {rec.description}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d73f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.3 Feature Capacity Analysis\n",
    "\n",
    "**üìñ Understanding Feature-to-Data Ratios**\n",
    "\n",
    "Before creating new features, it's critical to understand how many features your data can reliably support. This analysis uses the **Events Per Variable (EPV)** principle:\n",
    "\n",
    "| EPV Level | Risk Level | Recommendations |\n",
    "|-----------|------------|-----------------|\n",
    "| **EPV ‚â• 20** | Low risk | Stable coefficients, reliable inference |\n",
    "| **EPV = 10-20** | Moderate | Standard practice, consider regularization |\n",
    "| **EPV = 5-10** | Elevated | Strong regularization required (L1/Lasso) |\n",
    "| **EPV < 5** | High risk | Reduce features or collect more data |\n",
    "\n",
    "**Key Assumptions:**\n",
    "1. **Minority class drives capacity**: For classification, the smaller class limits feature count\n",
    "2. **Correlated features are redundant**: Highly correlated features (r > 0.8) count as ~1 effective feature\n",
    "3. **Model type matters**: Tree models are more flexible than linear models\n",
    "4. **Regularization helps**: L1/L2 penalties allow more features with less data\n",
    "\n",
    "**üìä What This Analysis Provides:**\n",
    "- Recommended feature counts (conservative/moderate/aggressive)\n",
    "- Effective feature count after removing redundancy\n",
    "- Model complexity guidance (linear vs tree-based)\n",
    "- Segment-specific capacity for multi-model strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf1fb3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Capacity Analysis\n",
    "capacity_analyzer = FeatureCapacityAnalyzer()\n",
    "\n",
    "# Get all potential feature columns (excluding target and identifiers)\n",
    "feature_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [\n",
    "        ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE,\n",
    "        ColumnType.CATEGORICAL_NOMINAL, ColumnType.CATEGORICAL_ORDINAL,\n",
    "        ColumnType.BINARY\n",
    "    ] and name != findings.target_column\n",
    "    and name not in TEMPORAL_METADATA_COLS\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE CAPACITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if findings.target_column:\n",
    "    # Analyze capacity with current features\n",
    "    numeric_features = [\n",
    "        name for name, col in findings.columns.items()\n",
    "        if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]\n",
    "        and name != findings.target_column\n",
    "    ]\n",
    "    \n",
    "    capacity_result = capacity_analyzer.analyze(\n",
    "        df,\n",
    "        feature_cols=numeric_features,\n",
    "        target_col=findings.target_column,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä DATA SUMMARY:\")\n",
    "    print(f\"   Total samples: {capacity_result.total_samples:,}\")\n",
    "    print(f\"   Minority class samples: {capacity_result.minority_class_samples:,}\")\n",
    "    print(f\"   Minority class rate: {capacity_result.minority_class_samples/capacity_result.total_samples:.1%}\")\n",
    "    print(f\"   Current numeric features: {capacity_result.total_features}\")\n",
    "    \n",
    "    print(f\"\\nüìà FEATURE CAPACITY METRICS:\")\n",
    "    print(f\"   Events Per Variable (EPV): {capacity_result.events_per_variable:.1f}\")\n",
    "    print(f\"   Samples Per Feature: {capacity_result.samples_per_feature:.1f}\")\n",
    "    print(f\"   Capacity Status: {capacity_result.capacity_status.upper()}\")\n",
    "    \n",
    "    # Capacity status visualization\n",
    "    status_colors = {\"adequate\": \"#2ecc71\", \"limited\": \"#f39c12\", \"inadequate\": \"#e74c3c\"}\n",
    "    status_color = status_colors.get(capacity_result.capacity_status, \"#95a5a6\")\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMENDED FEATURE COUNTS:\")\n",
    "    print(f\"   Conservative (EPV=20): {capacity_result.recommended_features_conservative} features\")\n",
    "    print(f\"   Moderate (EPV=10):     {capacity_result.recommended_features_moderate} features\")\n",
    "    print(f\"   Aggressive (EPV=5):    {capacity_result.recommended_features_aggressive} features\")\n",
    "    \n",
    "    # Effective features analysis\n",
    "    if capacity_result.effective_features_result:\n",
    "        eff = capacity_result.effective_features_result\n",
    "        print(f\"\\nüîç EFFECTIVE FEATURES (accounting for correlation):\")\n",
    "        print(f\"   Total features analyzed: {eff.total_count}\")\n",
    "        print(f\"   Effective independent features: {eff.effective_count:.1f}\")\n",
    "        print(f\"   Redundant features identified: {len(eff.redundant_features)}\")\n",
    "        \n",
    "        if eff.redundant_features:\n",
    "            print(f\"\\n   ‚ö†Ô∏è Redundant features (highly correlated):\")\n",
    "            for feat in eff.redundant_features[:5]:\n",
    "                print(f\"      ‚Ä¢ {feat}\")\n",
    "        \n",
    "        if eff.feature_clusters:\n",
    "            print(f\"\\n   üì¶ Correlated feature clusters ({len(eff.feature_clusters)}):\")\n",
    "            for i, cluster in enumerate(eff.feature_clusters[:3]):\n",
    "                print(f\"      Cluster {i+1}: {', '.join(cluster[:4])}\")\n",
    "                if len(cluster) > 4:\n",
    "                    print(f\"                  ... and {len(cluster)-4} more\")\n",
    "    \n",
    "    # Persist feature capacity to registry\n",
    "    registry.add_bronze_feature_capacity(\n",
    "        epv=capacity_result.events_per_variable,\n",
    "        capacity_status=capacity_result.capacity_status,\n",
    "        recommended_features=capacity_result.recommended_features_moderate,\n",
    "        current_features=capacity_result.total_features,\n",
    "        rationale=f\"EPV={capacity_result.events_per_variable:.1f}, status={capacity_result.capacity_status}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Persisted feature capacity recommendation to registry\")\n",
    "    \n",
    "    # Store capacity info in findings\n",
    "    findings.metadata[\"feature_capacity\"] = capacity_result.to_dict()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No target column detected. Capacity analysis requires a target variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03d5b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.3.1 Model Complexity Guidance\n",
    "\n",
    "Based on your data capacity, here's guidance on model complexity and feature limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df864f50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Complexity Guidance\n",
    "if findings.target_column and 'capacity_result' in dir():\n",
    "    guidance = capacity_result.complexity_guidance\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"MODEL COMPLEXITY GUIDANCE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create visualization of feature limits by model type\n",
    "    model_types = [\"Linear\\n(no regularization)\", \"Regularized\\n(L1/L2)\", \"Tree-based\\n(RF/XGBoost)\"]\n",
    "    max_features = [guidance.max_features_linear, guidance.max_features_regularized, guidance.max_features_tree]\n",
    "    current_features = capacity_result.total_features\n",
    "    \n",
    "    colors = ['#e74c3c' if m < current_features else '#2ecc71' for m in max_features]\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=model_types,\n",
    "        y=max_features,\n",
    "        marker_color=colors,\n",
    "        text=[f\"{m}\" for m in max_features],\n",
    "        textposition='outside',\n",
    "        name='Max Features'\n",
    "    ))\n",
    "    \n",
    "    # Add horizontal line for current feature count\n",
    "    fig.add_hline(\n",
    "        y=current_features,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"#3498db\",\n",
    "        annotation_text=f\"Current: {current_features}\",\n",
    "        annotation_position=\"right\"\n",
    "    )\n",
    "    \n",
    "    # Calculate y-axis range to fit labels\n",
    "    max_val = max(max_features)\n",
    "    fig.update_layout(\n",
    "        title=\"Maximum Recommended Features by Model Type\",\n",
    "        xaxis_title=\"Model Type\",\n",
    "        yaxis_title=\"Max Features\",\n",
    "        yaxis_range=[0, max_val * 1.15],  # Add 15% headroom for labels\n",
    "        template='plotly_white',\n",
    "        height=400,\n",
    "        showlegend=False,\n",
    "    )\n",
    "    \n",
    "    display_figure(fig)\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMENDED MODEL TYPE: {guidance.recommended_model_type.replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(\"\\nüìã MODEL-SPECIFIC RECOMMENDATIONS:\")\n",
    "    for rec in guidance.model_recommendations:\n",
    "        print(f\"   ‚Ä¢ {rec}\")\n",
    "    \n",
    "    print(\"\\nüí° GENERAL GUIDANCE:\")\n",
    "    for rec in guidance.recommendations:\n",
    "        print(f\"   {rec}\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"FEATURE BUDGET SUMMARY:\")\n",
    "    print(\"-\" * 70)\n",
    "    summary_data = {\n",
    "        \"Model Type\": [\"Linear (no regularization)\", \"Regularized (L1/L2)\", \"Tree-based\"],\n",
    "        \"Max Features\": [guidance.max_features_linear, guidance.max_features_regularized, guidance.max_features_tree],\n",
    "        \"Current\": [current_features] * 3,\n",
    "        \"Status\": [\n",
    "            \"‚úÖ OK\" if guidance.max_features_linear >= current_features else \"‚ö†Ô∏è Reduce\",\n",
    "            \"‚úÖ OK\" if guidance.max_features_regularized >= current_features else \"‚ö†Ô∏è Reduce\", \n",
    "            \"‚úÖ OK\" if guidance.max_features_tree >= current_features else \"‚ö†Ô∏è Reduce\"\n",
    "        ]\n",
    "    }\n",
    "    display(pd.DataFrame(summary_data))\n",
    "    \n",
    "    # Persist model type recommendation to registry\n",
    "    registry.add_bronze_model_type(\n",
    "        model_type=guidance.recommended_model_type,\n",
    "        max_features_linear=guidance.max_features_linear,\n",
    "        max_features_regularized=guidance.max_features_regularized,\n",
    "        max_features_tree=guidance.max_features_tree,\n",
    "        rationale=f\"Recommended: {guidance.recommended_model_type}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "    print(f\"\\n‚úÖ Persisted model type recommendation to registry: {guidance.recommended_model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b49427",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.3.2 Segment-Specific Capacity (for Multi-Model Strategy)\n",
    "\n",
    "When considering **separate models per customer segment**, each segment must have sufficient data to support the feature set. This analysis shows whether segmented modeling is viable.\n",
    "\n",
    "**üìñ Single Model vs Segment Models:**\n",
    "\n",
    "| Approach | When to Use | Pros | Cons |\n",
    "|----------|------------|------|------|\n",
    "| **Single Model** | Small data, uniform segments | More data per model, simpler | May miss segment-specific patterns |\n",
    "| **Segment Models** | Large data, distinct segments | Tailored patterns | Need sufficient data per segment |\n",
    "| **Hybrid** | Mixed segment sizes | Best of both | More complex to maintain |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2fbb1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Segment Capacity Analysis\n",
    "categorical_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [ColumnType.CATEGORICAL_NOMINAL, ColumnType.CATEGORICAL_ORDINAL]\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SEGMENT CAPACITY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if findings.target_column and categorical_cols and 'numeric_features' in dir():\n",
    "    # Analyze the first categorical column as potential segment\n",
    "    segment_col = categorical_cols[0]\n",
    "    \n",
    "    print(f\"\\nüìä Analyzing segments by: {segment_col}\")\n",
    "    print(f\"   Features to evaluate: {len(numeric_features)}\")\n",
    "    \n",
    "    segment_result = capacity_analyzer.analyze_segment_capacity(\n",
    "        df,\n",
    "        feature_cols=numeric_features,\n",
    "        target_col=findings.target_column,\n",
    "        segment_col=segment_col,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMENDED STRATEGY: {segment_result.recommended_strategy.replace('_', ' ').title()}\")\n",
    "    print(f\"   Reason: {segment_result.strategy_reason}\")\n",
    "    \n",
    "    # Segment details table\n",
    "    segment_data = []\n",
    "    for seg_name, cap in segment_result.segment_capacities.items():\n",
    "        segment_data.append({\n",
    "            \"Segment\": seg_name,\n",
    "            \"Samples\": cap.total_samples,\n",
    "            \"Minority Events\": cap.minority_class_samples,\n",
    "            \"EPV\": f\"{cap.events_per_variable:.1f}\",\n",
    "            \"Max Features (EPV=10)\": cap.recommended_features_moderate,\n",
    "            \"Status\": cap.capacity_status.title()\n",
    "        })\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_data)\n",
    "    segment_df = segment_df.sort_values(\"Samples\", ascending=False)\n",
    "    display(segment_df)\n",
    "    \n",
    "    # Visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    max_events = 0\n",
    "    for seg_name, cap in segment_result.segment_capacities.items():\n",
    "        color = \"#2ecc71\" if cap.capacity_status == \"adequate\" else \"#f39c12\" if cap.capacity_status == \"limited\" else \"#e74c3c\"\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=seg_name,\n",
    "            x=[seg_name],\n",
    "            y=[cap.minority_class_samples],\n",
    "            marker_color=color,\n",
    "            text=[f\"EPV={cap.events_per_variable:.1f}\"],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "        max_events = max(max_events, cap.minority_class_samples)\n",
    "    \n",
    "    # Add threshold line\n",
    "    threshold_events = len(numeric_features) * 10  # EPV=10 threshold\n",
    "    fig.add_hline(\n",
    "        y=threshold_events,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"#3498db\",\n",
    "        annotation_text=f\"Min events for {len(numeric_features)} features (EPV=10)\",\n",
    "        annotation_position=\"right\"\n",
    "    )\n",
    "    \n",
    "    # Calculate y-axis range to fit labels\n",
    "    y_max = max(max_events, threshold_events)\n",
    "    fig.update_layout(\n",
    "        title=f\"Minority Class Events by Segment ({segment_col})\",\n",
    "        xaxis_title=\"Segment\",\n",
    "        yaxis_title=\"Minority Class Events\",\n",
    "        yaxis_range=[0, y_max * 1.15],  # Add 15% headroom for labels\n",
    "        template='plotly_white',\n",
    "        height=400,\n",
    "        showlegend=False,\n",
    "    )\n",
    "    display_figure(fig)\n",
    "    \n",
    "    print(\"\\nüìã SEGMENT RECOMMENDATIONS:\")\n",
    "    for rec in segment_result.recommendations:\n",
    "        print(f\"   {rec}\")\n",
    "    \n",
    "    if segment_result.viable_segments:\n",
    "        print(f\"\\n   ‚úÖ Viable for separate models: {', '.join(segment_result.viable_segments)}\")\n",
    "    if segment_result.insufficient_segments:\n",
    "        print(f\"   ‚ö†Ô∏è Insufficient data: {', '.join(segment_result.insufficient_segments)}\")\n",
    "    \n",
    "    # Store in findings\n",
    "    findings.metadata[\"segment_capacity\"] = segment_result.to_dict()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No categorical columns available for segment analysis.\")\n",
    "    print(\"   Segment capacity analysis requires at least one categorical column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd537f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 6.3.3 Feature Capacity Action Items\n",
    "\n",
    "Based on the analysis above, here are the key considerations for feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502792c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Capacity Action Items Summary\n",
    "if findings.target_column and 'capacity_result' in dir():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FEATURE CAPACITY ACTION ITEMS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nüìã BASED ON YOUR DATA CAPACITY:\")\n",
    "    \n",
    "    # Action items based on capacity status\n",
    "    if capacity_result.capacity_status == \"adequate\":\n",
    "        print(\"\\n‚úÖ ADEQUATE CAPACITY - You have room to add features\")\n",
    "        print(f\"   ‚Ä¢ Current features: {capacity_result.total_features}\")\n",
    "        print(f\"   ‚Ä¢ Can add up to: {capacity_result.recommended_features_moderate - capacity_result.total_features} more features (EPV=10)\")\n",
    "        print(f\"   ‚Ä¢ Consider: Creating derived features from datetime and categorical columns\")\n",
    "    elif capacity_result.capacity_status == \"limited\":\n",
    "        print(\"\\n‚ö†Ô∏è LIMITED CAPACITY - Be selective with new features\")\n",
    "        print(f\"   ‚Ä¢ Current features: {capacity_result.total_features}\")\n",
    "        print(f\"   ‚Ä¢ Recommended max: {capacity_result.recommended_features_moderate} features (EPV=10)\")\n",
    "        print(f\"   ‚Ä¢ Action: Remove {max(0, capacity_result.total_features - capacity_result.recommended_features_moderate)} redundant features before adding new ones\")\n",
    "        print(f\"   ‚Ä¢ Consider: Using regularization (L1/Lasso) if keeping all features\")\n",
    "    else:\n",
    "        print(\"\\nüî¥ INADEQUATE CAPACITY - Reduce features or get more data\")\n",
    "        print(f\"   ‚Ä¢ Current features: {capacity_result.total_features}\")\n",
    "        print(f\"   ‚Ä¢ Recommended max: {capacity_result.recommended_features_moderate} features (EPV=10)\")\n",
    "        print(f\"   ‚Ä¢ CRITICAL: Reduce to {capacity_result.recommended_features_conservative} features for stable estimates\")\n",
    "        print(f\"   ‚Ä¢ Options: (1) Feature selection, (2) PCA, (3) Collect more data\")\n",
    "    \n",
    "    # Redundancy recommendations\n",
    "    if capacity_result.effective_features_result and capacity_result.effective_features_result.redundant_features:\n",
    "        redundant = capacity_result.effective_features_result.redundant_features\n",
    "        print(f\"\\nüîÑ REDUNDANT FEATURES TO CONSIDER REMOVING:\")\n",
    "        print(f\"   These features are highly correlated with others and add little new information:\")\n",
    "        for feat in redundant[:5]:\n",
    "            print(f\"   ‚Ä¢ {feat}\")\n",
    "        if len(redundant) > 5:\n",
    "            print(f\"   ... and {len(redundant) - 5} more\")\n",
    "    \n",
    "    # New feature budget\n",
    "    print(\"\\nüí∞ FEATURE BUDGET FOR NEW FEATURES:\")\n",
    "    remaining_budget = capacity_result.recommended_features_moderate - capacity_result.total_features\n",
    "    if remaining_budget > 0:\n",
    "        print(f\"   You can safely add {remaining_budget} new features\")\n",
    "        print(\"   Prioritize:\")\n",
    "        print(\"   ‚Ä¢ Recency features (days_since_last_activity)\")\n",
    "        print(\"   ‚Ä¢ Tenure features (days_since_created)\")\n",
    "        print(\"   ‚Ä¢ Engagement composites (email_engagement_score)\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è At or over capacity. Remove {-remaining_budget} features before adding new ones.\")\n",
    "    \n",
    "    # Model selection summary\n",
    "    print(\"\\nüéØ RECOMMENDED MODELING APPROACH:\")\n",
    "    if capacity_result.complexity_guidance:\n",
    "        print(f\"   Model type: {capacity_result.complexity_guidance.recommended_model_type.replace('_', ' ').title()}\")\n",
    "        if \"regularized\" in capacity_result.complexity_guidance.recommended_model_type:\n",
    "            print(\"   ‚Üí Use Lasso (L1) for automatic feature selection\")\n",
    "            print(\"   ‚Üí Use Ridge (L2) if you want to keep all features\")\n",
    "        elif \"tree\" in capacity_result.complexity_guidance.recommended_model_type:\n",
    "            print(\"   ‚Üí Random Forest or XGBoost recommended\")\n",
    "            print(\"   ‚Üí Trees handle correlated features naturally\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df07cd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.4 Datetime Feature Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af13a7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datetime_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type == ColumnType.DATETIME\n",
    "]\n",
    "\n",
    "if datetime_cols:\n",
    "    print(\"Datetime Feature Opportunities:\")\n",
    "    print(\"=\"*50)\n",
    "    for col in datetime_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  - {col}_year: Extract year\")\n",
    "        print(f\"  - {col}_month: Extract month\")\n",
    "        print(f\"  - {col}_day: Extract day of month\")\n",
    "        print(f\"  - {col}_dayofweek: Extract day of week (0-6)\")\n",
    "        print(f\"  - {col}_is_weekend: Is weekend flag\")\n",
    "        print(f\"  - days_since_{col}: Days since date\")\n",
    "else:\n",
    "    print(\"No datetime columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd05d60",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.5 Business-Driven Derived Features\n",
    "\n",
    "These features are based on domain knowledge from the reference analysis (my_take Phase 1).\n",
    "\n",
    "**üìñ Key Derived Features:**\n",
    "- **Tenure Days**: Days from account creation to analysis date\n",
    "- **Days Since Last Order**: Recency indicator (critical for churn)\n",
    "- **Active Period Days**: Duration of customer activity\n",
    "- **Email Engagement Score**: Composite of open rate and click rate\n",
    "- **Click-to-Open Ratio**: Quality of email engagement\n",
    "- **Service Adoption Score**: Sum of service flags (paperless, refill, doorstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083c298",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CREATING DERIVED FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "segmenter = CustomerSegmenter()\n",
    "df_features = df.copy()\n",
    "\n",
    "datetime_cols = [name for name, col in findings.columns.items() \n",
    "                 if col.inferred_type == ColumnType.DATETIME\n",
    "                 and name not in TEMPORAL_METADATA_COLS]\n",
    "binary_cols = [name for name, col in findings.columns.items() \n",
    "               if col.inferred_type == ColumnType.BINARY\n",
    "               and name not in TEMPORAL_METADATA_COLS]\n",
    "numeric_cols = [name for name, col in findings.columns.items() \n",
    "                if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]]\n",
    "\n",
    "for col in datetime_cols:\n",
    "    df_features[col] = pd.to_datetime(df_features[col], errors='coerce', format='mixed')\n",
    "\n",
    "reference_date = pd.Timestamp.now()\n",
    "if datetime_cols:\n",
    "    last_dates = [df_features[col].max() for col in datetime_cols if df_features[col].notna().any()]\n",
    "    if last_dates:\n",
    "        reference_date = max(last_dates)\n",
    "print(f\"\\nReference date: {reference_date}\")\n",
    "\n",
    "print(\"\\nüìÖ TIME-BASED FEATURES:\")\n",
    "created_cols = [c for c in datetime_cols if 'creat' in c.lower() or 'signup' in c.lower() or 'register' in c.lower()]\n",
    "if created_cols:\n",
    "    created_col = created_cols[0]\n",
    "    df_features = segmenter.create_tenure_features(df_features, created_column=created_col, reference_date=reference_date)\n",
    "    print(f\"  ‚úì tenure_days from {created_col}\")\n",
    "    registry.add_silver_derived(\n",
    "        column=\"tenure_days\",\n",
    "        expression=f\"(reference_date - {created_col}).days\",\n",
    "        feature_type=\"tenure\",\n",
    "        rationale=f\"Customer tenure in days from {created_col}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "\n",
    "activity_cols = [c for c in datetime_cols if 'last' in c.lower() or 'recent' in c.lower()]\n",
    "if activity_cols:\n",
    "    activity_col = activity_cols[0]\n",
    "    df_features = segmenter.create_recency_features(df_features, last_activity_column=activity_col, \n",
    "                                                     reference_date=reference_date, output_column='days_since_last_activity')\n",
    "    print(f\"  ‚úì days_since_last_activity from {activity_col}\")\n",
    "    registry.add_silver_derived(\n",
    "        column=\"days_since_last_activity\",\n",
    "        expression=f\"(reference_date - {activity_col}).days\",\n",
    "        feature_type=\"recency\",\n",
    "        rationale=f\"Days since last activity from {activity_col}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüìß ENGAGEMENT FEATURES:\")\n",
    "rate_cols = [c for c in numeric_cols if 'rate' in c.lower() or 'pct' in c.lower() or 'percent' in c.lower()]\n",
    "open_rate_cols = [c for c in rate_cols if 'open' in c.lower()]\n",
    "click_rate_cols = [c for c in rate_cols if 'click' in c.lower()]\n",
    "\n",
    "if open_rate_cols and click_rate_cols:\n",
    "    open_col, click_col = open_rate_cols[0], click_rate_cols[0]\n",
    "    df_features = segmenter.create_engagement_score(df_features, open_rate_column=open_col, \n",
    "                                                     click_rate_column=click_col, output_column='email_engagement_score')\n",
    "    print(f\"  ‚úì email_engagement_score from {open_col}, {click_col}\")\n",
    "    registry.add_silver_derived(\n",
    "        column=\"email_engagement_score\",\n",
    "        expression=f\"0.6 * {open_col} + 0.4 * {click_col}\",\n",
    "        feature_type=\"composite\",\n",
    "        rationale=f\"Weighted engagement score from {open_col} and {click_col}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "    \n",
    "    df_features['click_to_open_rate'] = np.where(df_features[open_col] > 0, df_features[click_col] / df_features[open_col], 0)\n",
    "    print(f\"  ‚úì click_to_open_rate\")\n",
    "    registry.add_silver_ratio(\n",
    "        column=\"click_to_open_rate\",\n",
    "        numerator=click_col,\n",
    "        denominator=open_col,\n",
    "        rationale=f\"Click-to-open ratio: {click_col} / {open_col}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "\n",
    "print(\"\\nüîß SERVICE ADOPTION:\")\n",
    "if binary_cols:\n",
    "    service_binary = [c for c in binary_cols if c != findings.target_column]\n",
    "    if service_binary:\n",
    "        df_features['service_adoption_score'] = df_features[service_binary].sum(axis=1)\n",
    "        print(f\"  ‚úì service_adoption_score from {service_binary}\")\n",
    "        registry.add_silver_derived(\n",
    "            column=\"service_adoption_score\",\n",
    "            expression=f\"sum([{', '.join(service_binary)}])\",\n",
    "            feature_type=\"composite\",\n",
    "            rationale=f\"Service adoption count from {len(service_binary)} binary flags\",\n",
    "            source_notebook=\"06_feature_opportunities\"\n",
    "        )\n",
    "\n",
    "print(\"\\nüí∞ VALUE FEATURES:\")\n",
    "value_cols = [c for c in numeric_cols if 'order' in c.lower() or 'amount' in c.lower() or 'value' in c.lower() or 'avg' in c.lower()]\n",
    "freq_cols = [c for c in numeric_cols if 'freq' in c.lower() or 'count' in c.lower()]\n",
    "if value_cols and freq_cols:\n",
    "    df_features['value_frequency_product'] = df_features[value_cols[0]] * df_features[freq_cols[0]]\n",
    "    print(f\"  ‚úì value_frequency_product from {value_cols[0]}, {freq_cols[0]}\")\n",
    "    registry.add_silver_interaction(\n",
    "        column=\"value_frequency_product\",\n",
    "        features=[value_cols[0], freq_cols[0]],\n",
    "        rationale=f\"Value-frequency interaction: {value_cols[0]} √ó {freq_cols[0]}\",\n",
    "        source_notebook=\"06_feature_opportunities\"\n",
    "    )\n",
    "\n",
    "new_cols = len(df_features.columns) - len(df.columns)\n",
    "print(f\"\\n‚úì Created {new_cols} new features (total: {len(df_features.columns)})\")\n",
    "print(f\"‚úÖ Persisted {len([c for c in ['tenure_days', 'days_since_last_activity', 'email_engagement_score', 'click_to_open_rate', 'service_adoption_score', 'value_frequency_product'] if c in df_features.columns])} derived feature recommendations to registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25431ef3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.6 Customer Segmentation Features\n",
    "\n",
    "Create business-meaningful segments for analysis and modeling.\n",
    "\n",
    "**üìñ Segmentation Strategy:**\n",
    "- **Value Dimension**: High vs Low (based on avgorder median)\n",
    "- **Frequency Dimension**: Frequent vs Infrequent (based on ordfreq median)\n",
    "- **Recency Buckets**: Active, Recent, Lapsing, Dormant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b8fe4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CUSTOMER SEGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüéØ VALUE-FREQUENCY SEGMENTS:\")\n",
    "value_cols = [c for c in numeric_cols if 'order' in c.lower() or 'amount' in c.lower() or 'value' in c.lower() or 'avg' in c.lower()]\n",
    "freq_cols = [c for c in numeric_cols if 'freq' in c.lower() or 'count' in c.lower()]\n",
    "\n",
    "if value_cols and freq_cols:\n",
    "    df_features, vf_result = segmenter.segment_by_value_frequency(\n",
    "        df_features, value_column=value_cols[0], frequency_column=freq_cols[0])\n",
    "    print(f\"  Using {value_cols[0]} √ó {freq_cols[0]}\")\n",
    "    for seg in vf_result.segments:\n",
    "        print(f\"    {seg.name}: {seg.count:,} ({seg.percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No suitable value/frequency columns found\")\n",
    "\n",
    "print(\"\\nüìÖ RECENCY SEGMENTS:\")\n",
    "if 'days_since_last_activity' in df_features.columns:\n",
    "    df_features, recency_result = segmenter.segment_by_recency(df_features, days_since_column='days_since_last_activity')\n",
    "    for seg in recency_result.segments:\n",
    "        print(f\"    {seg.name}: {seg.count:,} ({seg.percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"  No recency column available\")\n",
    "\n",
    "print(\"\\nüìß ENGAGEMENT SEGMENTS:\")\n",
    "if 'email_engagement_score' in df_features.columns:\n",
    "    max_score = df_features['email_engagement_score'].max()\n",
    "    if max_score > 0:\n",
    "        df_features['engagement_normalized'] = df_features['email_engagement_score'] / max_score\n",
    "        df_features, eng_result = segmenter.segment_by_engagement(df_features, engagement_column='engagement_normalized')\n",
    "        for seg in eng_result.segments:\n",
    "            print(f\"    {seg.name}: {seg.count:,} ({seg.percentage:.1f}%)\")\n",
    "        df_features = df_features.drop(columns=['engagement_normalized'])\n",
    "else:\n",
    "    print(\"  No engagement score available\")\n",
    "\n",
    "if 'customer_segment' in df_features.columns and findings.target_column and findings.target_column in df_features.columns:\n",
    "    target = findings.target_column\n",
    "    segment_retention = df_features.groupby('customer_segment')[target].mean() * 100\n",
    "    \n",
    "    max_rate = segment_retention.max()\n",
    "    fig = go.Figure(go.Bar(\n",
    "        x=segment_retention.index, y=segment_retention.values,\n",
    "        marker_color=['#2ca02c' if r > 70 else '#ffbb00' if r > 50 else '#d62728' for r in segment_retention.values],\n",
    "        text=[f'{r:.1f}%' for r in segment_retention.values], textposition='outside'))\n",
    "    fig.update_layout(\n",
    "        title='Retention Rate by Customer Segment', \n",
    "        xaxis_title='Segment', \n",
    "        yaxis_title='Retention Rate (%)',\n",
    "        yaxis_range=[0, max_rate * 1.15],  # Add 15% headroom for labels\n",
    "        template='plotly_white', \n",
    "        height=400,\n",
    "    )\n",
    "    display_figure(fig)\n",
    "\n",
    "segment_cols = [c for c in df_features.columns if 'segment' in c.lower() or 'bucket' in c.lower()]\n",
    "print(f\"\\n‚úì Created {len(segment_cols)} segmentation features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f04e97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.7 Numeric Transformation Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a554a0f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]\n",
    "    and name not in TEMPORAL_METADATA_COLS\n",
    "]\n",
    "\n",
    "transform_count = 0\n",
    "if numeric_cols:\n",
    "    print(\"Numeric Transformation Opportunities:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for col_name in numeric_cols:\n",
    "        col_info = findings.columns[col_name]\n",
    "        series = df[col_name].dropna()\n",
    "        skewness = series.skew()\n",
    "        \n",
    "        print(f\"\\n{col_name}:\")\n",
    "        print(f\"  Skewness: {skewness:.2f}\")\n",
    "        \n",
    "        if abs(skewness) > 1:\n",
    "            print(f\"  Recommendation: Apply log transform (highly skewed)\")\n",
    "            registry.add_gold_transformation(\n",
    "                column=col_name,\n",
    "                transform=\"log\",\n",
    "                parameters={\"skewness\": float(skewness), \"reason\": \"highly_skewed\"},\n",
    "                rationale=f\"Log transform for highly skewed distribution (skewness={skewness:.2f})\",\n",
    "                source_notebook=\"06_feature_opportunities\"\n",
    "            )\n",
    "            transform_count += 1\n",
    "        elif abs(skewness) > 0.5:\n",
    "            print(f\"  Recommendation: Consider sqrt transform (moderately skewed)\")\n",
    "            registry.add_gold_transformation(\n",
    "                column=col_name,\n",
    "                transform=\"sqrt\",\n",
    "                parameters={\"skewness\": float(skewness), \"reason\": \"moderately_skewed\"},\n",
    "                rationale=f\"Sqrt transform for moderately skewed distribution (skewness={skewness:.2f})\",\n",
    "                source_notebook=\"06_feature_opportunities\"\n",
    "            )\n",
    "            transform_count += 1\n",
    "        else:\n",
    "            print(f\"  Recommendation: Standard scaling sufficient\")\n",
    "            registry.add_gold_scaling(\n",
    "                column=col_name,\n",
    "                method=\"standard\",\n",
    "                rationale=f\"Standard scaling for normally distributed column (skewness={skewness:.2f})\",\n",
    "                source_notebook=\"06_feature_opportunities\"\n",
    "            )\n",
    "            transform_count += 1\n",
    "        \n",
    "        if col_info.inferred_type == ColumnType.NUMERIC_CONTINUOUS:\n",
    "            print(f\"  Binning: Consider creating bins for {col_name}_binned\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Persisted {transform_count} transformation recommendations to registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01f480",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.8 Categorical Encoding Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb3bfc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    name for name, col in findings.columns.items()\n",
    "    if col.inferred_type in [ColumnType.CATEGORICAL_NOMINAL, ColumnType.CATEGORICAL_ORDINAL]\n",
    "    and name not in TEMPORAL_METADATA_COLS\n",
    "]\n",
    "\n",
    "encoding_count = 0\n",
    "if categorical_cols:\n",
    "    print(\"Categorical Encoding Recommendations:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for col_name in categorical_cols:\n",
    "        col_info = findings.columns[col_name]\n",
    "        distinct = col_info.universal_metrics.get(\"distinct_count\", 0)\n",
    "        \n",
    "        print(f\"\\n{col_name}: ({distinct} unique values)\")\n",
    "        \n",
    "        if distinct <= 5:\n",
    "            print(f\"  Recommendation: One-hot encoding\")\n",
    "            registry.add_gold_encoding(\n",
    "                column=col_name,\n",
    "                method=\"onehot\",\n",
    "                rationale=f\"One-hot encoding for low cardinality ({distinct} unique values)\",\n",
    "                source_notebook=\"06_feature_opportunities\"\n",
    "            )\n",
    "            encoding_count += 1\n",
    "        elif distinct <= 20:\n",
    "            print(f\"  Recommendation: Target encoding or one-hot with frequency threshold\")\n",
    "            registry.add_gold_encoding(\n",
    "                column=col_name,\n",
    "                method=\"target\",\n",
    "                rationale=f\"Target encoding for medium cardinality ({distinct} unique values)\",\n",
    "                source_notebook=\"06_feature_opportunities\"\n",
    "            )\n",
    "            encoding_count += 1\n",
    "        else:\n",
    "            print(f\"  Recommendation: Target encoding or embedding (high cardinality)\")\n",
    "            registry.add_gold_encoding(\n",
    "                column=col_name,\n",
    "                method=\"target\",\n",
    "                rationale=f\"Target encoding for high cardinality ({distinct} unique values)\",\n",
    "                source_notebook=\"06_feature_opportunities\"\n",
    "            )\n",
    "            encoding_count += 1\n",
    "        \n",
    "        if col_info.inferred_type == ColumnType.CATEGORICAL_ORDINAL:\n",
    "            print(f\"  Note: Consider ordinal encoding to preserve order\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Persisted {encoding_count} encoding recommendations to registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e743f8f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Learned\n",
    "\n",
    "In this notebook, we identified feature engineering opportunities and analyzed data capacity:\n",
    "\n",
    "### Feature Capacity Analysis\n",
    "1. **Events Per Variable (EPV)** - Calculated the data's capacity to support features\n",
    "2. **Effective Features** - Identified redundant features due to high correlation\n",
    "3. **Model Complexity Guidance** - Determined appropriate model types based on data size\n",
    "4. **Segment Capacity** - Evaluated whether segmented modeling is viable\n",
    "\n",
    "### Feature Engineering\n",
    "5. **Automated Recommendations** - Framework suggested feature opportunities\n",
    "6. **Time-Based Features** - Created tenure, recency, active period metrics\n",
    "7. **Engagement Scores** - Built composite email engagement metrics\n",
    "8. **Customer Segments** - Created value-frequency and recency-based segments\n",
    "9. **Encoding Strategies** - Identified optimal encoding for each categorical\n",
    "\n",
    "## Feature Capacity Key Concepts\n",
    "\n",
    "| Metric | What It Means | Rule of Thumb |\n",
    "|--------|---------------|---------------|\n",
    "| **EPV ‚â• 20** | Stable, reliable estimates | Conservative, regulatory-grade |\n",
    "| **EPV = 10-20** | Standard practice | Use for most applications |\n",
    "| **EPV = 5-10** | Limited capacity | Requires strong regularization |\n",
    "| **EPV < 5** | High risk | Reduce features or get more data |\n",
    "\n",
    "## Key Derived Features Created\n",
    "\n",
    "| Feature | Formula | Business Meaning |\n",
    "|---------|---------|-----------------|\n",
    "| `tenure_days` | reference_date - created | Customer longevity |\n",
    "| `days_since_last_order` | reference_date - lastorder | Recency/engagement |\n",
    "| `email_engagement_score` | 0.6√óopenrate + 0.4√óclickrate | Overall engagement |\n",
    "| `service_adoption_score` | paperless + refill + doorstep | Service utilization |\n",
    "| `customer_segment` | Value √ó Frequency quadrant | Customer type |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **07_modeling_readiness.ipynb** to:\n",
    "- Validate data is ready for modeling\n",
    "- Check for data leakage\n",
    "- Assess class imbalance\n",
    "- Review feature completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e34de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Potential Interaction Features:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(numeric_cols) >= 2:\n",
    "    print(\"\\nNumeric Interactions:\")\n",
    "    for i, col1 in enumerate(numeric_cols[:3]):\n",
    "        for col2 in numeric_cols[i+1:4]:\n",
    "            print(f\"  - {col1}_x_{col2}: Multiplication\")\n",
    "            print(f\"  - {col1}_div_{col2}: Division (if {col2} > 0)\")\n",
    "\n",
    "if categorical_cols and numeric_cols:\n",
    "    print(\"\\nCategorical-Numeric Interactions:\")\n",
    "    for cat_col in categorical_cols[:2]:\n",
    "        for num_col in numeric_cols[:2]:\n",
    "            print(f\"  - {num_col}_by_{cat_col}_mean: Group mean\")\n",
    "            print(f\"  - {num_col}_by_{cat_col}_std: Group std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a4bf8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6.9 Feature Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c887ca3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_summary = []\n",
    "for rec in feature_recs:\n",
    "    feature_summary.append({\n",
    "        \"Feature Name\": rec.feature_name,\n",
    "        \"Source\": rec.source_column,\n",
    "        \"Type\": rec.feature_type,\n",
    "        \"Priority\": rec.priority\n",
    "    })\n",
    "\n",
    "if feature_summary:\n",
    "    summary_df = pd.DataFrame(feature_summary)\n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b341679",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **07_modeling_readiness.ipynb** to validate data is ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a7178",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save recommendations\n",
    "with open(RECOMMENDATIONS_PATH, \"w\") as f:\n",
    "    yaml.dump(registry.to_dict(), f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(registry.all_recommendations)} recommendations to {RECOMMENDATIONS_PATH}\")\n",
    "print(f\"\\nRecommendations by layer:\")\n",
    "for layer in [\"bronze\", \"silver\", \"gold\"]:\n",
    "    recs = registry.get_by_layer(layer)\n",
    "    print(f\"  {layer.upper()}: {len(recs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.588345,
   "end_time": "2026-01-22T14:18:13.127764",
   "environment_variables": {},
   "exception": true,
   "input_path": "exploration_notebooks/06_feature_opportunities.ipynb",
   "output_path": "docs/tutorial/executed/06_feature_opportunities.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T14:18:10.539419",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}