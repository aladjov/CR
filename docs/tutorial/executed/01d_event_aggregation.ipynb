{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edacdaf6",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.001977,
     "end_time": "2026-01-22T14:17:53.739192",
     "exception": false,
     "start_time": "2026-01-22T14:17:53.737215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chapter 1d: Event Aggregation (Event Bronze Track → Entity Bronze Track)\n",
    "\n",
    "**Purpose:** Aggregate event-level data to entity-level, producing a dataset ready for standard exploration.\n",
    "\n",
    "**When to use this notebook:**\n",
    "- After completing 01a, 01b, 01c (temporal exploration)\n",
    "- Your dataset is EVENT_LEVEL granularity\n",
    "- You want to create entity-level features from events\n",
    "\n",
    "**What this notebook produces:**\n",
    "- Aggregated parquet file (one row per entity)\n",
    "- New findings file for the aggregated data\n",
    "- Updated original findings with aggregation metadata\n",
    "\n",
    "**Aggregation Strategy:**\n",
    "\n",
    "| Feature Type | Examples | Purpose |\n",
    "|--------------|----------|--------|\n",
    "| **Event Counts** | event_count_7d, event_count_30d | Activity level |\n",
    "| **Value Aggregations** | amount_sum_30d, clicks_mean_7d | Behavior magnitude |\n",
    "| **Recency** | days_since_last_event | Recent engagement |\n",
    "| **Tenure** | days_since_first_event | Customer lifecycle |\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Shape Transformation\n",
    "\n",
    "```\n",
    "EVENT-LEVEL (input)              ENTITY-LEVEL (output)\n",
    "┌─────────────────────┐          ┌─────────────────────────────────────┐\n",
    "│ customer │ date     │          │ customer │ events_7d │ events_30d │ ...\n",
    "├──────────┼──────────┤    →     ├──────────┼───────────┼────────────┤\n",
    "│ A        │ Jan 1    │          │ A        │ 3         │ 12         │\n",
    "│ A        │ Jan 5    │          │ B        │ 1         │ 5          │\n",
    "│ A        │ Jan 10   │          │ C        │ 0         │ 2          │\n",
    "│ B        │ Jan 3    │          └──────────┴───────────┴────────────┘\n",
    "│ ...      │ ...      │\n",
    "└──────────┴──────────┘\n",
    "Many rows per entity           One row per entity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {
    "papermill": {
     "duration": 0.001224,
     "end_time": "2026-01-22T14:17:53.742164",
     "exception": false,
     "start_time": "2026-01-22T14:17:53.740940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:17:53.746007Z",
     "iopub.status.busy": "2026-01-22T14:17:53.745895Z",
     "iopub.status.idle": "2026-01-22T14:17:55.123962Z",
     "shell.execute_reply": "2026-01-22T14:17:55.123534Z"
    },
    "papermill": {
     "duration": 1.381383,
     "end_time": "2026-01-22T14:17:55.124733",
     "exception": false,
     "start_time": "2026-01-22T14:17:53.743350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings, DataExplorer\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType, DatasetGranularity\n",
    "from customer_retention.stages.profiling import TimeWindowAggregator\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036e484",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T14:17:55.128242Z",
     "iopub.status.busy": "2026-01-22T14:17:55.128099Z",
     "iopub.status.idle": "2026-01-22T14:17:55.259335Z",
     "shell.execute_reply": "2026-01-22T14:17:55.258627Z"
    },
    "papermill": {
     "duration": 0.133957,
     "end_time": "2026-01-22T14:17:55.260265",
     "exception": true,
     "start_time": "2026-01-22T14:17:55.126308",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No findings files found in ../experiments/findings. Run notebook 01 first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m findings_files = [\n\u001b[32m      6\u001b[39m     f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m FINDINGS_DIR.glob(\u001b[33m\"\u001b[39m\u001b[33m*_findings.yaml\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmulti_dataset\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f.name \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_aggregated\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f.name\n\u001b[32m      8\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m findings_files:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo findings files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFINDINGS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run notebook 01 first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m findings_files.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m f: f.stat().st_mtime, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m FINDINGS_PATH = \u001b[38;5;28mstr\u001b[39m(findings_files[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No findings files found in ../experiments/findings. Run notebook 01 first."
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "# Find findings files (exclude multi_dataset and already-aggregated)\n",
    "findings_files = [\n",
    "    f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") \n",
    "    if \"multi_dataset\" not in f.name and \"_aggregated\" not in f.name\n",
    "]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Loaded findings for {findings.column_count} columns from {findings.source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify this is event-level data\n",
    "if not findings.is_time_series:\n",
    "    print(\"\\u26a0\\ufe0f This dataset is NOT event-level. Aggregation not needed.\")\n",
    "    print(\"   Proceed directly to 02_column_deep_dive.ipynb\")\n",
    "    raise SystemExit(\"Skipping aggregation - data is already entity-level\")\n",
    "\n",
    "ts_meta = findings.time_series_metadata\n",
    "ENTITY_COLUMN = ts_meta.entity_column\n",
    "TIME_COLUMN = ts_meta.time_column\n",
    "\n",
    "print(f\"\\u2705 Dataset confirmed as EVENT-LEVEL\")\n",
    "print(f\"   Entity column: {ENTITY_COLUMN}\")\n",
    "print(f\"   Time column: {TIME_COLUMN}\")\n",
    "print(f\"   Unique entities: {ts_meta.unique_entities:,}\")\n",
    "print(f\"   Avg events/entity: {ts_meta.avg_events_per_entity:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "\n",
    "# Load source data (prefers snapshots over raw files)\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\n",
    "df[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN])\n",
    "charts = ChartBuilder()\n",
    "\n",
    "print(f\"Loaded {len(df):,} events x {len(df.columns)} columns\")\n",
    "print(f\"Data source: {data_source}\")\n",
    "print(f\"Date range: {df[TIME_COLUMN].min()} to {df[TIME_COLUMN].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1d.2 Configure Aggregation\n",
    "\n",
    "Configure the time windows and aggregation functions to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === AGGREGATION CONFIGURATION ===\n",
    "\n",
    "# Time windows (from findings or defaults)\n",
    "DEFAULT_WINDOWS = [\"7d\", \"30d\", \"90d\", \"180d\", \"365d\", \"all_time\"]\n",
    "WINDOWS = ts_meta.suggested_aggregations if ts_meta.suggested_aggregations else DEFAULT_WINDOWS\n",
    "\n",
    "# Reference date for window calculations\n",
    "# Options: use max date in data, or a fixed date\n",
    "REFERENCE_DATE = df[TIME_COLUMN].max()\n",
    "# REFERENCE_DATE = pd.Timestamp(\"2024-01-01\")  # Uncomment to use fixed date\n",
    "\n",
    "# Value columns to aggregate (numeric columns excluding entity/time)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "VALUE_COLUMNS = [c for c in numeric_cols if c not in [ENTITY_COLUMN]]\n",
    "\n",
    "# Aggregation functions\n",
    "AGG_FUNCTIONS = [\"sum\", \"mean\", \"max\", \"count\"]\n",
    "\n",
    "# Include recency and tenure features\n",
    "INCLUDE_RECENCY = True\n",
    "INCLUDE_TENURE = True\n",
    "\n",
    "print(\"Aggregation Configuration:\")\n",
    "print(f\"   Windows: {WINDOWS}\")\n",
    "print(f\"   Reference date: {REFERENCE_DATE}\")\n",
    "print(f\"   Value columns: {VALUE_COLUMNS[:5]}{'...' if len(VALUE_COLUMNS) > 5 else ''}\")\n",
    "print(f\"   Aggregation functions: {AGG_FUNCTIONS}\")\n",
    "print(f\"   Include recency: {INCLUDE_RECENCY}\")\n",
    "print(f\"   Include tenure: {INCLUDE_TENURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1d.3 Preview Aggregation Plan\n",
    "\n",
    "See what features will be created before executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize aggregator\n",
    "aggregator = TimeWindowAggregator(\n",
    "    entity_column=ENTITY_COLUMN,\n",
    "    time_column=TIME_COLUMN\n",
    ")\n",
    "\n",
    "# Generate plan\n",
    "plan = aggregator.generate_plan(\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEntity column: {plan.entity_column}\")\n",
    "print(f\"Time column: {plan.time_column}\")\n",
    "print(f\"Windows: {[w.name for w in plan.windows]}\")\n",
    "print(f\"\\nFeatures to be created ({len(plan.feature_columns)}):\")\n",
    "for feat in plan.feature_columns[:20]:\n",
    "    print(f\"   - {feat}\")\n",
    "if len(plan.feature_columns) > 20:\n",
    "    print(f\"   ... and {len(plan.feature_columns) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1d.4 Execute Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Executing aggregation...\")\n",
    "print(f\"   Input: {len(df):,} events\")\n",
    "print(f\"   Expected output: {df[ENTITY_COLUMN].nunique():,} entities\")\n",
    "\n",
    "df_aggregated = aggregator.aggregate(\n",
    "    df,\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    reference_date=REFERENCE_DATE,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "print(f\"\\n\\u2705 Aggregation complete!\")\n",
    "print(f\"   Output: {len(df_aggregated):,} entities x {len(df_aggregated.columns)} features\")\n",
    "print(f\"   Memory: {df_aggregated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview aggregated data\n",
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(df_aggregated.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nFeature Summary Statistics:\")\n",
    "display(df_aggregated.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1d.5 Quality Check on Aggregated Data\n",
    "\n",
    "Quick validation of the aggregated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AGGREGATED DATA QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for nulls\n",
    "null_counts = df_aggregated.isnull().sum()\n",
    "cols_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "if len(cols_with_nulls) > 0:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Columns with null values ({len(cols_with_nulls)}):\")\n",
    "    for col, count in cols_with_nulls.items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
    "    print(\"\\n   Note: Nulls in aggregated features typically mean no events in that window.\")\n",
    "    print(\"   Consider filling with 0 for count/sum features.\")\n",
    "else:\n",
    "    print(\"\\n\\u2705 No null values in aggregated data\")\n",
    "\n",
    "# Check entity count matches\n",
    "original_entities = df[ENTITY_COLUMN].nunique()\n",
    "aggregated_entities = len(df_aggregated)\n",
    "\n",
    "if original_entities == aggregated_entities:\n",
    "    print(f\"\\n\\u2705 Entity count matches: {aggregated_entities:,}\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Entity count mismatch!\")\n",
    "    print(f\"   Original: {original_entities:,}\")\n",
    "    print(f\"   Aggregated: {aggregated_entities:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1d.6 Save Aggregated Data and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate output paths\n",
    "original_name = Path(findings.source_path).stem\n",
    "findings_name = Path(FINDINGS_PATH).stem.replace(\"_findings\", \"\")\n",
    "\n",
    "# Save aggregated data as parquet\n",
    "AGGREGATED_DATA_PATH = FINDINGS_DIR / f\"{findings_name}_aggregated.parquet\"\n",
    "df_aggregated.to_parquet(AGGREGATED_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"\\u2705 Aggregated data saved to: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   Size: {AGGREGATED_DATA_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new findings for aggregated data using DataExplorer\n",
    "print(\"\\nGenerating findings for aggregated data...\")\n",
    "\n",
    "explorer = DataExplorer(output_dir=str(FINDINGS_DIR))\n",
    "aggregated_findings = explorer.explore(\n",
    "    str(AGGREGATED_DATA_PATH),\n",
    "    dataset_name=f\"{findings_name}_aggregated\"\n",
    ")\n",
    "\n",
    "AGGREGATED_FINDINGS_PATH = explorer.last_findings_path\n",
    "print(f\"\\u2705 Aggregated findings saved to: {AGGREGATED_FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update original findings with aggregation metadata\n",
    "findings.time_series_metadata.aggregation_executed = True\n",
    "findings.time_series_metadata.aggregated_data_path = str(AGGREGATED_DATA_PATH)\n",
    "findings.time_series_metadata.aggregated_findings_path = str(AGGREGATED_FINDINGS_PATH)\n",
    "findings.time_series_metadata.aggregation_windows_used = WINDOWS\n",
    "findings.time_series_metadata.aggregation_timestamp = datetime.now().isoformat()\n",
    "\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"\\u2705 Original findings updated with aggregation metadata: {FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summary of outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATION COMPLETE - OUTPUT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n\\U0001f4c1 Files created:\")\n",
    "print(f\"   1. Aggregated data: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   2. Aggregated findings: {AGGREGATED_FINDINGS_PATH}\")\n",
    "print(f\"   3. Updated original findings: {FINDINGS_PATH}\")\n",
    "\n",
    "print(f\"\\n\\U0001f4ca Aggregation stats:\")\n",
    "print(f\"   Input events: {len(df):,}\")\n",
    "print(f\"   Output entities: {len(df_aggregated):,}\")\n",
    "print(f\"   Features created: {len(df_aggregated.columns)}\")\n",
    "print(f\"   Windows used: {WINDOWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Did\n",
    "\n",
    "In this notebook, we transformed event-level data to entity-level:\n",
    "\n",
    "1. **Loaded event data** with entity and time columns\n",
    "2. **Configured aggregation** windows and functions\n",
    "3. **Executed aggregation** using TimeWindowAggregator\n",
    "4. **Quality checked** the aggregated output\n",
    "5. **Saved outputs** - data file, findings, and metadata\n",
    "\n",
    "## Output Files\n",
    "\n",
    "| File | Purpose | Next Use |\n",
    "|------|---------|----------|\n",
    "| `*_aggregated.parquet` | Entity-level data | Input for notebooks 02-04 |\n",
    "| `*_aggregated_findings.yaml` | Auto-profiled findings | Loaded by 02_column_deep_dive |\n",
    "| Original findings (updated) | Aggregation tracking | Reference |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Event Bronze Track complete!** Continue with the **Entity Bronze Track** on the aggregated data:\n",
    "\n",
    "1. **02_column_deep_dive.ipynb** - Profile the aggregated feature distributions\n",
    "2. **03_quality_assessment.ipynb** - Run quality checks on entity-level data\n",
    "3. **04_relationship_analysis.ipynb** - Analyze feature correlations and target relationships\n",
    "\n",
    "The notebooks will auto-discover the aggregated findings file (most recently modified).\n",
    "\n",
    "```python\n",
    "# The aggregated findings file is now the most recent, so notebooks 02-04\n",
    "# will automatically use it via the standard discovery pattern.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.548522,
   "end_time": "2026-01-22T14:17:55.579357",
   "environment_variables": {},
   "exception": true,
   "input_path": "exploration_notebooks/01d_event_aggregation.ipynb",
   "output_path": "docs/tutorial/executed/01d_event_aggregation.ipynb",
   "parameters": {},
   "start_time": "2026-01-22T14:17:53.030835",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}