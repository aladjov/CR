{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "papermill": {
     "duration": 0.002485,
     "end_time": "2026-01-29T23:48:25.562272",
     "exception": false,
     "start_time": "2026-01-29T23:48:25.559787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Chapter 1d: Event Aggregation (Event Bronze Track â†’ Entity Bronze Track)\n",
    "\n",
    "**Purpose:** Aggregate event-level data to entity-level, applying all insights from 01a-01c.\n",
    "\n",
    "**When to use this notebook:**\n",
    "- After completing 01a (temporal profiling), 01b (quality checks), 01c (pattern analysis)\n",
    "- Your dataset is EVENT_LEVEL granularity\n",
    "- You want to create entity-level features informed by temporal patterns\n",
    "\n",
    "**What this notebook produces:**\n",
    "- Aggregated parquet file (one row per entity)\n",
    "- New findings file for the aggregated data\n",
    "- Updated original findings with aggregation metadata\n",
    "\n",
    "**How 01a-01c findings inform aggregation:**\n",
    "\n",
    "| Source | Insight Applied |\n",
    "|--------|----------------|\n",
    "| **01a** | Recommended windows (e.g., 180d, 365d), lifecycle quadrant feature |\n",
    "| **01b** | Quality issues to handle (gaps, duplicates) |\n",
    "| **01c** | Divergent columns for velocity/momentum (prioritize these features) |\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Shape Transformation\n",
    "\n",
    "```\n",
    "EVENT-LEVEL (input)              ENTITY-LEVEL (output)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ customer â”‚ date     â”‚          â”‚ customer â”‚ events_180d â”‚ quadrant â”‚ ...\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â†’     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ A        â”‚ Jan 1    â”‚          â”‚ A        â”‚ 12          â”‚ Steady   â”‚\n",
    "â”‚ A        â”‚ Jan 5    â”‚          â”‚ B        â”‚ 5           â”‚ Brief    â”‚\n",
    "â”‚ A        â”‚ Jan 10   â”‚          â”‚ C        â”‚ 2           â”‚ Loyal    â”‚\n",
    "â”‚ B        â”‚ Jan 3    â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚ ...      â”‚ ...      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "Many rows per entity           One row per entity + lifecycle features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {
    "papermill": {
     "duration": 0.001684,
     "end_time": "2026-01-29T23:48:25.566296",
     "exception": false,
     "start_time": "2026-01-29T23:48:25.564612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:25.571006Z",
     "iopub.status.busy": "2026-01-29T23:48:25.570861Z",
     "iopub.status.idle": "2026-01-29T23:48:26.990899Z",
     "shell.execute_reply": "2026-01-29T23:48:26.990329Z"
    },
    "papermill": {
     "duration": 1.42386,
     "end_time": "2026-01-29T23:48:26.992027",
     "exception": false,
     "start_time": "2026-01-29T23:48:25.568167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings, DataExplorer\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType, DatasetGranularity\n",
    "from customer_retention.stages.profiling import (\n",
    "    AggregationFeatureConfig,\n",
    "    TimeWindowAggregator,\n",
    "    TimeSeriesProfiler,\n",
    "    classify_lifecycle_quadrants,\n",
    "    classify_activity_segments,\n",
    "    create_momentum_ratio_features,\n",
    "    create_recency_bucket_feature,\n",
    "    deduplicate_events,\n",
    "    get_duplicate_event_count,\n",
    ")\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from customer_retention.core.config.experiments import FINDINGS_DIR, EXPERIMENTS_DIR, OUTPUT_DIR, setup_experiments_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:26.998040Z",
     "iopub.status.busy": "2026-01-29T23:48:26.997853Z",
     "iopub.status.idle": "2026-01-29T23:48:27.034617Z",
     "shell.execute_reply": "2026-01-29T23:48:27.034180Z"
    },
    "papermill": {
     "duration": 0.040995,
     "end_time": "2026-01-29T23:48:27.035362",
     "exception": false,
     "start_time": "2026-01-29T23:48:26.994367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_findings.yaml\n",
      "Loaded findings for 16 columns from ../tests/fixtures/customer_emails.csv\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "# FINDINGS_DIR imported from customer_retention.core.config.experiments\n",
    "\n",
    "# Find findings files (exclude multi_dataset and already-aggregated)\n",
    "findings_files = [\n",
    "    f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") \n",
    "    if \"multi_dataset\" not in f.name and \"_aggregated\" not in f.name\n",
    "]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Loaded findings for {findings.column_count} columns from {findings.source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:27.039884Z",
     "iopub.status.busy": "2026-01-29T23:48:27.039765Z",
     "iopub.status.idle": "2026-01-29T23:48:27.084122Z",
     "shell.execute_reply": "2026-01-29T23:48:27.083361Z"
    },
    "papermill": {
     "duration": 0.047427,
     "end_time": "2026-01-29T23:48:27.084795",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.037368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINDINGS SUMMARY FROM 01a-01c\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š FROM 01a (Temporal Profiling):\n",
      "   Entity column: customer_id\n",
      "   Time column: feature_timestamp\n",
      "   Unique entities: 4,998\n",
      "   Avg events/entity: 15.0\n",
      "   Time span: 2,825 days\n",
      "\n",
      "   âœ… Recommended windows: ['180d', '365d', 'all_time']\n",
      "\n",
      "   ðŸ“‹ Segmentation recommendation:\n",
      "      Add lifecycle_quadrant as a categorical feature to the model\n",
      "      Heterogeneity: high\n",
      "\n",
      "   âš ï¸ Drift risk: HIGH\n",
      "      Volume drift: declining\n",
      "      Population stability: 0.66\n",
      "\n",
      "ðŸ“‹ FROM 01b (Temporal Quality):\n",
      "   Quality score: 96.3\n",
      "   Quality grade: A\n",
      "   âš ï¸ Duplicate events: 371\n",
      "\n",
      "ðŸ“ˆ FROM 01c (Temporal Patterns):\n",
      "   Velocity window: 180 days\n",
      "   Momentum pairs: [[180, 365]]\n",
      "\n",
      "   Trend: stable (strength: 0.47)\n",
      "   Seasonality: weekly (7d), tri-weekly (21d), bi-weekly (14d)\n",
      "\n",
      "   ðŸ“‹ Seasonality Recommendations:\n",
      "      â†’ Add day_of_week with sin_cos encoding\n",
      "      â†’ Warning: Windows don't align with cycles [7, 21, 14]\n",
      "   Recency: median=246 days, target_corr=0.77\n",
      "\n",
      "   ðŸ‘¥ Cohort: Skip features - 90% onboarded in 2015 - insufficient variation\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify this is event-level data and display findings summary\n",
    "if not findings.is_time_series:\n",
    "    print(\"âš ï¸ This dataset is NOT event-level. Aggregation not needed.\")\n",
    "    print(\"   Proceed directly to 02_column_deep_dive.ipynb\")\n",
    "    raise SystemExit(\"Skipping aggregation - data is already entity-level\")\n",
    "\n",
    "ts_meta = findings.time_series_metadata\n",
    "ENTITY_COLUMN = ts_meta.entity_column\n",
    "TIME_COLUMN = ts_meta.time_column\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINDINGS SUMMARY FROM 01a-01c\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === 01a: Time Series Metadata ===\n",
    "print(\"\\nðŸ“Š FROM 01a (Temporal Profiling):\")\n",
    "print(f\"   Entity column: {ENTITY_COLUMN}\")\n",
    "print(f\"   Time column: {TIME_COLUMN}\")\n",
    "if ts_meta.unique_entities:\n",
    "    print(f\"   Unique entities: {ts_meta.unique_entities:,}\")\n",
    "if ts_meta.avg_events_per_entity:\n",
    "    print(f\"   Avg events/entity: {ts_meta.avg_events_per_entity:.1f}\")\n",
    "if ts_meta.time_span_days:\n",
    "    print(f\"   Time span: {ts_meta.time_span_days:,} days\")\n",
    "\n",
    "if ts_meta.suggested_aggregations:\n",
    "    print(f\"\\n   âœ… Recommended windows: {ts_meta.suggested_aggregations}\")\n",
    "else:\n",
    "    print(\"\\n   âš ï¸ No window recommendations - will use defaults\")\n",
    "\n",
    "if ts_meta.temporal_segmentation_recommendation:\n",
    "    print(f\"\\n   ðŸ“‹ Segmentation recommendation:\")\n",
    "    print(f\"      {ts_meta.temporal_segmentation_recommendation}\")\n",
    "    if ts_meta.heterogeneity_level:\n",
    "        print(f\"      Heterogeneity: {ts_meta.heterogeneity_level}\")\n",
    "\n",
    "if ts_meta.drift_risk_level:\n",
    "    print(f\"\\n   âš ï¸ Drift risk: {ts_meta.drift_risk_level.upper()}\")\n",
    "    if ts_meta.volume_drift_risk:\n",
    "        print(f\"      Volume drift: {ts_meta.volume_drift_risk}\")\n",
    "    if ts_meta.population_stability is not None:\n",
    "        print(f\"      Population stability: {ts_meta.population_stability:.2f}\")\n",
    "\n",
    "# === 01b: Temporal Quality ===\n",
    "quality_meta = findings.metadata.get(\"temporal_quality\", {})\n",
    "if quality_meta:\n",
    "    print(f\"\\nðŸ“‹ FROM 01b (Temporal Quality):\")\n",
    "    if quality_meta.get(\"temporal_quality_score\"):\n",
    "        print(f\"   Quality score: {quality_meta.get('temporal_quality_score'):.1f}\")\n",
    "    if quality_meta.get(\"temporal_quality_grade\"):\n",
    "        print(f\"   Quality grade: {quality_meta.get('temporal_quality_grade')}\")\n",
    "    issues = quality_meta.get(\"issues\", {})\n",
    "    if issues.get(\"duplicate_events\", 0) > 0:\n",
    "        print(f\"   âš ï¸ Duplicate events: {issues['duplicate_events']:,}\")\n",
    "    if issues.get(\"temporal_gaps\", 0) > 0:\n",
    "        print(f\"   âš ï¸ Temporal gaps: {issues['temporal_gaps']:,}\")\n",
    "\n",
    "# === 01c: Temporal Patterns ===\n",
    "pattern_meta = findings.metadata.get(\"temporal_patterns\", {})\n",
    "SEASONALITY_RECOMMENDATIONS = []  # Store for later application\n",
    "TEMPORAL_PATTERN_RECOMMENDATIONS = []  # Store for later application\n",
    "TREND_RECOMMENDATIONS = []  # Store for later application\n",
    "COHORT_RECOMMENDATIONS = []  # Store for later application\n",
    "\n",
    "if pattern_meta:\n",
    "    print(f\"\\nðŸ“ˆ FROM 01c (Temporal Patterns):\")\n",
    "    windows_used = pattern_meta.get(\"windows_used\", {})\n",
    "    if windows_used:\n",
    "        if windows_used.get(\"aggregation_windows\"):\n",
    "            print(f\"   Windows analyzed: {windows_used.get('aggregation_windows')}\")\n",
    "        if windows_used.get(\"velocity_window\"):\n",
    "            print(f\"   Velocity window: {windows_used.get('velocity_window')} days\")\n",
    "        if windows_used.get(\"momentum_pairs\"):\n",
    "            print(f\"   Momentum pairs: {windows_used.get('momentum_pairs')}\")\n",
    "    \n",
    "    trend = pattern_meta.get(\"trend\", {})\n",
    "    if trend and trend.get(\"direction\"):\n",
    "        print(f\"\\n   Trend: {trend.get('direction')} (strength: {trend.get('strength', 0):.2f})\")\n",
    "        TREND_RECOMMENDATIONS = trend.get(\"recommendations\", [])\n",
    "        trend_features = [r for r in TREND_RECOMMENDATIONS if r.get(\"features\")]\n",
    "        if trend_features:\n",
    "            print(f\"\\n   ðŸ“ˆ Trend Features to Add:\")\n",
    "            for rec in trend_features:\n",
    "                print(f\"      â†’ {', '.join(rec['features'])} ({rec['priority']} priority)\")\n",
    "    \n",
    "    # Handle both old format (list) and new format (dict with patterns and recommendations)\n",
    "    seasonality = pattern_meta.get(\"seasonality\", {})\n",
    "    if isinstance(seasonality, list):\n",
    "        patterns = seasonality\n",
    "        SEASONALITY_RECOMMENDATIONS = []\n",
    "    else:\n",
    "        patterns = seasonality.get(\"patterns\", [])\n",
    "        SEASONALITY_RECOMMENDATIONS = seasonality.get(\"recommendations\", [])\n",
    "    \n",
    "    if patterns:\n",
    "        periods = [f\"{s.get('name', 'period')} ({s.get('period')}d)\" for s in patterns[:3]]\n",
    "        print(f\"   Seasonality: {', '.join(periods)}\")\n",
    "    \n",
    "    # Display seasonality recommendations\n",
    "    if SEASONALITY_RECOMMENDATIONS:\n",
    "        print(f\"\\n   ðŸ“‹ Seasonality Recommendations:\")\n",
    "        for rec in SEASONALITY_RECOMMENDATIONS:\n",
    "            action = rec.get(\"action\", \"\").replace(\"_\", \" \")\n",
    "            if action == \"add cyclical feature\":\n",
    "                print(f\"      â†’ Add {rec.get('feature')} with {rec.get('encoding')} encoding\")\n",
    "            elif action == \"window captures cycle\":\n",
    "                print(f\"      â†’ Windows {rec.get('windows')} align with detected cycles âœ“\")\n",
    "            elif action == \"window partial cycle\":\n",
    "                print(f\"      â†’ Warning: Windows don't align with cycles {rec.get('detected_periods')}\")\n",
    "            elif action == \"consider deseasonalization\":\n",
    "                print(f\"      â†’ Consider deseasonalizing for periods {rec.get('periods')}\")\n",
    "    \n",
    "    recency = pattern_meta.get(\"recency\", {})\n",
    "    if recency and recency.get(\"median_days\"):\n",
    "        print(f\"   Recency: median={recency.get('median_days'):.0f} days, \"\n",
    "              f\"target_corr={recency.get('target_correlation', 0):.2f}\")\n",
    "    \n",
    "    # Divergent columns (important for feature prioritization)\n",
    "    velocity = pattern_meta.get(\"velocity\", {})\n",
    "    divergent_velocity = [k for k, v in velocity.items() if isinstance(v, dict) and v.get(\"divergent\")]\n",
    "    if divergent_velocity:\n",
    "        print(f\"\\n   ðŸŽ¯ Divergent velocity columns: {divergent_velocity}\")\n",
    "    \n",
    "    momentum = pattern_meta.get(\"momentum\", {})\n",
    "    divergent_momentum = momentum.get(\"_divergent_columns\", [])\n",
    "    if divergent_momentum:\n",
    "        print(f\"   ðŸŽ¯ Divergent momentum columns: {divergent_momentum}\")\n",
    "\n",
    "    # Extract cohort recommendations\n",
    "    cohort_meta = pattern_meta.get(\"cohort\", {})\n",
    "    if cohort_meta:\n",
    "        COHORT_RECOMMENDATIONS = cohort_meta.get(\"recommendations\", [])\n",
    "        skip_cohort = any(r.get(\"action\") == \"skip_cohort_features\" for r in COHORT_RECOMMENDATIONS)\n",
    "        if skip_cohort:\n",
    "            skip_rec = next(r for r in COHORT_RECOMMENDATIONS if r.get(\"action\") == \"skip_cohort_features\")\n",
    "            print(f\"\\n   ðŸ‘¥ Cohort: Skip features - {skip_rec.get('reason', 'insufficient variation')}\")\n",
    "        else:\n",
    "            cohort_features = [r for r in COHORT_RECOMMENDATIONS if r.get(\"features\")]\n",
    "            if cohort_features:\n",
    "                print(f\"\\n   ðŸ‘¥ Cohort Features to Add:\")\n",
    "                for rec in cohort_features:\n",
    "                    print(f\"      â†’ {', '.join(rec['features'])} ({rec['priority']} priority)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Validate that prior notebooks have been run (01a required, 01c recommended)\n",
    "from customer_retention.stages.profiling import validate_temporal_findings\n",
    "\n",
    "validation = validate_temporal_findings(findings)\n",
    "if not validation.valid:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"â›” MISSING REQUIRED ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    for m in validation.missing_sections:\n",
    "        print(f\"   - {m}\")\n",
    "    raise ValueError(\"Cannot proceed - run prior notebooks first\")\n",
    "if validation.warnings:\n",
    "    print(\"\\nâš ï¸ VALIDATION WARNINGS:\")\n",
    "    for w in validation.warnings:\n",
    "        print(f\"   - {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:27.089704Z",
     "iopub.status.busy": "2026-01-29T23:48:27.089598Z",
     "iopub.status.idle": "2026-01-29T23:48:27.202263Z",
     "shell.execute_reply": "2026-01-29T23:48:27.201662Z"
    },
    "papermill": {
     "duration": 0.115895,
     "end_time": "2026-01-29T23:48:27.202895",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.087000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74,842 events x 16 columns\n",
      "Data source: snapshot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2015-01-01 00:00:00 to 2022-09-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "\n",
    "# Load source data (prefers snapshots over raw files)\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=str(FINDINGS_DIR))\n",
    "df[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN])\n",
    "charts = ChartBuilder()\n",
    "\n",
    "print(f\"Loaded {len(df):,} events x {len(df.columns)} columns\")\n",
    "print(f\"Data source: {data_source}\")\n",
    "print(f\"Date range: {df[TIME_COLUMN].min()} to {df[TIME_COLUMN].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swzzbsxq5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:27.207928Z",
     "iopub.status.busy": "2026-01-29T23:48:27.207819Z",
     "iopub.status.idle": "2026-01-29T23:48:27.219489Z",
     "shell.execute_reply": "2026-01-29T23:48:27.219120Z"
    },
    "papermill": {
     "duration": 0.01498,
     "end_time": "2026-01-29T23:48:27.220187",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.205207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication: removed 371 duplicate events (01b flagged 371)\n",
      "Events after dedup: 74,471\n"
     ]
    }
   ],
   "source": [
    "# Apply quality deduplication from 01b findings\n",
    "dup_count = get_duplicate_event_count(findings)\n",
    "if dup_count > 0:\n",
    "    df, removed = deduplicate_events(df, ENTITY_COLUMN, TIME_COLUMN, duplicate_count=dup_count)\n",
    "    print(f\"Deduplication: removed {removed:,} duplicate events (01b flagged {dup_count:,})\")\n",
    "    print(f\"Events after dedup: {len(df):,}\")\n",
    "else:\n",
    "    print(\"No duplicate events flagged by 01b - skipping deduplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {
    "papermill": {
     "duration": 0.002184,
     "end_time": "2026-01-29T23:48:27.224623",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.222439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.2 Configure Aggregation Based on Findings\n",
    "\n",
    "Apply all insights from 01a-01c to configure optimal aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:27.229125Z",
     "iopub.status.busy": "2026-01-29T23:48:27.229008Z",
     "iopub.status.idle": "2026-01-29T23:48:27.235852Z",
     "shell.execute_reply": "2026-01-29T23:48:27.235403Z"
    },
    "papermill": {
     "duration": 0.009866,
     "end_time": "2026-01-29T23:48:27.236400",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.226534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AGGREGATION CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "Windows: ['180d', '365d', 'all_time']\n",
      "   Source: 01a recommendations\n",
      "\n",
      "Reference date: 2022-09-26 00:00:00\n",
      "\n",
      "Value columns (5 total):\n",
      "   Other: ['opened', 'clicked', 'send_hour', 'bounced', 'time_to_open_hours']\n",
      "\n",
      "   Excluded from aggregation: target (target - prevents leakage)\n",
      "\n",
      "Aggregation functions: ['sum', 'mean', 'max', 'count']\n",
      "\n",
      "Additional features:\n",
      "   Include lifecycle_quadrant: True\n",
      "   Include recency: True\n",
      "   Include tenure: True\n",
      "\n",
      "   Duplicate events to remove: 371\n",
      "   Momentum ratio features: 2 recommendation(s)\n",
      "\n",
      "RECOMMENDATION APPLICATION SUMMARY\n",
      "==================================================\n",
      "Section              Features\n",
      "------------------------------\n",
      "trend                       0\n",
      "seasonality                 0\n",
      "recency                     2\n",
      "cohort                      0\n",
      "velocity                    0\n",
      "momentum                    2\n",
      "lag                         0\n",
      "sparkline                   7\n",
      "effect_size                17\n",
      "predictive_power           17\n",
      "text_pca                    0\n",
      "------------------------------\n",
      "Total                      45\n",
      "\n",
      "Feature flags: {'include_recency': True, 'include_tenure': True, 'include_lifecycle_quadrant': True, 'include_trend_features': True, 'include_seasonality_features': True, 'include_cohort_features': False}\n",
      "Scaling recs: 1\n"
     ]
    }
   ],
   "source": [
    "# === AGGREGATION CONFIGURATION ===\n",
    "# Windows are loaded from findings (01a recommendations) with option to override\n",
    "\n",
    "# Manual override (set to None to use findings recommendations)\n",
    "WINDOW_OVERRIDE = None  # e.g., [\"7d\", \"30d\", \"90d\"] to override\n",
    "\n",
    "# Get windows from findings or use defaults\n",
    "if WINDOW_OVERRIDE:\n",
    "    WINDOWS = WINDOW_OVERRIDE\n",
    "    window_source = \"manual override\"\n",
    "elif ts_meta.suggested_aggregations:\n",
    "    WINDOWS = ts_meta.suggested_aggregations\n",
    "    window_source = \"01a recommendations\"\n",
    "else:\n",
    "    WINDOWS = [\"7d\", \"30d\", \"90d\", \"180d\", \"365d\", \"all_time\"]\n",
    "    window_source = \"defaults (no findings)\"\n",
    "\n",
    "# Reference date for window calculations\n",
    "REFERENCE_DATE = df[TIME_COLUMN].max()\n",
    "\n",
    "# Load all recommendations via AggregationFeatureConfig\n",
    "agg_feature_config = AggregationFeatureConfig.from_findings(findings)\n",
    "\n",
    "# Extract pattern metadata for feature prioritization\n",
    "pattern_meta = findings.metadata.get(\"temporal_patterns\", {})\n",
    "velocity_meta = pattern_meta.get(\"velocity\", {})\n",
    "momentum_meta = pattern_meta.get(\"momentum\", {})\n",
    "\n",
    "# Identify divergent columns (these are most predictive for target)\n",
    "DIVERGENT_VELOCITY_COLS = [k for k, v in velocity_meta.items() \n",
    "                           if isinstance(v, dict) and v.get(\"divergent\")]\n",
    "DIVERGENT_MOMENTUM_COLS = momentum_meta.get(\"_divergent_columns\", [])\n",
    "\n",
    "# Value columns: prioritize divergent columns, then other numerics\n",
    "# IMPORTANT: Exclude target column and temporal metadata to prevent data leakage!\n",
    "TARGET_COLUMN = findings.target_column\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = {ENTITY_COLUMN, TIME_COLUMN} | set(TEMPORAL_METADATA_COLS)\n",
    "if TARGET_COLUMN:\n",
    "    exclude_cols.add(TARGET_COLUMN)\n",
    "available_numeric = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "# Put divergent columns first (they showed predictive signal in 01c)\n",
    "priority_cols = [c for c in DIVERGENT_VELOCITY_COLS + DIVERGENT_MOMENTUM_COLS \n",
    "                 if c in available_numeric]\n",
    "other_cols = [c for c in available_numeric if c not in priority_cols]\n",
    "\n",
    "# Include text PCA columns from findings if text processing was performed\n",
    "text_pca_cols = [c for c in agg_feature_config.text_pca_columns if c in df.columns]\n",
    "VALUE_COLUMNS = priority_cols + other_cols + text_pca_cols\n",
    "\n",
    "# Aggregation functions\n",
    "AGG_FUNCTIONS = [\"sum\", \"mean\", \"max\", \"count\"]\n",
    "\n",
    "# Lifecycle features - read from 01c feature_flags, fallback to 01a/defaults\n",
    "feature_flags = pattern_meta.get(\"feature_flags\", {})\n",
    "INCLUDE_LIFECYCLE_QUADRANT = feature_flags.get(\n",
    "    \"include_lifecycle_quadrant\",\n",
    "    ts_meta.temporal_segmentation_recommendation is not None\n",
    ")\n",
    "INCLUDE_RECENCY = feature_flags.get(\"include_recency\", True)\n",
    "INCLUDE_TENURE = feature_flags.get(\"include_tenure\", True)\n",
    "\n",
    "# Quality: check for duplicate events from 01b\n",
    "DUPLICATE_EVENT_COUNT = get_duplicate_event_count(findings)\n",
    "\n",
    "# Momentum recommendations for ratio features\n",
    "MOMENTUM_RECOMMENDATIONS = pattern_meta.get(\"momentum\", {}).get(\"recommendations\", [])\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"AGGREGATION CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nWindows: {WINDOWS}\")\n",
    "print(f\"   Source: {window_source}\")\n",
    "print(f\"\\nReference date: {REFERENCE_DATE}\")\n",
    "print(f\"\\nValue columns ({len(VALUE_COLUMNS)} total):\")\n",
    "if priority_cols:\n",
    "    print(f\"   Priority (divergent): {priority_cols}\")\n",
    "print(f\"   Other: {other_cols[:5]}{'...' if len(other_cols) > 5 else ''}\")\n",
    "if text_pca_cols:\n",
    "    print(f\"   Text PCA: {text_pca_cols}\")\n",
    "if TARGET_COLUMN:\n",
    "    print(f\"\\n   Excluded from aggregation: {TARGET_COLUMN} (target - prevents leakage)\")\n",
    "print(f\"\\nAggregation functions: {AGG_FUNCTIONS}\")\n",
    "print(f\"\\nAdditional features:\")\n",
    "print(f\"   Include lifecycle_quadrant: {INCLUDE_LIFECYCLE_QUADRANT}\")\n",
    "print(f\"   Include recency: {INCLUDE_RECENCY}\")\n",
    "print(f\"   Include tenure: {INCLUDE_TENURE}\")\n",
    "if DUPLICATE_EVENT_COUNT > 0:\n",
    "    print(f\"\\n   Duplicate events to remove: {DUPLICATE_EVENT_COUNT:,}\")\n",
    "if MOMENTUM_RECOMMENDATIONS:\n",
    "    print(f\"   Momentum ratio features: {len(MOMENTUM_RECOMMENDATIONS)} recommendation(s)\")\n",
    "\n",
    "# Print recommendation summary from 01c\n",
    "print(\"\\n\" + agg_feature_config.format_recommendation_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "papermill": {
     "duration": 0.002418,
     "end_time": "2026-01-29T23:48:27.241013",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.238595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.3 Preview Aggregation Plan\n",
    "\n",
    "See what features will be created before executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:27.245899Z",
     "iopub.status.busy": "2026-01-29T23:48:27.245531Z",
     "iopub.status.idle": "2026-01-29T23:48:27.249757Z",
     "shell.execute_reply": "2026-01-29T23:48:27.249184Z"
    },
    "papermill": {
     "duration": 0.007225,
     "end_time": "2026-01-29T23:48:27.250191",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.242966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AGGREGATION PLAN\n",
      "============================================================\n",
      "\n",
      "Entity column: customer_id\n",
      "Time column: feature_timestamp\n",
      "Windows: ['180d', '365d', 'all_time']\n",
      "\n",
      "Features from aggregation (65):\n",
      "   - event_count_180d\n",
      "   - event_count_365d\n",
      "   - event_count_all_time\n",
      "   - opened_sum_180d\n",
      "   - opened_mean_180d\n",
      "   - opened_max_180d\n",
      "   - opened_count_180d\n",
      "   - clicked_sum_180d\n",
      "   - clicked_mean_180d\n",
      "   - clicked_max_180d\n",
      "   - clicked_count_180d\n",
      "   - send_hour_sum_180d\n",
      "   - send_hour_mean_180d\n",
      "   - send_hour_max_180d\n",
      "   - send_hour_count_180d\n",
      "   ... and 50 more\n",
      "\n",
      "Additional features:\n",
      "   - lifecycle_quadrant\n",
      "   - target (entity target)\n",
      "\n",
      "Total expected features: 68\n"
     ]
    }
   ],
   "source": [
    "# Initialize aggregator\n",
    "aggregator = TimeWindowAggregator(\n",
    "    entity_column=ENTITY_COLUMN,\n",
    "    time_column=TIME_COLUMN\n",
    ")\n",
    "\n",
    "# Generate plan\n",
    "plan = aggregator.generate_plan(\n",
    "    df=df,\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "# Count additional features we'll add\n",
    "additional_features = []\n",
    "if INCLUDE_LIFECYCLE_QUADRANT:\n",
    "    additional_features.append(\"lifecycle_quadrant\")\n",
    "if findings.target_column and findings.target_column in df.columns:\n",
    "    additional_features.append(f\"{findings.target_column} (entity target)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEntity column: {plan.entity_column}\")\n",
    "print(f\"Time column: {plan.time_column}\")\n",
    "print(f\"Windows: {[w.name for w in plan.windows]}\")\n",
    "\n",
    "print(f\"\\nFeatures from aggregation ({len(plan.feature_columns)}):\")\n",
    "for feat in plan.feature_columns[:15]:\n",
    "    # Highlight divergent column features\n",
    "    is_priority = any(dc in feat for dc in priority_cols) if priority_cols else False\n",
    "    marker = \" ðŸŽ¯\" if is_priority else \"\"\n",
    "    print(f\"   - {feat}{marker}\")\n",
    "if len(plan.feature_columns) > 15:\n",
    "    print(f\"   ... and {len(plan.feature_columns) - 15} more\")\n",
    "\n",
    "if additional_features:\n",
    "    print(f\"\\nAdditional features:\")\n",
    "    for feat in additional_features:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "print(f\"\\nTotal expected features: {len(plan.feature_columns) + len(additional_features) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "papermill": {
     "duration": 0.001979,
     "end_time": "2026-01-29T23:48:27.254327",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.252348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.4 Execute Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:27.259296Z",
     "iopub.status.busy": "2026-01-29T23:48:27.259192Z",
     "iopub.status.idle": "2026-01-29T23:48:29.084445Z",
     "shell.execute_reply": "2026-01-29T23:48:29.084033Z"
    },
    "papermill": {
     "duration": 1.828627,
     "end_time": "2026-01-29T23:48:29.085054",
     "exception": false,
     "start_time": "2026-01-29T23:48:27.256427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing aggregation...\n",
      "   Input: 74,471 events\n",
      "   Expected output: 4,998 entities\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Adding lifecycle_quadrant feature...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quadrant distribution:\n",
      "      Occasional & Loyal: 1,632 (32.7%)\n",
      "      Intense & Brief: 1,627 (32.6%)\n",
      "      Steady & Loyal: 872 (17.4%)\n",
      "      One-shot: 867 (17.3%)\n",
      "\n",
      "   Adding entity-level target (target)...\n",
      "      target=0: 3,034 (60.7%)\n",
      "      target=1: 1,964 (39.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Adding cyclical features from seasonality analysis:\n",
      "      -> day_of_week (dow_sin, dow_cos)\n",
      "\n",
      "   Skipping cohort features (insufficient variation)\n",
      "\n",
      "   Adding momentum ratio features:\n",
      "      -> clicked_momentum_180_365\n",
      "\n",
      "   Adding recency_bucket feature:\n",
      "      0-7d: 134 (2.7%)\n",
      "      31-90d: 818 (16.4%)\n",
      "      8-30d: 358 (7.2%)\n",
      "      91-180d: 804 (16.1%)\n",
      "      >180d: 2,884 (57.7%)\n",
      "\n",
      "   Aggregation complete!\n",
      "   Output: 4,998 entities x 72 features\n",
      "   Memory: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Executing aggregation...\")\n",
    "print(f\"   Input: {len(df):,} events\")\n",
    "print(f\"   Expected output: {df[ENTITY_COLUMN].nunique():,} entities\")\n",
    "\n",
    "# Step 1: Basic time window aggregation\n",
    "df_aggregated = aggregator.aggregate(\n",
    "    df,\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    reference_date=REFERENCE_DATE,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "# Step 2: Add lifecycle quadrant (from 01a recommendation)\n",
    "if INCLUDE_LIFECYCLE_QUADRANT:\n",
    "    print(\"\\n   Adding lifecycle_quadrant feature...\")\n",
    "    profiler = TimeSeriesProfiler(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN)\n",
    "    ts_profile = profiler.profile(df)\n",
    "    \n",
    "    # Rename 'entity' column to match our entity column name\n",
    "    lifecycles = ts_profile.entity_lifecycles.copy()\n",
    "    lifecycles = lifecycles.rename(columns={\"entity\": ENTITY_COLUMN})\n",
    "    \n",
    "    quadrant_result = classify_lifecycle_quadrants(lifecycles)\n",
    "    \n",
    "    # Merge lifecycle_quadrant into aggregated data\n",
    "    quadrant_map = quadrant_result.lifecycles.set_index(ENTITY_COLUMN)[\"lifecycle_quadrant\"]\n",
    "    df_aggregated[\"lifecycle_quadrant\"] = df_aggregated[ENTITY_COLUMN].map(quadrant_map)\n",
    "    \n",
    "    print(f\"   Quadrant distribution:\")\n",
    "    for quad, count in df_aggregated[\"lifecycle_quadrant\"].value_counts().items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"      {quad}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Step 3: Add entity-level target (if available)\n",
    "TARGET_COLUMN = findings.target_column\n",
    "if TARGET_COLUMN and TARGET_COLUMN in df.columns:\n",
    "    print(f\"\\n   Adding entity-level target ({TARGET_COLUMN})...\")\n",
    "    # For entity-level target, use max (if any event has target=1, entity has target=1)\n",
    "    entity_target = df.groupby(ENTITY_COLUMN)[TARGET_COLUMN].max()\n",
    "    df_aggregated[TARGET_COLUMN] = df_aggregated[ENTITY_COLUMN].map(entity_target)\n",
    "    \n",
    "    target_dist = df_aggregated[TARGET_COLUMN].value_counts()\n",
    "    for val, count in target_dist.items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"      {TARGET_COLUMN}={val}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Step 4: Add cyclical features based on seasonality recommendations\n",
    "if SEASONALITY_RECOMMENDATIONS:\n",
    "    cyclical_added = []\n",
    "    for rec in SEASONALITY_RECOMMENDATIONS:\n",
    "        if rec.get(\"action\") == \"add_cyclical_feature\":\n",
    "            feature = rec.get(\"feature\")\n",
    "            if feature == \"day_of_week\":\n",
    "                entity_dow = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(\n",
    "                    lambda x: x.dt.dayofweek.mean()\n",
    "                )\n",
    "                df_aggregated[\"dow_sin\"] = np.sin(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_dow) / 7)\n",
    "                df_aggregated[\"dow_cos\"] = np.cos(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_dow) / 7)\n",
    "                cyclical_added.append(\"day_of_week (dow_sin, dow_cos)\")\n",
    "            elif feature == \"day_of_month\":\n",
    "                entity_dom = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(\n",
    "                    lambda x: x.dt.day.mean()\n",
    "                )\n",
    "                df_aggregated[\"dom_sin\"] = np.sin(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_dom) / 31)\n",
    "                df_aggregated[\"dom_cos\"] = np.cos(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_dom) / 31)\n",
    "                cyclical_added.append(\"day_of_month (dom_sin, dom_cos)\")\n",
    "            elif feature == \"quarter\":\n",
    "                entity_quarter = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(\n",
    "                    lambda x: x.dt.quarter.mean()\n",
    "                )\n",
    "                df_aggregated[\"quarter_sin\"] = np.sin(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_quarter) / 4)\n",
    "                df_aggregated[\"quarter_cos\"] = np.cos(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_quarter) / 4)\n",
    "                cyclical_added.append(\"quarter (quarter_sin, quarter_cos)\")\n",
    "    \n",
    "    if cyclical_added:\n",
    "        print(f\"\\n   Adding cyclical features from seasonality analysis:\")\n",
    "        for feat in cyclical_added:\n",
    "            print(f\"      -> {feat}\")\n",
    "\n",
    "# Step 5: Add cyclical features based on temporal pattern analysis (from grid)\n",
    "if TEMPORAL_PATTERN_RECOMMENDATIONS:\n",
    "    tp_added = []\n",
    "    for rec in TEMPORAL_PATTERN_RECOMMENDATIONS:\n",
    "        features = rec.get(\"features\", [])\n",
    "        pattern = rec.get(\"pattern\", \"\")\n",
    "        \n",
    "        if pattern == \"day_of_week\" and \"dow_sin\" in df_aggregated.columns:\n",
    "            continue\n",
    "        if pattern == \"month\" and \"month_sin\" in df_aggregated.columns:\n",
    "            continue\n",
    "        if pattern == \"quarter\" and \"quarter_sin\" in df_aggregated.columns:\n",
    "            continue\n",
    "            \n",
    "        if \"dow_sin\" in features or \"dow_cos\" in features:\n",
    "            if \"dow_sin\" not in df_aggregated.columns:\n",
    "                entity_dow = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(lambda x: x.dt.dayofweek.mean())\n",
    "                df_aggregated[\"dow_sin\"] = np.sin(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_dow) / 7)\n",
    "                df_aggregated[\"dow_cos\"] = np.cos(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_dow) / 7)\n",
    "                tp_added.append(\"day_of_week (dow_sin, dow_cos)\")\n",
    "        \n",
    "        if \"is_weekend\" in features:\n",
    "            if \"is_weekend\" not in df_aggregated.columns:\n",
    "                entity_weekend_pct = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(\n",
    "                    lambda x: (x.dt.dayofweek >= 5).mean()\n",
    "                )\n",
    "                df_aggregated[\"is_weekend_pct\"] = df_aggregated[ENTITY_COLUMN].map(entity_weekend_pct)\n",
    "                tp_added.append(\"is_weekend_pct\")\n",
    "        \n",
    "        if \"month_sin\" in features or \"month_cos\" in features:\n",
    "            if \"month_sin\" not in df_aggregated.columns:\n",
    "                entity_month = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(lambda x: x.dt.month.mean())\n",
    "                df_aggregated[\"month_sin\"] = np.sin(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_month) / 12)\n",
    "                df_aggregated[\"month_cos\"] = np.cos(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_month) / 12)\n",
    "                tp_added.append(\"month (month_sin, month_cos)\")\n",
    "        \n",
    "        if \"quarter_sin\" in features or \"quarter_cos\" in features:\n",
    "            if \"quarter_sin\" not in df_aggregated.columns:\n",
    "                entity_quarter = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(lambda x: x.dt.quarter.mean())\n",
    "                df_aggregated[\"quarter_sin\"] = np.sin(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_quarter) / 4)\n",
    "                df_aggregated[\"quarter_cos\"] = np.cos(2 * np.pi * df_aggregated[ENTITY_COLUMN].map(entity_quarter) / 4)\n",
    "                tp_added.append(\"quarter (quarter_sin, quarter_cos)\")\n",
    "        \n",
    "        if \"year_trend\" in features:\n",
    "            if \"year_trend\" not in df_aggregated.columns:\n",
    "                entity_year = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(lambda x: x.dt.year.mean())\n",
    "                min_year = entity_year.min()\n",
    "                df_aggregated[\"year_trend\"] = df_aggregated[ENTITY_COLUMN].map(entity_year) - min_year\n",
    "                tp_added.append(f\"year_trend (normalized from {min_year:.0f})\")\n",
    "        \n",
    "        if \"year_categorical\" in features:\n",
    "            if \"year_mode\" not in df_aggregated.columns:\n",
    "                entity_year_mode = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].apply(\n",
    "                    lambda x: x.dt.year.mode().iloc[0] if len(x.dt.year.mode()) > 0 else x.dt.year.median()\n",
    "                )\n",
    "                df_aggregated[\"year_mode\"] = df_aggregated[ENTITY_COLUMN].map(entity_year_mode).astype(int)\n",
    "                tp_added.append(\"year_mode (categorical - encode before modeling)\")\n",
    "    \n",
    "    if tp_added:\n",
    "        print(f\"\\n   Adding features from temporal pattern analysis:\")\n",
    "        for feat in tp_added:\n",
    "            print(f\"      -> {feat}\")\n",
    "\n",
    "# Step 6: Add trend features based on trend recommendations\n",
    "if TREND_RECOMMENDATIONS:\n",
    "    trend_added = []\n",
    "    for rec in TREND_RECOMMENDATIONS:\n",
    "        features = rec.get(\"features\", [])\n",
    "        \n",
    "        if \"recent_vs_overall_ratio\" in features:\n",
    "            if \"recent_vs_overall_ratio\" not in df_aggregated.columns:\n",
    "                time_span = (df[TIME_COLUMN].max() - df[TIME_COLUMN].min()).days\n",
    "                recent_cutoff = df[TIME_COLUMN].max() - pd.Timedelta(days=int(time_span * 0.3))\n",
    "                \n",
    "                overall_counts = df.groupby(ENTITY_COLUMN).size()\n",
    "                recent_counts = df[df[TIME_COLUMN] >= recent_cutoff].groupby(ENTITY_COLUMN).size()\n",
    "                \n",
    "                ratio = recent_counts / overall_counts\n",
    "                ratio = ratio.fillna(0)\n",
    "                df_aggregated[\"recent_vs_overall_ratio\"] = df_aggregated[ENTITY_COLUMN].map(ratio).fillna(0)\n",
    "                trend_added.append(\"recent_vs_overall_ratio\")\n",
    "        \n",
    "        if \"entity_trend_slope\" in features:\n",
    "            if \"entity_trend_slope\" not in df_aggregated.columns:\n",
    "                def compute_entity_slope(group):\n",
    "                    if len(group) < 3:\n",
    "                        return 0.0\n",
    "                    x = (group[TIME_COLUMN] - group[TIME_COLUMN].min()).dt.days.values\n",
    "                    y = np.arange(len(group))\n",
    "                    if x.std() == 0:\n",
    "                        return 0.0\n",
    "                    slope = np.polyfit(x, y, 1)[0]\n",
    "                    return slope\n",
    "                \n",
    "                entity_slopes = df.groupby(ENTITY_COLUMN).apply(compute_entity_slope)\n",
    "                df_aggregated[\"entity_trend_slope\"] = df_aggregated[ENTITY_COLUMN].map(entity_slopes).fillna(0)\n",
    "                trend_added.append(\"entity_trend_slope\")\n",
    "    \n",
    "    if trend_added:\n",
    "        print(f\"\\n   Adding features from trend analysis:\")\n",
    "        for feat in trend_added:\n",
    "            print(f\"      -> {feat}\")\n",
    "\n",
    "# Step 7: Add cohort features based on cohort recommendations\n",
    "if COHORT_RECOMMENDATIONS:\n",
    "    skip_cohort = any(r.get(\"action\") == \"skip_cohort_features\" for r in COHORT_RECOMMENDATIONS)\n",
    "    if not skip_cohort:\n",
    "        cohort_added = []\n",
    "        cohort_features = [f for r in COHORT_RECOMMENDATIONS for f in r.get(\"features\", [])]\n",
    "        \n",
    "        if \"cohort_year\" in cohort_features or \"cohort_quarter\" in cohort_features:\n",
    "            entity_first = df.groupby(ENTITY_COLUMN)[TIME_COLUMN].min()\n",
    "            \n",
    "            if \"cohort_year\" in cohort_features and \"cohort_year\" not in df_aggregated.columns:\n",
    "                df_aggregated[\"cohort_year\"] = df_aggregated[ENTITY_COLUMN].map(entity_first).dt.year\n",
    "                cohort_added.append(\"cohort_year\")\n",
    "            \n",
    "            if \"cohort_quarter\" in cohort_features and \"cohort_quarter\" not in df_aggregated.columns:\n",
    "                first_dates = df_aggregated[ENTITY_COLUMN].map(entity_first)\n",
    "                df_aggregated[\"cohort_quarter\"] = first_dates.dt.year.astype(str) + \"Q\" + first_dates.dt.quarter.astype(str)\n",
    "                cohort_added.append(\"cohort_quarter\")\n",
    "        \n",
    "        if cohort_added:\n",
    "            print(f\"\\n   Adding cohort features:\")\n",
    "            for feat in cohort_added:\n",
    "                print(f\"      -> {feat}\")\n",
    "    else:\n",
    "        print(f\"\\n   Skipping cohort features (insufficient variation)\")\n",
    "\n",
    "# Step 8: Add momentum ratio features from 01c momentum recommendations\n",
    "if MOMENTUM_RECOMMENDATIONS:\n",
    "    before_cols = set(df_aggregated.columns)\n",
    "    df_aggregated = create_momentum_ratio_features(df_aggregated, MOMENTUM_RECOMMENDATIONS)\n",
    "    new_momentum_cols = set(df_aggregated.columns) - before_cols\n",
    "    if new_momentum_cols:\n",
    "        print(f\"\\n   Adding momentum ratio features:\")\n",
    "        for feat in sorted(new_momentum_cols):\n",
    "            print(f\"      -> {feat}\")\n",
    "    else:\n",
    "        print(f\"\\n   Momentum ratio features: columns not available in aggregated data (skipped)\")\n",
    "\n",
    "# Step 9: Add recency bucket feature\n",
    "if INCLUDE_RECENCY and \"days_since_last_event\" in df_aggregated.columns:\n",
    "    df_aggregated = create_recency_bucket_feature(df_aggregated)\n",
    "    if \"recency_bucket\" in df_aggregated.columns:\n",
    "        print(f\"\\n   Adding recency_bucket feature:\")\n",
    "        for bucket, count in df_aggregated[\"recency_bucket\"].value_counts().sort_index().items():\n",
    "            pct = count / len(df_aggregated) * 100\n",
    "            print(f\"      {bucket}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Aggregation complete!\")\n",
    "print(f\"   Output: {len(df_aggregated):,} entities x {len(df_aggregated.columns)} features\")\n",
    "print(f\"   Memory: {df_aggregated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.090322Z",
     "iopub.status.busy": "2026-01-29T23:48:29.090240Z",
     "iopub.status.idle": "2026-01-29T23:48:29.100661Z",
     "shell.execute_reply": "2026-01-29T23:48:29.100198Z"
    },
    "papermill": {
     "duration": 0.013503,
     "end_time": "2026-01-29T23:48:29.101085",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.087582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>event_count_180d</th>\n",
       "      <th>event_count_365d</th>\n",
       "      <th>event_count_all_time</th>\n",
       "      <th>opened_sum_180d</th>\n",
       "      <th>opened_mean_180d</th>\n",
       "      <th>opened_max_180d</th>\n",
       "      <th>opened_count_180d</th>\n",
       "      <th>clicked_sum_180d</th>\n",
       "      <th>clicked_mean_180d</th>\n",
       "      <th>...</th>\n",
       "      <th>time_to_open_hours_max_all_time</th>\n",
       "      <th>time_to_open_hours_count_all_time</th>\n",
       "      <th>days_since_last_event</th>\n",
       "      <th>days_since_first_event</th>\n",
       "      <th>lifecycle_quadrant</th>\n",
       "      <th>target</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>clicked_momentum_180_365</th>\n",
       "      <th>recency_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6A2E47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7</td>\n",
       "      <td>1836</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "      <td>5.350932e-01</td>\n",
       "      <td>-8.447930e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58D29E</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.1</td>\n",
       "      <td>6</td>\n",
       "      <td>477</td>\n",
       "      <td>2825</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>0</td>\n",
       "      <td>8.963931e-02</td>\n",
       "      <td>-9.959743e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3DA827</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>235</td>\n",
       "      <td>2825</td>\n",
       "      <td>Steady &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "      <td>4.338837e-01</td>\n",
       "      <td>-9.009689e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6897C2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2824</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>0</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCAF7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>2825</td>\n",
       "      <td>Steady &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "      <td>9.956785e-02</td>\n",
       "      <td>-9.950308e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7F0800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1557</td>\n",
       "      <td>2825</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>1</td>\n",
       "      <td>8.865993e-01</td>\n",
       "      <td>-4.625383e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22507F</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.1</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>2825</td>\n",
       "      <td>Steady &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "      <td>7.232373e-02</td>\n",
       "      <td>-9.973812e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31-90d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CFBB70</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3</td>\n",
       "      <td>241</td>\n",
       "      <td>2825</td>\n",
       "      <td>Occasional &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "      <td>2.536546e-01</td>\n",
       "      <td>-9.672949e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>307116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2597</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>168A39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2434</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "      <td>1.490423e-01</td>\n",
       "      <td>-9.888308e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>&gt;180d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  event_count_180d  event_count_365d  event_count_all_time  \\\n",
       "0      6A2E47                 0                 0                    31   \n",
       "1      58D29E                 0                 0                    15   \n",
       "2      3DA827                 0                 2                    21   \n",
       "3      6897C2                 0                 0                     2   \n",
       "4      ACCAF7                 0                 2                    18   \n",
       "5      7F0800                 0                 0                     7   \n",
       "6      22507F                 4                 5                    31   \n",
       "7      CFBB70                 0                 2                    14   \n",
       "8      307116                 0                 0                     4   \n",
       "9      168A39                 0                 0                     9   \n",
       "\n",
       "   opened_sum_180d  opened_mean_180d  opened_max_180d  opened_count_180d  \\\n",
       "0                0               NaN              NaN                  0   \n",
       "1                0               NaN              NaN                  0   \n",
       "2                0               NaN              NaN                  0   \n",
       "3                0               NaN              NaN                  0   \n",
       "4                0               NaN              NaN                  0   \n",
       "5                0               NaN              NaN                  0   \n",
       "6                0               0.0              0.0                  4   \n",
       "7                0               NaN              NaN                  0   \n",
       "8                0               NaN              NaN                  0   \n",
       "9                0               NaN              NaN                  0   \n",
       "\n",
       "   clicked_sum_180d  clicked_mean_180d  ...  time_to_open_hours_max_all_time  \\\n",
       "0                 0                NaN  ...                              6.5   \n",
       "1                 0                NaN  ...                              9.1   \n",
       "2                 0                NaN  ...                             10.0   \n",
       "3                 0                NaN  ...                              1.4   \n",
       "4                 0                NaN  ...                              3.8   \n",
       "5                 0                NaN  ...                              NaN   \n",
       "6                 0                0.0  ...                             15.1   \n",
       "7                 0                NaN  ...                             11.0   \n",
       "8                 0                NaN  ...                              NaN   \n",
       "9                 0                NaN  ...                              2.1   \n",
       "\n",
       "   time_to_open_hours_count_all_time  days_since_last_event  \\\n",
       "0                                  7                   1836   \n",
       "1                                  6                    477   \n",
       "2                                  5                    235   \n",
       "3                                  1                   2824   \n",
       "4                                  3                    289   \n",
       "5                                  0                   1557   \n",
       "6                                  5                     42   \n",
       "7                                  3                    241   \n",
       "8                                  0                   2597   \n",
       "9                                  2                   2434   \n",
       "\n",
       "   days_since_first_event  lifecycle_quadrant  target       dow_sin  \\\n",
       "0                    2825     Intense & Brief       1  5.350932e-01   \n",
       "1                    2825            One-shot       0  8.963931e-02   \n",
       "2                    2825      Steady & Loyal       0  4.338837e-01   \n",
       "3                    2825     Intense & Brief       0  1.224647e-16   \n",
       "4                    2825      Steady & Loyal       0  9.956785e-02   \n",
       "5                    2825            One-shot       1  8.865993e-01   \n",
       "6                    2825      Steady & Loyal       0  7.232373e-02   \n",
       "7                    2825  Occasional & Loyal       0  2.536546e-01   \n",
       "8                    2825     Intense & Brief       1  1.000000e+00   \n",
       "9                    2825     Intense & Brief       1  1.490423e-01   \n",
       "\n",
       "        dow_cos  clicked_momentum_180_365  recency_bucket  \n",
       "0 -8.447930e-01                       1.0           >180d  \n",
       "1 -9.959743e-01                       1.0           >180d  \n",
       "2 -9.009689e-01                       1.0           >180d  \n",
       "3 -1.000000e+00                       1.0           >180d  \n",
       "4 -9.950308e-01                       1.0           >180d  \n",
       "5 -4.625383e-01                       1.0           >180d  \n",
       "6 -9.973812e-01                       1.0          31-90d  \n",
       "7 -9.672949e-01                       1.0           >180d  \n",
       "8  6.123234e-17                       1.0           >180d  \n",
       "9 -9.888308e-01                       1.0           >180d  \n",
       "\n",
       "[10 rows x 72 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview aggregated data\n",
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(df_aggregated.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.107004Z",
     "iopub.status.busy": "2026-01-29T23:48:29.106895Z",
     "iopub.status.idle": "2026-01-29T23:48:29.144805Z",
     "shell.execute_reply": "2026-01-29T23:48:29.144237Z"
    },
    "papermill": {
     "duration": 0.041762,
     "end_time": "2026-01-29T23:48:29.145433",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.103671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>event_count_180d</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.704882</td>\n",
       "      <td>1.036236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_count_365d</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>1.475590</td>\n",
       "      <td>1.722290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_count_all_time</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>14.900160</td>\n",
       "      <td>8.175178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opened_sum_180d</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.157063</td>\n",
       "      <td>0.426204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opened_mean_180d</th>\n",
       "      <td>2114.0</td>\n",
       "      <td>0.217306</td>\n",
       "      <td>0.353255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_since_first_event</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>2669.424570</td>\n",
       "      <td>158.136559</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>2603.000000</td>\n",
       "      <td>2719.000000</td>\n",
       "      <td>2784.000000</td>\n",
       "      <td>2825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.392957</td>\n",
       "      <td>0.488456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow_sin</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.378318</td>\n",
       "      <td>0.420471</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.127877</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow_cos</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>-0.772429</td>\n",
       "      <td>0.288939</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.967295</td>\n",
       "      <td>-0.875223</td>\n",
       "      <td>-0.683392</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clicked_momentum_180_365</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.993549</td>\n",
       "      <td>0.242794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count         mean         std     min  \\\n",
       "event_count_180d          4998.0     0.704882    1.036236     0.0   \n",
       "event_count_365d          4998.0     1.475590    1.722290     0.0   \n",
       "event_count_all_time      4998.0    14.900160    8.175178     1.0   \n",
       "opened_sum_180d           4998.0     0.157063    0.426204     0.0   \n",
       "opened_mean_180d          2114.0     0.217306    0.353255     0.0   \n",
       "...                          ...          ...         ...     ...   \n",
       "days_since_first_event    4998.0  2669.424570  158.136559  1498.0   \n",
       "target                    4998.0     0.392957    0.488456     0.0   \n",
       "dow_sin                   4998.0     0.378318    0.420471    -1.0   \n",
       "dow_cos                   4998.0    -0.772429    0.288939    -1.0   \n",
       "clicked_momentum_180_365  4998.0     0.993549    0.242794     0.0   \n",
       "\n",
       "                                  25%          50%          75%     max  \n",
       "event_count_180d             0.000000     0.000000     1.000000    12.0  \n",
       "event_count_365d             0.000000     1.000000     2.000000    25.0  \n",
       "event_count_all_time        11.000000    14.000000    17.000000   106.0  \n",
       "opened_sum_180d              0.000000     0.000000     0.000000     6.0  \n",
       "opened_mean_180d             0.000000     0.000000     0.500000     1.0  \n",
       "...                               ...          ...          ...     ...  \n",
       "days_since_first_event    2603.000000  2719.000000  2784.000000  2825.0  \n",
       "target                       0.000000     0.000000     1.000000     1.0  \n",
       "dow_sin                      0.127877     0.433884     0.707107     1.0  \n",
       "dow_cos                     -0.967295    -0.875223    -0.683392     1.0  \n",
       "clicked_momentum_180_365     1.000000     1.000000     1.000000     3.5  \n",
       "\n",
       "[69 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nFeature Summary Statistics:\")\n",
    "display(df_aggregated.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {
    "papermill": {
     "duration": 0.00288,
     "end_time": "2026-01-29T23:48:29.151083",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.148203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.5 Quality Check on Aggregated Data\n",
    "\n",
    "Quick validation of the aggregated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.156563Z",
     "iopub.status.busy": "2026-01-29T23:48:29.156445Z",
     "iopub.status.idle": "2026-01-29T23:48:29.170446Z",
     "shell.execute_reply": "2026-01-29T23:48:29.169919Z"
    },
    "papermill": {
     "duration": 0.017482,
     "end_time": "2026-01-29T23:48:29.170956",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.153474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AGGREGATED DATA QUALITY CHECK\n",
      "============================================================\n",
      "\n",
      "âš ï¸ Columns with null values (22):\n",
      "   opened_mean_180d: 2,884 (57.7%)\n",
      "   opened_max_180d: 2,884 (57.7%)\n",
      "   clicked_mean_180d: 2,884 (57.7%)\n",
      "   clicked_max_180d: 2,884 (57.7%)\n",
      "   send_hour_mean_180d: 2,884 (57.7%)\n",
      "   send_hour_max_180d: 2,884 (57.7%)\n",
      "   bounced_mean_180d: 2,884 (57.7%)\n",
      "   bounced_max_180d: 2,884 (57.7%)\n",
      "   time_to_open_hours_mean_180d: 4,314 (86.3%)\n",
      "   time_to_open_hours_max_180d: 4,314 (86.3%)\n",
      "   ... and 12 more\n",
      "\n",
      "   Note: Nulls in aggregated features typically mean no events in that window.\n",
      "   Consider filling with 0 for count/sum features.\n",
      "\n",
      "âœ… Entity count matches: 4,998\n",
      "\n",
      "ðŸ“Š Feature Statistics:\n",
      "   Total features: 72\n",
      "   Numeric features: 68\n",
      "\n",
      "ðŸ“Š Lifecycle Quadrant vs Target:\n",
      "   Intense & Brief: 77.7% positive\n",
      "   Occasional & Loyal: 7.8% positive\n",
      "   One-shot: 59.1% positive\n",
      "   Steady & Loyal: 6.9% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AGGREGATED DATA QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for nulls\n",
    "null_counts = df_aggregated.isnull().sum()\n",
    "cols_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "if len(cols_with_nulls) > 0:\n",
    "    print(f\"\\nâš ï¸ Columns with null values ({len(cols_with_nulls)}):\")\n",
    "    for col, count in cols_with_nulls.head(10).items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
    "    if len(cols_with_nulls) > 10:\n",
    "        print(f\"   ... and {len(cols_with_nulls) - 10} more\")\n",
    "    print(\"\\n   Note: Nulls in aggregated features typically mean no events in that window.\")\n",
    "    print(\"   Consider filling with 0 for count/sum features.\")\n",
    "else:\n",
    "    print(\"\\nâœ… No null values in aggregated data\")\n",
    "\n",
    "# Check entity count matches\n",
    "original_entities = df[ENTITY_COLUMN].nunique()\n",
    "aggregated_entities = len(df_aggregated)\n",
    "\n",
    "if original_entities == aggregated_entities:\n",
    "    print(f\"\\nâœ… Entity count matches: {aggregated_entities:,}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Entity count mismatch!\")\n",
    "    print(f\"   Original: {original_entities:,}\")\n",
    "    print(f\"   Aggregated: {aggregated_entities:,}\")\n",
    "\n",
    "# Check feature statistics\n",
    "print(f\"\\nðŸ“Š Feature Statistics:\")\n",
    "numeric_agg_cols = df_aggregated.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if TARGET_COLUMN:\n",
    "    numeric_agg_cols = [c for c in numeric_agg_cols if c != TARGET_COLUMN]\n",
    "\n",
    "print(f\"   Total features: {len(df_aggregated.columns)}\")\n",
    "print(f\"   Numeric features: {len(numeric_agg_cols)}\")\n",
    "\n",
    "# Check for constant columns (no variance)\n",
    "const_cols = [c for c in numeric_agg_cols if df_aggregated[c].std() == 0]\n",
    "if const_cols:\n",
    "    print(f\"\\nâš ï¸ Constant columns (zero variance): {len(const_cols)}\")\n",
    "    print(f\"   {const_cols[:5]}{'...' if len(const_cols) > 5 else ''}\")\n",
    "\n",
    "# If lifecycle_quadrant was added, show its correlation with target\n",
    "if INCLUDE_LIFECYCLE_QUADRANT and TARGET_COLUMN and TARGET_COLUMN in df_aggregated.columns:\n",
    "    print(f\"\\nðŸ“Š Lifecycle Quadrant vs Target:\")\n",
    "    cross = pd.crosstab(df_aggregated[\"lifecycle_quadrant\"], df_aggregated[TARGET_COLUMN], normalize='index')\n",
    "    if 1 in cross.columns:\n",
    "        for quad in cross.index:\n",
    "            rate = cross.loc[quad, 1] * 100\n",
    "            print(f\"   {quad}: {rate:.1f}% positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {
    "papermill": {
     "duration": 0.002444,
     "end_time": "2026-01-29T23:48:29.176112",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.173668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.6 Save Aggregated Data and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.182198Z",
     "iopub.status.busy": "2026-01-29T23:48:29.182090Z",
     "iopub.status.idle": "2026-01-29T23:48:29.197067Z",
     "shell.execute_reply": "2026-01-29T23:48:29.196597Z"
    },
    "papermill": {
     "duration": 0.018529,
     "end_time": "2026-01-29T23:48:29.197554",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.179025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Aggregated data saved to: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_aggregated.parquet\n",
      "   Size: 321.7 KB\n"
     ]
    }
   ],
   "source": [
    "# Generate output paths\n",
    "original_name = Path(findings.source_path).stem\n",
    "findings_name = Path(FINDINGS_PATH).stem.replace(\"_findings\", \"\")\n",
    "\n",
    "# Save aggregated data as parquet\n",
    "AGGREGATED_DATA_PATH = FINDINGS_DIR / f\"{findings_name}_aggregated.parquet\"\n",
    "df_aggregated.to_parquet(AGGREGATED_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"\\u2705 Aggregated data saved to: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   Size: {AGGREGATED_DATA_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.204027Z",
     "iopub.status.busy": "2026-01-29T23:48:29.203867Z",
     "iopub.status.idle": "2026-01-29T23:48:29.427069Z",
     "shell.execute_reply": "2026-01-29T23:48:29.426634Z"
    },
    "papermill": {
     "duration": 0.227256,
     "end_time": "2026-01-29T23:48:29.427602",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.200346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating findings for aggregated data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: sans-serif; padding: 20px; color: #333;\">\n",
       "            <h2 style=\"color: #222;\">Data Exploration: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_aggregated.parquet</h2>\n",
       "            <div style=\"display: flex; gap: 20px; margin-bottom: 20px;\">\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Rows</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #222;\">4,998</span>\n",
       "                </div>\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Columns</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #222;\">72</span>\n",
       "                </div>\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Completeness</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #ff7f0e;\">84.1%</span>\n",
       "                </div>\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Memory</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #222;\">3.5 MB</span>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings saved to: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_aggregated_846212_findings.yaml\n",
      "âœ… Aggregated findings saved to: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_aggregated_846212_findings.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create new findings for aggregated data using DataExplorer\n",
    "print(\"\\nGenerating findings for aggregated data...\")\n",
    "\n",
    "explorer = DataExplorer(output_dir=str(FINDINGS_DIR))\n",
    "aggregated_findings = explorer.explore(\n",
    "    str(AGGREGATED_DATA_PATH),\n",
    "    name=f\"{findings_name}_aggregated\"\n",
    ")\n",
    "\n",
    "AGGREGATED_FINDINGS_PATH = explorer.last_findings_path\n",
    "print(f\"âœ… Aggregated findings saved to: {AGGREGATED_FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.433616Z",
     "iopub.status.busy": "2026-01-29T23:48:29.433514Z",
     "iopub.status.idle": "2026-01-29T23:48:29.454988Z",
     "shell.execute_reply": "2026-01-29T23:48:29.454640Z"
    },
    "papermill": {
     "duration": 0.025154,
     "end_time": "2026-01-29T23:48:29.455575",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.430421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Original findings updated with aggregation metadata: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_findings.yaml\n"
     ]
    }
   ],
   "source": [
    "# Update original findings with comprehensive aggregation metadata\n",
    "findings.time_series_metadata.aggregation_executed = True\n",
    "findings.time_series_metadata.aggregated_data_path = str(AGGREGATED_DATA_PATH)\n",
    "findings.time_series_metadata.aggregated_findings_path = str(AGGREGATED_FINDINGS_PATH)\n",
    "findings.time_series_metadata.aggregation_windows_used = WINDOWS\n",
    "findings.time_series_metadata.aggregation_timestamp = datetime.now().isoformat()\n",
    "\n",
    "# Add aggregation details to metadata\n",
    "findings.metadata[\"aggregation\"] = {\n",
    "    \"windows_used\": WINDOWS,\n",
    "    \"window_source\": window_source,\n",
    "    \"reference_date\": str(REFERENCE_DATE),\n",
    "    \"value_columns_count\": len(VALUE_COLUMNS),\n",
    "    \"priority_columns\": priority_cols,  # Divergent columns from 01c\n",
    "    \"agg_functions\": AGG_FUNCTIONS,\n",
    "    \"include_lifecycle_quadrant\": INCLUDE_LIFECYCLE_QUADRANT,\n",
    "    \"include_recency\": INCLUDE_RECENCY,\n",
    "    \"include_tenure\": INCLUDE_TENURE,\n",
    "    \"output_entities\": len(df_aggregated),\n",
    "    \"output_features\": len(df_aggregated.columns),\n",
    "    \"target_column\": TARGET_COLUMN,\n",
    "}\n",
    "\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"âœ… Original findings updated with aggregation metadata: {FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.461936Z",
     "iopub.status.busy": "2026-01-29T23:48:29.461837Z",
     "iopub.status.idle": "2026-01-29T23:48:29.465136Z",
     "shell.execute_reply": "2026-01-29T23:48:29.464734Z"
    },
    "papermill": {
     "duration": 0.00724,
     "end_time": "2026-01-29T23:48:29.465818",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.458578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "AGGREGATION COMPLETE - OUTPUT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ Files created:\n",
      "   1. Aggregated data: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_aggregated.parquet\n",
      "   2. Aggregated findings: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_aggregated_846212_findings.yaml\n",
      "   3. Updated original findings: /Users/Vital/python/CustomerRetention/experiments/findings/customer_emails_408768_findings.yaml\n",
      "\n",
      "ðŸ“Š Transformation stats:\n",
      "   Input events: 74,471\n",
      "   Output entities: 4,998\n",
      "   Features created: 72\n",
      "\n",
      "âš™ï¸ Configuration applied:\n",
      "   Windows: ['180d', '365d', 'all_time'] (from 01a recommendations)\n",
      "   Aggregation functions: ['sum', 'mean', 'max', 'count']\n",
      "   Lifecycle quadrant: included (from 01a recommendation)\n",
      "\n",
      "ðŸŽ¯ Ready for modeling:\n",
      "   Entity column: customer_id\n",
      "   Target column: target\n",
      "   Target positive rate: 39.3%\n",
      "\n",
      "âš ï¸ DRIFT WARNING: High drift risk detected in 01a\n",
      "   Volume drift: declining\n",
      "   Consider: temporal validation splits, monitoring for distribution shift\n"
     ]
    }
   ],
   "source": [
    "# Summary of outputs\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATION COMPLETE - OUTPUT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“ Files created:\")\n",
    "print(f\"   1. Aggregated data: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   2. Aggregated findings: {AGGREGATED_FINDINGS_PATH}\")\n",
    "print(f\"   3. Updated original findings: {FINDINGS_PATH}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Transformation stats:\")\n",
    "print(f\"   Input events: {len(df):,}\")\n",
    "print(f\"   Output entities: {len(df_aggregated):,}\")\n",
    "print(f\"   Features created: {len(df_aggregated.columns)}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Configuration applied:\")\n",
    "print(f\"   Windows: {WINDOWS} (from {window_source})\")\n",
    "print(f\"   Aggregation functions: {AGG_FUNCTIONS}\")\n",
    "if priority_cols:\n",
    "    print(f\"   Priority columns (from 01c divergence): {priority_cols}\")\n",
    "if INCLUDE_LIFECYCLE_QUADRANT:\n",
    "    print(f\"   Lifecycle quadrant: included (from 01a recommendation)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ready for modeling:\")\n",
    "print(f\"   Entity column: {ENTITY_COLUMN}\")\n",
    "if TARGET_COLUMN:\n",
    "    print(f\"   Target column: {TARGET_COLUMN}\")\n",
    "    if TARGET_COLUMN in df_aggregated.columns:\n",
    "        positive_rate = df_aggregated[TARGET_COLUMN].mean() * 100\n",
    "        print(f\"   Target positive rate: {positive_rate:.1f}%\")\n",
    "\n",
    "# Drift warning if applicable\n",
    "if ts_meta.drift_risk_level == \"high\":\n",
    "    print(f\"\\nâš ï¸ DRIFT WARNING: High drift risk detected in 01a\")\n",
    "    print(f\"   Volume drift: {ts_meta.volume_drift_risk or 'unknown'}\")\n",
    "    print(f\"   Consider: temporal validation splits, monitoring for distribution shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bead9c",
   "metadata": {
    "papermill": {
     "duration": 0.00256,
     "end_time": "2026-01-29T23:48:29.471287",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.468727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1d.X Leakage Validation\n",
    "\n",
    "**CRITICAL CHECK:** Verify no target leakage in aggregated features before proceeding.\n",
    "\n",
    "| Check | What It Detects | Severity |\n",
    "|-------|-----------------|----------|\n",
    "| LD052 | Target column or target-derived features in feature matrix | CRITICAL |\n",
    "| LD053 | Domain patterns (churn/cancel/retain) with high correlation | CRITICAL |\n",
    "| LD001-003 | Suspiciously high feature-target correlations | HIGH |\n",
    "\n",
    "**If any CRITICAL issues are detected, do NOT proceed to modeling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82b71ce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T23:48:29.477271Z",
     "iopub.status.busy": "2026-01-29T23:48:29.477169Z",
     "iopub.status.idle": "2026-01-29T23:48:30.229796Z",
     "shell.execute_reply": "2026-01-29T23:48:30.229401Z"
    },
    "papermill": {
     "duration": 0.756654,
     "end_time": "2026-01-29T23:48:30.230464",
     "exception": false,
     "start_time": "2026-01-29T23:48:29.473810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEAKAGE VALIDATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "âœ… PASSED: No critical leakage issues detected\n",
      "   Total checks run: 83\n",
      "\n",
      "   You may proceed to feature engineering and modeling.\n"
     ]
    }
   ],
   "source": [
    "# Leakage validation - MUST pass before proceeding to modeling\n",
    "from customer_retention.analysis.diagnostics import LeakageDetector\n",
    "\n",
    "if TARGET_COLUMN and TARGET_COLUMN in df_aggregated.columns:\n",
    "    detector = LeakageDetector()\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_cols = [c for c in df_aggregated.columns if c not in [ENTITY_COLUMN, TARGET_COLUMN]]\n",
    "    X = df_aggregated[feature_cols]\n",
    "    y = df_aggregated[TARGET_COLUMN]\n",
    "    \n",
    "    # Run leakage checks\n",
    "    result = detector.run_all_checks(X, y, include_pit=False)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"LEAKAGE VALIDATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if result.passed:\n",
    "        print(\"\\nâœ… PASSED: No critical leakage issues detected\")\n",
    "        print(f\"   Total checks run: {len(result.checks)}\")\n",
    "        print(\"\\n   You may proceed to feature engineering and modeling.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ FAILED: Critical leakage issues detected!\")\n",
    "        print(f\"   Critical issues: {len(result.critical_issues)}\")\n",
    "        print(\"\\n   DO NOT proceed to modeling until issues are resolved:\\n\")\n",
    "        for issue in result.critical_issues:\n",
    "            print(f\"   [{issue.check_id}] {issue.feature}: {issue.recommendation}\")\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        raise ValueError(f\"Leakage detected: {len(result.critical_issues)} critical issues\")\n",
    "else:\n",
    "    print(\"No target column - skipping leakage validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {
    "papermill": {
     "duration": 0.002695,
     "end_time": "2026-01-29T23:48:30.236243",
     "exception": false,
     "start_time": "2026-01-29T23:48:30.233548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Did\n",
    "\n",
    "In this notebook, we transformed event-level data to entity-level, applying all insights from 01a-01c:\n",
    "\n",
    "1. **Loaded findings** from prior notebooks (windows, patterns, quality)\n",
    "2. **Configured aggregation** using recommended windows from 01a\n",
    "3. **Prioritized features** based on divergent columns from 01c velocity/momentum analysis\n",
    "4. **Added lifecycle_quadrant** as recommended by 01a segmentation analysis\n",
    "5. **Added entity-level target** for downstream modeling\n",
    "6. **Saved outputs** - aggregated data, findings, and metadata\n",
    "\n",
    "## How Findings Were Applied\n",
    "\n",
    "| Finding | Source | Application |\n",
    "|---------|--------|-------------|\n",
    "| Aggregation windows | 01a | Used `suggested_aggregations` instead of defaults |\n",
    "| Lifecycle quadrant | 01a | Added as categorical feature for model |\n",
    "| Divergent columns | 01c | Prioritized in feature list (velocity/momentum signal) |\n",
    "| Drift warning | 01a | Flagged for temporal validation consideration |\n",
    "\n",
    "## Output Files\n",
    "\n",
    "| File | Purpose | Next Use |\n",
    "|------|---------|----------|\n",
    "| `*_aggregated.parquet` | Entity-level data with temporal features | Input for notebooks 02-04 |\n",
    "| `*_aggregated_findings.yaml` | Auto-profiled findings | Loaded by 02_column_deep_dive |\n",
    "| Original findings (updated) | Aggregation tracking | Reference and lineage |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Event Bronze Track complete!** Continue with the **Entity Bronze Track** on the aggregated data:\n",
    "\n",
    "1. **02_column_deep_dive.ipynb** - Profile the aggregated feature distributions\n",
    "2. **03_quality_assessment.ipynb** - Run quality checks on entity-level data  \n",
    "3. **04_relationship_analysis.ipynb** - Analyze feature correlations and target relationships\n",
    "\n",
    "The notebooks will auto-discover the aggregated findings file (most recently modified).\n",
    "\n",
    "```python\n",
    "# The aggregated findings file is now the most recent, so notebooks 02-04\n",
    "# will automatically use it via the standard discovery pattern.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.873998,
   "end_time": "2026-01-29T23:48:30.556749",
   "environment_variables": {},
   "exception": null,
   "input_path": "exploration_notebooks/01d_event_aggregation.ipynb",
   "output_path": "docs/tutorial/executed/01d_event_aggregation.ipynb",
   "parameters": {},
   "start_time": "2026-01-29T23:48:24.682751",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
