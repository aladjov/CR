{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Start Here: Prerequisites\n",
    "\n",
    "> **No sample data required!** This framework works directly with your own CSV, Parquet, or Delta files. The datasets below are internal examples for learning - skip to **01_data_discovery.ipynb** if you have your own data.\n",
    "\n",
    "**Purpose:** Set up your environment and optionally download sample datasets for learning.\n",
    "\n",
    "**What you'll do:**\n",
    "- Verify your Python environment\n",
    "- (Optional) Set up Kaggle API credentials\n",
    "- (Optional) Download sample churn datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 0.1 Verify Environment\n",
    "\n",
    "First, let's make sure the customer_retention package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import customer_retention\n",
    "    print(f\"customer_retention is installed\")\n",
    "except ImportError:\n",
    "    print(\"customer_retention not found. Install with:\")\n",
    "    print(\"  uv sync\")\n",
    "    print(\"  # or: pip install -e .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 0.2 Available Datasets\n",
    "\n",
    "This framework includes several internal datasets for testing and learning. **You do not need any of these to use the framework with your own data.**\n",
    "\n",
    "### Entity-Level Datasets (one row per customer)\n",
    "Use these with the standard exploration flow (notebooks 02, 03, 04).\n",
    "\n",
    "| Dataset | Status | Description |\n",
    "|---------|--------|-------------|\n",
    "| `customer_retention_retail.csv` | Included | Retail customer retention (~31K rows) |\n",
    "| `bank_customer_churn.csv` | Download | Bank customer churn (~10K rows) |\n",
    "| `netflix_customer_churn.csv` | Download | Netflix subscription churn (~10K rows) |\n",
    "\n",
    "### Event-Level Datasets (multiple rows per customer)\n",
    "Use these with the Event Bronze Track (notebooks 01a, 01b, 01c, 01d).\n",
    "\n",
    "| Dataset | Status | Description |\n",
    "|---------|--------|-------------|\n",
    "| `customer_transactions.csv` | Included | Transaction events (~5K rows) |\n",
    "| `customer_emails.csv` | Included | Email engagement events (large) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "FIXTURES_DIR = Path(\"../tests/fixtures\")\n",
    "\n",
    "# Entity-level datasets\n",
    "entity_datasets = {\n",
    "    \"customer_retention_retail.csv\": \"Included\",\n",
    "    \"bank_customer_churn.csv\": \"Download from Kaggle\",\n",
    "    \"netflix_customer_churn.csv\": \"Download from Kaggle\",\n",
    "}\n",
    "\n",
    "# Event-level datasets (internal)\n",
    "event_datasets = {\n",
    "    \"customer_transactions.csv\": \"Included\",\n",
    "    \"customer_emails.csv\": \"Included\",\n",
    "}\n",
    "\n",
    "print(\"Entity-Level Datasets:\")\n",
    "print(\"-\" * 50)\n",
    "for name, source in entity_datasets.items():\n",
    "    path = FIXTURES_DIR / name\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  [x] {name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  [ ] {name} - {source}\")\n",
    "\n",
    "print(\"\\nEvent-Level Datasets:\")\n",
    "print(\"-\" * 50)\n",
    "for name, source in event_datasets.items():\n",
    "    path = FIXTURES_DIR / name\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  [x] {name} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  [ ] {name} - {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 0.3 Kaggle API Setup\n",
    "\n",
    "To download datasets from Kaggle, you need to set up API credentials:\n",
    "\n",
    "1. Create a Kaggle account at https://www.kaggle.com\n",
    "2. Go to **Account Settings** → **API** → **Create New Token**\n",
    "3. This downloads `kaggle.json` - move it to `~/.kaggle/kaggle.json`\n",
    "4. Set permissions: `chmod 600 ~/.kaggle/kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Kaggle credentials exist\n",
    "kaggle_config = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "\n",
    "if kaggle_config.exists():\n",
    "    print(f\"Kaggle credentials found at {kaggle_config}\")\n",
    "else:\n",
    "    print(\"Kaggle credentials not found.\")\n",
    "    print(\"\\nTo set up:\")\n",
    "    print(\"1. Go to https://www.kaggle.com/settings\")\n",
    "    print(\"2. Scroll to 'API' section and click 'Create New Token'\")\n",
    "    print(f\"3. Move downloaded file to {kaggle_config}\")\n",
    "    print(f\"4. Run: chmod 600 {kaggle_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 0.4 Download Kaggle Datasets\n",
    "\n",
    "Run the cells below to download each dataset. You only need to do this once.\n",
    "\n",
    "### Bank Customer Churn Dataset\n",
    "Source: https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Bank Customer Churn dataset\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "FIXTURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "bank_churn_path = FIXTURES_DIR / \"bank_customer_churn.csv\"\n",
    "\n",
    "if bank_churn_path.exists():\n",
    "    print(f\"Already exists: {bank_churn_path}\")\n",
    "else:\n",
    "    print(\"Downloading Bank Customer Churn dataset...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\", \"-d\", \"gauravtopre/bank-customer-churn-dataset\",\n",
    "            \"-p\", str(FIXTURES_DIR), \"--unzip\"\n",
    "        ], check=True)\n",
    "        # Rename to consistent name\n",
    "        downloaded = FIXTURES_DIR / \"Bank_Churn.csv\"\n",
    "        if downloaded.exists():\n",
    "            shutil.move(downloaded, bank_churn_path)\n",
    "        print(f\"Downloaded to: {bank_churn_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: kaggle CLI not found. Install with: pip install kaggle\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Netflix Customer Churn Dataset\n",
    "Source: https://www.kaggle.com/datasets/vasifasad/netflix-customer-churn-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Netflix Customer Churn dataset\n",
    "netflix_churn_path = FIXTURES_DIR / \"netflix_customer_churn.csv\"\n",
    "\n",
    "if netflix_churn_path.exists():\n",
    "    print(f\"Already exists: {netflix_churn_path}\")\n",
    "else:\n",
    "    print(\"Downloading Netflix Customer Churn dataset...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\", \"-d\", \"vasifasad/netflix-customer-churn-prediction\",\n",
    "            \"-p\", str(FIXTURES_DIR), \"--unzip\"\n",
    "        ], check=True)\n",
    "        print(f\"Downloaded to: {netflix_churn_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: kaggle CLI not found. Install with: pip install kaggle\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error downloading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 0.5 Verify Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_datasets = {**entity_datasets, **event_datasets}\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name in all_datasets.keys():\n",
    "    path = FIXTURES_DIR / name\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Rows: {len(df):,}\")\n",
    "        print(f\"  Columns: {len(df.columns)}\")\n",
    "        print(f\"  Columns: {', '.join(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    else:\n",
    "        print(f\"\\n{name}: Not downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Temporal Framework Overview\n",
    "\n",
    "This framework includes a **leakage-safe temporal infrastructure** for preventing data leakage in ML pipelines:\n",
    "\n",
    "- **Timestamp Management**: Automatic detection and handling of `feature_timestamp` and `label_timestamp`\n",
    "- **Versioned Snapshots**: Point-in-time training snapshots with integrity hashing\n",
    "- **Scenario Detection**: Automatic detection of production vs Kaggle-style datasets\n",
    "- **Leakage Detection**: Multi-probe validation (correlation, separation, temporal logic)\n",
    "\n",
    "The temporal framework ensures that:\n",
    "1. Features are only computed using data available at prediction time\n",
    "2. Training data is versioned and reproducible\n",
    "3. Temporal leakage is detected before model training\n",
    "\n",
    "---\n",
    "\n",
    "## 0.6 Using the Temporal Framework\n",
    "\n",
    "### Loading Data with Snapshot Manager\n",
    "\n",
    "For production use, load data through the snapshot system to ensure reproducibility:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from customer_retention.stages.temporal import SnapshotManager, UnifiedDataPreparer, ScenarioDetector\n",
    "\n",
    "output_path = Path(\"../experiments/findings\")\n",
    "snapshot_manager = SnapshotManager(output_path)\n",
    "\n",
    "snapshots = snapshot_manager.list_snapshots()\n",
    "if snapshots:\n",
    "    latest = snapshot_manager.get_latest_snapshot()\n",
    "    df, metadata = snapshot_manager.load_snapshot(latest)\n",
    "    print(f\"Loaded {latest}: {df.shape}, created {metadata['created_at']}\")\n",
    "```\n",
    "\n",
    "### Auto-Detecting Dataset Scenario\n",
    "\n",
    "The framework automatically detects whether your data is production (with timestamps) or Kaggle-style:\n",
    "\n",
    "```python\n",
    "from customer_retention.stages.temporal import ScenarioDetector\n",
    "\n",
    "detector = ScenarioDetector()\n",
    "scenario, config, discovery_result = detector.detect(df, target_column=\"churned\")\n",
    "\n",
    "print(f\"Scenario: {scenario}\")\n",
    "print(f\"Feature timestamp: {config.feature_timestamp_column}\")\n",
    "print(f\"Label timestamp: {config.label_timestamp_column}\")\n",
    "print(f\"Strategy: {config.strategy.value}\")\n",
    "```\n",
    "\n",
    "### Manual Override (When Auto-Detection Fails)\n",
    "\n",
    "If auto-detection picks wrong columns or an unsuitable strategy, bypass it entirely by creating `TimestampConfig` directly:\n",
    "\n",
    "```python\n",
    "from customer_retention.stages.temporal import TimestampManager, TimestampConfig, TimestampStrategy\n",
    "\n",
    "config = TimestampConfig(\n",
    "    strategy=TimestampStrategy.PRODUCTION,\n",
    "    feature_timestamp_column=\"my_observation_date\",\n",
    "    label_timestamp_column=\"my_outcome_date\",\n",
    "    observation_window_days=90,\n",
    ")\n",
    "manager = TimestampManager(config)\n",
    "df_with_timestamps = manager.ensure_timestamps(df)\n",
    "```\n",
    "\n",
    "**Available strategies:**\n",
    "\n",
    "| Strategy | When to Use |\n",
    "|----------|-------------|\n",
    "| `PRODUCTION` | Data has explicit timestamp columns |\n",
    "| `DERIVED` | Timestamps can be computed from other columns (e.g., tenure) |\n",
    "| `SYNTHETIC_FIXED` | No temporal info - use fixed date for all rows |\n",
    "| `SYNTHETIC_RANDOM` | No temporal info - generate random dates within range |\n",
    "| `SYNTHETIC_INDEX` | No temporal info - generate dates based on row order |\n",
    "\n",
    "**Force synthetic timestamps (Kaggle-style data):**\n",
    "\n",
    "```python\n",
    "config = TimestampConfig(\n",
    "    strategy=TimestampStrategy.SYNTHETIC_FIXED,\n",
    "    synthetic_base_date=\"2024-01-01\",\n",
    "    observation_window_days=90,\n",
    ")\n",
    "```\n",
    "\n",
    "**Derive timestamps from tenure column:**\n",
    "\n",
    "```python\n",
    "config = TimestampConfig(\n",
    "    strategy=TimestampStrategy.DERIVED,\n",
    "    derivation_config={\n",
    "        \"feature_derivation\": {\n",
    "            \"formula\": \"reference_date - tenure_months\",\n",
    "            \"sources\": [\"tenure_months\"],\n",
    "        }\n",
    "    },\n",
    "    observation_window_days=90,\n",
    ")\n",
    "```\n",
    "\n",
    "### Creating a Training Snapshot from Raw Data\n",
    "\n",
    "```python\n",
    "from customer_retention.stages.temporal import UnifiedDataPreparer\n",
    "from customer_retention.core.config import TemporalConfig\n",
    "\n",
    "config = TemporalConfig(\n",
    "    feature_timestamp_column=\"feature_timestamp\",\n",
    "    label_timestamp_column=\"label_timestamp\",\n",
    ")\n",
    "\n",
    "preparer = UnifiedDataPreparer(output_path=Path(\"../experiments/findings\"), timestamp_config=config)\n",
    "\n",
    "unified_df = preparer.prepare_from_raw(\n",
    "    df=raw_df, target_column=\"churned\", entity_column=\"customer_id\"\n",
    ")\n",
    "\n",
    "snapshot_df, metadata = preparer.create_training_snapshot(df=unified_df, snapshot_name=\"training\")\n",
    "print(f\"Created snapshot: {metadata['snapshot_id']}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You're ready to start exploring! Continue to **01_data_discovery.ipynb**.\n",
    "\n",
    "**Using your own data?** Just set `DATA_PATH` to your file:\n",
    "```python\n",
    "DATA_PATH = \"/path/to/your/data.csv\"\n",
    "```\n",
    "\n",
    "**Using sample datasets?** Choose one based on your learning goal:\n",
    "```python\n",
    "# Entity-level (standard flow)\n",
    "DATA_PATH = \"../tests/fixtures/customer_retention_retail.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/bank_customer_churn.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/netflix_customer_churn.csv\"\n",
    "\n",
    "# Event-level (time series flow)\n",
    "DATA_PATH = \"../tests/fixtures/customer_transactions.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/customer_emails.csv\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
