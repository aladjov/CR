{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Start Here: Data Discovery\n",
    "\n",
    "**Purpose:** Create a point-in-time snapshot and understand your dataset's structure through automatic profiling.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to create temporally-safe training snapshots\n",
    "- How automatic type inference works and when to override it\n",
    "- How to identify entity-level vs event-level data\n",
    "- How to set up your target column for downstream analysis\n",
    "\n",
    "**Outputs:**\n",
    "- Point-in-time training snapshot (Parquet)\n",
    "- Dataset overview (rows, columns, memory, format, structure)\n",
    "- Automatic column type inference with confidence scores\n",
    "- Saved exploration findings (YAML)\n",
    "\n",
    "---\n",
    "\n",
    "## How to Read This Notebook\n",
    "\n",
    "Each section includes:\n",
    "- **ðŸ“Š Charts** - Interactive Plotly visualizations\n",
    "- **ðŸ“– Interpretation Guide** - How to read and understand the output\n",
    "- **âœ… Actions** - What to do based on the findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1.1 Configuration\n",
    "\n",
    "Configure your data source and target column **before** running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import DataExplorer\n",
    "from customer_retention.analysis.auto_explorer.findings import TimeSeriesMetadata\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table, console\n",
    "from customer_retention.stages.validation import TimeSeriesDetector\n",
    "from customer_retention.core.config.column_config import DatasetGranularity, ColumnType\n",
    "from customer_retention.stages.profiling import TypeDetector\n",
    "from customer_retention.stages.temporal import (\n",
    "    ScenarioDetector, UnifiedDataPreparer, SnapshotManager,\n",
    "    TimestampConfig, TimestampStrategy, PointInTimeRegistry, CutoffAnalyzer\n",
    ")\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Set these before running\n",
    "# =============================================================================\n",
    "\n",
    "# DATA_PATH: Path to your data file (CSV, Parquet, or Delta)\n",
    "DATA_PATH = \"../tests/fixtures/customer_retention_retail.csv\"\n",
    "\n",
    "# TARGET_COLUMN: Your prediction target (set to None for auto-detection)\n",
    "TARGET_COLUMN = \"unsubscribed\"\n",
    "\n",
    "# ENTITY_COLUMN: Customer/user ID column (set to None for auto-detection)\n",
    "ENTITY_COLUMN = None\n",
    "\n",
    "# LABEL_WINDOW_DAYS: Days after last activity to derive label timestamp\n",
    "# Used when no explicit label timestamp column exists (e.g., churn_date)\n",
    "# Default: 180 days (6 months observation window)\n",
    "LABEL_WINDOW_DAYS = 180\n",
    "\n",
    "# TIMESTAMP_CONFIG: Override auto-detection if needed (set to None for auto-detection)\n",
    "# Example manual override:\n",
    "# TIMESTAMP_CONFIG = TimestampConfig(\n",
    "#     strategy=TimestampStrategy.PRODUCTION,\n",
    "#     feature_timestamp_column=\"observation_date\",\n",
    "#     label_timestamp_column=\"churn_date\",\n",
    "# )\n",
    "TIMESTAMP_CONFIG = None\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLE DATASETS (for learning/testing only)\n",
    "# =============================================================================\n",
    "# ENTITY-LEVEL (one row per customer):\n",
    "# DATA_PATH = \"../tests/fixtures/customer_retention_retail.csv\"\n",
    "# DATA_PATH = \"../tests/fixtures/bank_customer_churn.csv\"\n",
    "# DATA_PATH = \"../tests/fixtures/netflix_customer_churn.csv\"\n",
    "#\n",
    "# EVENT-LEVEL (multiple rows per customer):\n",
    "# DATA_PATH = \"../tests/fixtures/customer_transactions.csv\"\n",
    "DATA_PATH = \"../tests/fixtures/customer_emails.csv\"\n",
    "# =============================================================================\n",
    "\n",
    "# OUTPUT_DIR: All outputs go here (gitignored)\n",
    "OUTPUT_DIR = Path(\"../experiments/findings\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1.2 Load Data & Create Point-in-Time Snapshot\n",
    "\n",
    "**This is the critical first step.** We:\n",
    "1. Load raw data\n",
    "2. Detect temporal scenario (production timestamps, derived, or synthetic)\n",
    "3. Create a versioned snapshot with `feature_timestamp` and `label_timestamp`\n",
    "4. All subsequent analysis uses the snapshot data\n",
    "\n",
    "This ensures temporal integrity and prevents data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "raw_df = pd.read_csv(DATA_PATH) if DATA_PATH.endswith('.csv') else pd.read_parquet(DATA_PATH)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Raw Data Loaded\")\n",
    "console.metric(\"Source\", DATA_PATH)\n",
    "console.metric(\"Rows\", f\"{len(raw_df):,}\")\n",
    "console.metric(\"Columns\", len(raw_df.columns))\n",
    "console.end_section()\n",
    "\n",
    "# Detect granularity and entity column\n",
    "type_detector = TypeDetector()\n",
    "granularity_result = type_detector.detect_granularity(raw_df)\n",
    "entity_column = ENTITY_COLUMN or granularity_result.entity_column\n",
    "\n",
    "# Detect or use provided timestamp configuration\n",
    "if TIMESTAMP_CONFIG:\n",
    "    ts_config = TIMESTAMP_CONFIG\n",
    "    scenario = \"MANUAL_OVERRIDE\"\n",
    "    discovery_result = None\n",
    "    console.info(f\"Using manual timestamp config: {ts_config.strategy.value}\")\n",
    "else:\n",
    "    detector = ScenarioDetector(label_window_days=LABEL_WINDOW_DAYS)\n",
    "    scenario, ts_config, discovery_result = detector.detect(raw_df, TARGET_COLUMN)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Temporal Scenario Detection\")\n",
    "console.metric(\"Scenario\", scenario)\n",
    "console.metric(\"Strategy\", ts_config.strategy.value)\n",
    "console.metric(\"Label Window\", f\"{LABEL_WINDOW_DAYS} days\")\n",
    "\n",
    "if discovery_result:\n",
    "    if discovery_result.feature_timestamp:\n",
    "        source_col = discovery_result.feature_timestamp.column_name\n",
    "        if discovery_result.feature_timestamp.is_derived:\n",
    "            console.metric(\"Feature Timestamp\", f\"derived from {discovery_result.feature_timestamp.source_columns}\")\n",
    "        else:\n",
    "            was_promoted = \"promoted\" in discovery_result.feature_timestamp.notes.lower()\n",
    "            if was_promoted:\n",
    "                console.metric(\"Feature Timestamp\", f\"{source_col} (auto-selected as latest activity)\")\n",
    "            else:\n",
    "                console.metric(\"Feature Timestamp\", f\"{source_col} (explicit match)\")\n",
    "    \n",
    "    if discovery_result.label_timestamp:\n",
    "        if discovery_result.label_timestamp.is_derived:\n",
    "            console.metric(\"Label Timestamp\", f\"derived: {discovery_result.label_timestamp.derivation_formula}\")\n",
    "        else:\n",
    "            console.metric(\"Label Timestamp\", f\"{discovery_result.label_timestamp.column_name} (explicit match)\")\n",
    "    \n",
    "    if \"datetime_ordering\" in discovery_result.discovery_report:\n",
    "        ordering = discovery_result.discovery_report[\"datetime_ordering\"]\n",
    "        if ordering:\n",
    "            console.info(f\"Datetime column ordering: {' â†’ '.join(ordering)}\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlzs6bfy2z",
   "metadata": {},
   "source": [
    "### Cutoff Date Selection\n",
    "\n",
    "The chart below shows the temporal distribution of your data. Use it to select an appropriate cutoff date:\n",
    "\n",
    "- **Top chart**: Records per time bin and cumulative count\n",
    "- **Bottom chart**: Train/Score split percentage at each potential cutoff date\n",
    "- **Suggested cutoff** (blue dashed): Achieves ~90% train / 10% score split\n",
    "\n",
    "**Final data allocation:**\n",
    "- Cutoff: 90% train, 10% score (holdout for final evaluation)\n",
    "- Train/Test split: 89% train, 11% test (from the 90%)\n",
    "- **Result: ~80% training, ~10% test, ~10% score**\n",
    "\n",
    "Adjust `CUTOFF_DATE` below if the suggested date doesn't fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lfuwm8yytw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal distribution for cutoff selection\n",
    "from customer_retention.stages.temporal import DatetimeOrderAnalyzer\n",
    "\n",
    "cutoff_analyzer = CutoffAnalyzer()\n",
    "cutoff_analysis = None\n",
    "\n",
    "# Derive last_action_date by coalescing all datetime columns (latest-median first)\n",
    "datetime_order_analyzer = DatetimeOrderAnalyzer()\n",
    "last_action_series = datetime_order_analyzer.derive_last_action_date(raw_df)\n",
    "\n",
    "# Fallback to feature_timestamp column if no datetime columns found\n",
    "timestamp_col = None\n",
    "if last_action_series is None:\n",
    "    if discovery_result and discovery_result.feature_timestamp:\n",
    "        if not discovery_result.feature_timestamp.is_derived:\n",
    "            timestamp_col = discovery_result.feature_timestamp.column_name\n",
    "\n",
    "# Check registry for existing cutoff\n",
    "pit_registry = PointInTimeRegistry(OUTPUT_DIR)\n",
    "registry_cutoff = pit_registry.check_consistency().reference_cutoff\n",
    "\n",
    "if last_action_series is not None:\n",
    "    cutoff_analysis = cutoff_analyzer.analyze(raw_df, timestamp_series=last_action_series, n_bins=50)\n",
    "    data_suggested_cutoff = cutoff_analysis.suggest_cutoff(train_ratio=0.9)\n",
    "\n",
    "    console.start_section()\n",
    "    console.header(\"Cutoff Date Analysis\")\n",
    "    console.metric(\"Timestamp Source\", \"last_action_date (coalesced)\")\n",
    "    console.metric(\"Coverage\", f\"{cutoff_analysis.covered_rows:,} / {cutoff_analysis.source_rows:,} rows ({cutoff_analysis.coverage_ratio:.1%})\")\n",
    "    if cutoff_analysis.coverage_ratio < 0.95:\n",
    "        console.warning(\"Low timestamp coverage â€” consider filling missing dates\")\n",
    "    console.metric(\"Date Range\", f\"{cutoff_analysis.date_range[0].strftime('%Y-%m-%d')} to {cutoff_analysis.date_range[1].strftime('%Y-%m-%d')}\")\n",
    "    console.metric(\"Data-Suggested Cutoff\", data_suggested_cutoff.strftime(\"%Y-%m-%d\"))\n",
    "    split = cutoff_analysis.get_split_at_date(data_suggested_cutoff)\n",
    "    console.metric(\"At Suggested Split\", f\"{split['train_pct']:.0f}% train / {split['score_pct']:.0f}% score\")\n",
    "\n",
    "    if registry_cutoff:\n",
    "        console.warning(f\"Registry has cutoff: {registry_cutoff.date()} (may be stale)\")\n",
    "        console.info(\"To clear: pit_registry.clear_registry()\")\n",
    "\n",
    "    # Show milestones for reference\n",
    "    milestones = cutoff_analysis.get_percentage_milestones(step=10)\n",
    "    if milestones:\n",
    "        console.subheader(\"Reference Dates (10% intervals)\")\n",
    "        for m in milestones:\n",
    "            console.info(f\"  {m['train_pct']:.0f}% train: {m['date'].strftime('%Y-%m-%d')}\")\n",
    "    console.end_section()\n",
    "elif timestamp_col:\n",
    "    cutoff_analysis = cutoff_analyzer.analyze(raw_df, timestamp_column=timestamp_col, n_bins=50)\n",
    "    data_suggested_cutoff = cutoff_analysis.suggest_cutoff(train_ratio=0.9)\n",
    "\n",
    "    console.start_section()\n",
    "    console.header(\"Cutoff Date Analysis\")\n",
    "    console.metric(\"Timestamp Column\", timestamp_col)\n",
    "    console.metric(\"Coverage\", f\"{cutoff_analysis.covered_rows:,} / {cutoff_analysis.source_rows:,} rows ({cutoff_analysis.coverage_ratio:.1%})\")\n",
    "    if cutoff_analysis.coverage_ratio < 0.95:\n",
    "        console.warning(\"Low timestamp coverage â€” consider filling missing dates\")\n",
    "    console.metric(\"Date Range\", f\"{cutoff_analysis.date_range[0].strftime('%Y-%m-%d')} to {cutoff_analysis.date_range[1].strftime('%Y-%m-%d')}\")\n",
    "    console.metric(\"Data-Suggested Cutoff\", data_suggested_cutoff.strftime(\"%Y-%m-%d\"))\n",
    "    split = cutoff_analysis.get_split_at_date(data_suggested_cutoff)\n",
    "    console.metric(\"At Suggested Split\", f\"{split['train_pct']:.0f}% train / {split['score_pct']:.0f}% score\")\n",
    "\n",
    "    if registry_cutoff:\n",
    "        console.warning(f\"Registry has cutoff: {registry_cutoff.date()} (may be stale)\")\n",
    "        console.info(\"To clear: pit_registry.clear_registry()\")\n",
    "\n",
    "    milestones = cutoff_analysis.get_percentage_milestones(step=10)\n",
    "    if milestones:\n",
    "        console.subheader(\"Reference Dates (10% intervals)\")\n",
    "        for m in milestones:\n",
    "            console.info(f\"  {m['train_pct']:.0f}% train: {m['date'].strftime('%Y-%m-%d')}\")\n",
    "    console.end_section()\n",
    "else:\n",
    "    data_suggested_cutoff = datetime.now()\n",
    "    console.start_section()\n",
    "    console.header(\"Cutoff Date Analysis\")\n",
    "    console.warning(\"No timestamp column detected\")\n",
    "    console.metric(\"Default Cutoff\", data_suggested_cutoff.strftime(\"%Y-%m-%d\"))\n",
    "    if registry_cutoff:\n",
    "        console.info(f\"Registry cutoff: {registry_cutoff.date()}\")\n",
    "    console.end_section()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ls2ezi1t5ag",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUTOFF DATE SELECTION - Set your preferred cutoff date\n",
    "# =============================================================================\n",
    "# Options:\n",
    "#   None = use data-suggested cutoff (~90/10 split)\n",
    "#   datetime(YYYY, M, D) = use specific date\n",
    "#\n",
    "# To clear stale registry: pit_registry.clear_registry()\n",
    "# =============================================================================\n",
    "CUTOFF_DATE = None  # e.g., datetime(2017, 7, 1)\n",
    "\n",
    "# Compute final selected cutoff\n",
    "selected_cutoff = CUTOFF_DATE or data_suggested_cutoff\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Selected Cutoff Date\")\n",
    "if CUTOFF_DATE:\n",
    "    console.info(f\"Manual override: {CUTOFF_DATE.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    console.info(f\"Using data-suggested: {selected_cutoff.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "if cutoff_analysis:\n",
    "    split = cutoff_analysis.get_split_at_date(selected_cutoff)\n",
    "    console.metric(\"Train/Score Split\", f\"{split['train_pct']:.0f}% / {split['score_pct']:.0f}%\")\n",
    "    console.metric(\"Train Records\", f\"{split['train_count']:,}\")\n",
    "    console.metric(\"Score Records\", f\"{split['score_count']:,}\")\n",
    "console.end_section()\n",
    "\n",
    "# Display chart with selected cutoff\n",
    "if cutoff_analysis:\n",
    "    chart_builder = ChartBuilder()\n",
    "    display_figure(chart_builder.cutoff_selection_chart(\n",
    "        cutoff_analysis, \n",
    "        suggested_cutoff=selected_cutoff,\n",
    "        current_cutoff=registry_cutoff\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# pit_registry already initialized in cutoff analysis cell\ndataset_name = Path(DATA_PATH).stem\n\n# Use the user's selected cutoff (not forced by registry)\ncutoff_date = selected_cutoff\n\n# Warn if overriding registry\nif registry_cutoff and registry_cutoff.date() != selected_cutoff.date():\n    console.start_section()\n    console.header(\"Registry Update\")\n    console.warning(f\"Overriding registry cutoff ({registry_cutoff.date()}) with {selected_cutoff.date()}\")\n    console.info(\"All datasets in this project should use the same cutoff date\")\n    console.end_section()\n\npreparer = UnifiedDataPreparer(OUTPUT_DIR, ts_config)\ndf = preparer.prepare_from_raw(raw_df, target_column=TARGET_COLUMN, entity_column=entity_column or \"entity_id\")\n\n# Use the same last_action_series from cutoff analysis for snapshot splitting\n# (do NOT re-derive on prepared df, which has extra timestamp columns)\nsnapshot_df, snapshot_metadata = preparer.create_training_snapshot(\n    df, cutoff_date, timestamp_series=last_action_series\n)\n\npit_registry.register_snapshot(\n    dataset_name=dataset_name,\n    snapshot_id=snapshot_metadata['snapshot_id'],\n    cutoff_date=cutoff_date,\n    source_path=DATA_PATH,\n    row_count=snapshot_metadata['row_count']\n)\n\nconsole.start_section()\nconsole.header(\"Point-in-Time Snapshot Created\")\nconsole.metric(\"Dataset\", dataset_name)\nconsole.metric(\"Snapshot ID\", snapshot_metadata['snapshot_id'])\nconsole.metric(\"Rows\", f\"{snapshot_metadata['row_count']:,}\")\nconsole.metric(\"Features\", len(snapshot_metadata['feature_columns']))\nconsole.metric(\"Cutoff Date\", str(cutoff_date.date()))\nconsole.metric(\"Data Hash\", snapshot_metadata['data_hash'][:16] + \"...\")\n\n# Sanity check: snapshot size should be consistent with cutoff analysis\nif cutoff_analysis:\n    expected_split = cutoff_analysis.get_split_at_date(cutoff_date)\n    expected_train = expected_split['train_count']\n    actual_ratio = snapshot_metadata['row_count'] / len(df) * 100\n    console.metric(\"Split Ratio\", f\"{actual_ratio:.0f}% train / {100 - actual_ratio:.0f}% score\")\n    if abs(actual_ratio - expected_split['train_pct']) > 10:\n        console.error(\n            f\"SPLIT MISMATCH: snapshot has {snapshot_metadata['row_count']:,} rows \"\n            f\"({actual_ratio:.0f}%) but analysis expected {expected_train:,} ({expected_split['train_pct']:.0f}%)\"\n        )\n\nif \"feature_timestamp\" in df.columns:\n    console.success(\"Temporal columns added: feature_timestamp, label_timestamp\")\nelse:\n    console.warning(\"No temporal columns added (synthetic strategy)\")\n\nupdated_report = pit_registry.check_consistency()\nif updated_report.is_consistent:\n    console.success(f\"All {len(pit_registry.snapshots)} datasets use cutoff: {cutoff_date.date()}\")\nelse:\n    console.error(\"INCONSISTENT CUTOFF DATES DETECTED\")\n    console.warning(f\"Out of sync: {', '.join(updated_report.inconsistent_datasets)}\")\n    console.info(\"Re-run notebook 01 for out-of-sync datasets to align cutoff dates\")\n\nconsole.end_section()\n\ndf = snapshot_df"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 1.3 Dataset Exploration\n",
    "\n",
    "Now we explore the **snapshot data** (not raw data). This ensures all visualizations and metrics reflect the actual training data with temporal integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the snapshot data\n",
    "# Note: UnifiedDataPreparer renames the target column to \"target\" in the snapshot\n",
    "# So we use \"target\" as the hint, not the original TARGET_COLUMN name\n",
    "explorer = DataExplorer(visualize=False, save_findings=True, output_dir=str(OUTPUT_DIR))\n",
    "findings = explorer.explore(df, target_hint=\"target\", name=dataset_name)\n",
    "findings.source_path = DATA_PATH\n",
    "\n",
    "# Store snapshot info in findings\n",
    "findings.snapshot_id = snapshot_metadata['snapshot_id']\n",
    "findings.snapshot_path = str(OUTPUT_DIR / \"snapshots\" / f\"{snapshot_metadata['snapshot_id']}.parquet\")\n",
    "findings.timestamp_scenario = scenario\n",
    "findings.timestamp_strategy = ts_config.strategy.value\n",
    "\n",
    "# Also store the original target column name for reference\n",
    "findings.metadata[\"original_target_column\"] = TARGET_COLUMN\n",
    "\n",
    "granularity = \"event\" if granularity_result.granularity == DatasetGranularity.EVENT_LEVEL else \"entity\"\n",
    "\n",
    "# Display dataset overview\n",
    "chart_builder = ChartBuilder()\n",
    "display_figure(chart_builder.dataset_at_a_glance(\n",
    "    df, findings,\n",
    "    source_path=f\"Snapshot: {snapshot_metadata['snapshot_id']}\",\n",
    "    granularity=granularity,\n",
    "    max_columns=15,\n",
    "    columns_per_row=5\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 1.4 Column Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude temporal metadata columns from summary\n",
    "TEMPORAL_METADATA_COLS = {\"feature_timestamp\", \"label_timestamp\", \"label_available_flag\"}\n",
    "\n",
    "summary_data = []\n",
    "for name, col in findings.columns.items():\n",
    "    if name in TEMPORAL_METADATA_COLS:\n",
    "        continue\n",
    "    null_pct = col.universal_metrics.get(\"null_percentage\", 0)\n",
    "    distinct = col.universal_metrics.get(\"distinct_count\", \"N/A\")\n",
    "    summary_data.append({\n",
    "        \"Column\": name,\n",
    "        \"Type\": col.inferred_type.value,\n",
    "        \"Confidence\": f\"{col.confidence:.0%}\",\n",
    "        \"Nulls %\": f\"{null_pct:.1f}%\",\n",
    "        \"Distinct\": distinct,\n",
    "        \"Evidence\": col.evidence[0] if col.evidence else \"\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display_table(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 1.5 Target Column Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.start_section()\n",
    "console.header(\"Target Column\")\n",
    "\n",
    "if findings.target_column and findings.target_column in df.columns:\n",
    "    console.success(f\"Target: {findings.target_column}\")\n",
    "    target_counts = df[findings.target_column].value_counts()\n",
    "    for val, count in target_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        console.metric(f\"Class {val}\", f\"{count:,} ({pct:.1f}%)\")\n",
    "else:\n",
    "    console.warning(\"No target column configured\")\n",
    "    console.info(\"Set TARGET_COLUMN in the configuration cell above\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 1.6 Dataset Structure Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_detector = TimeSeriesDetector()\n",
    "ts_characteristics = ts_detector.detect(df, entity_column=entity_column)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Dataset Structure\")\n",
    "console.metric(\"Type\", ts_characteristics.dataset_type.value.upper())\n",
    "console.metric(\"Granularity\", granularity_result.granularity.value.upper())\n",
    "console.metric(\"Entity Column\", entity_column or \"N/A\")\n",
    "\n",
    "if granularity_result.unique_entities:\n",
    "    console.metric(\"Unique Entities\", f\"{granularity_result.unique_entities:,}\")\n",
    "if granularity_result.avg_events_per_entity:\n",
    "    console.metric(\"Avg Events/Entity\", f\"{granularity_result.avg_events_per_entity:.1f}\")\n",
    "\n",
    "is_event_level = granularity_result.granularity == DatasetGranularity.EVENT_LEVEL\n",
    "if is_event_level:\n",
    "    console.info(\"EVENT-LEVEL DATA - Use Event Bronze Track:\")\n",
    "    console.info(\"  -> 01a_temporal_deep_dive.ipynb\")\n",
    "    console.info(\"  -> 01b_temporal_quality.ipynb\")\n",
    "    console.info(\"  -> 01c_temporal_patterns.ipynb\")\n",
    "    console.info(\"  -> 01d_event_aggregation.ipynb\")\n",
    "else:\n",
    "    console.info(\"ENTITY-LEVEL DATA - Use standard flow:\")\n",
    "    console.info(\"  -> 02_column_deep_dive.ipynb\")\n",
    "    console.info(\"  -> 03_quality_assessment.ipynb\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 1.7 Type Override (Optional)\n",
    "\n",
    "Override any incorrectly inferred column types before saving findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TYPE OVERRIDES ===\n",
    "TYPE_OVERRIDES = {\n",
    "    # \"column_name\": ColumnType.NEW_TYPE,\n",
    "}\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Type Override Review\")\n",
    "\n",
    "low_conf = [(name, col.inferred_type.value, col.confidence) \n",
    "            for name, col in findings.columns.items() \n",
    "            if col.confidence < 0.8 and name not in TEMPORAL_METADATA_COLS]\n",
    "if low_conf:\n",
    "    console.subheader(\"Low Confidence Detections\")\n",
    "    for col_name, col_type, conf in sorted(low_conf, key=lambda x: x[2]):\n",
    "        console.warning(f\"{col_name}: {col_type} ({conf:.0%})\")\n",
    "else:\n",
    "    console.success(\"All type detections have high confidence (>=80%)\")\n",
    "\n",
    "if TYPE_OVERRIDES:\n",
    "    console.subheader(\"Applying Overrides\")\n",
    "    for col_name, new_type in TYPE_OVERRIDES.items():\n",
    "        if col_name in findings.columns:\n",
    "            old_type = findings.columns[col_name].inferred_type.value\n",
    "            findings.columns[col_name].inferred_type = new_type\n",
    "            findings.columns[col_name].confidence = 1.0\n",
    "            console.success(f\"{col_name}: {old_type} -> {new_type.value}\")\n",
    "\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 1.8 Save Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate time series metadata if event-level\n",
    "if is_event_level:\n",
    "    findings.time_series_metadata = TimeSeriesMetadata(\n",
    "        granularity=DatasetGranularity.EVENT_LEVEL,\n",
    "        entity_column=entity_column,\n",
    "        time_column=granularity_result.time_column or ts_characteristics.timestamp_column,\n",
    "        avg_events_per_entity=granularity_result.avg_events_per_entity,\n",
    "        time_span_days=int(ts_characteristics.time_span_days) if ts_characteristics.time_span_days else None,\n",
    "        unique_entities=granularity_result.unique_entities,\n",
    "        suggested_aggregations=[\"24h\", \"7d\", \"30d\", \"90d\", \"all_time\"]\n",
    "    )\n",
    "\n",
    "FINDINGS_PATH = explorer.last_findings_path\n",
    "findings.save(FINDINGS_PATH)\n",
    "\n",
    "console.start_section()\n",
    "console.header(\"Findings Saved\")\n",
    "console.success(f\"Findings: {FINDINGS_PATH}\")\n",
    "console.success(f\"Snapshot: {findings.snapshot_path}\")\n",
    "console.metric(\"Columns\", findings.column_count)\n",
    "console.metric(\"Target\", findings.target_column or \"Not set\")\n",
    "console.metric(\"Snapshot ID\", findings.snapshot_id)\n",
    "console.metric(\"Timestamp Strategy\", findings.timestamp_strategy)\n",
    "console.end_section()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 1.9 Summary\n",
    "\n",
    "**What was created:**\n",
    "- Point-in-time snapshot with `feature_timestamp` and `label_timestamp`\n",
    "- Exploration findings with column types and metrics\n",
    "\n",
    "**All downstream notebooks load the snapshot**, ensuring:\n",
    "- Temporal integrity (no data leakage)\n",
    "- Reproducibility (SHA256 hash verification)\n",
    "- Consistency (same data across all analysis)\n",
    "\n",
    "**Next steps:**\n",
    "- Entity-level data: `02_column_deep_dive.ipynb`\n",
    "- Event-level data: `01a_temporal_deep_dive.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}