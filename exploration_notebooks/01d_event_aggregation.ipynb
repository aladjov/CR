{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 1d: Event Aggregation (Event Bronze Track \u2192 Entity Bronze Track)\n",
    "\n",
    "**Purpose:** Aggregate event-level data to entity-level, applying all insights from 01a-01c.\n",
    "\n",
    "**When to use this notebook:**\n",
    "- After completing 01a (temporal profiling), 01b (quality checks), 01c (pattern analysis)\n",
    "- Your dataset is EVENT_LEVEL granularity\n",
    "- You want to create entity-level features informed by temporal patterns\n",
    "\n",
    "**What this notebook produces:**\n",
    "- Aggregated parquet file (one row per entity)\n",
    "- New findings file for the aggregated data\n",
    "- Updated original findings with aggregation metadata\n",
    "\n",
    "**How 01a-01c findings inform aggregation:**\n",
    "\n",
    "| Source | Insight Applied |\n",
    "|--------|----------------|\n",
    "| **01a** | Recommended windows (e.g., 180d, 365d), lifecycle quadrant feature |\n",
    "| **01b** | Quality issues to handle (gaps, duplicates) |\n",
    "| **01c** | Divergent columns for velocity/momentum (prioritize these features) |\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Shape Transformation\n",
    "\n",
    "```\n",
    "EVENT-LEVEL (input)              ENTITY-LEVEL (output)\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 customer \u2502 date     \u2502          \u2502 customer \u2502 events_180d \u2502 quadrant \u2502 ...\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2192     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 A        \u2502 Jan 1    \u2502          \u2502 A        \u2502 12          \u2502 Steady   \u2502\n",
    "\u2502 A        \u2502 Jan 5    \u2502          \u2502 B        \u2502 5           \u2502 Brief    \u2502\n",
    "\u2502 A        \u2502 Jan 10   \u2502          \u2502 C        \u2502 2           \u2502 Loyal    \u2502\n",
    "\u2502 B        \u2502 Jan 3    \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u2502 ...      \u2502 ...      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "Many rows per entity           One row per entity + lifecycle features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1d.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings, DataExplorer\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType, DatasetGranularity\n",
    "from customer_retention.stages.profiling import (\n",
    "    TimeWindowAggregator,\n",
    "    TimeSeriesProfiler,\n",
    "    classify_lifecycle_quadrants,\n",
    "    classify_activity_segments,\n",
    ")\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from customer_retention.core.config.experiments import FINDINGS_DIR, EXPERIMENTS_DIR, OUTPUT_DIR, setup_experiments_structure\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ../experiments/findings/customer_emails_408768_findings.yaml\n",
      "Loaded findings for 16 columns from ../tests/fixtures/customer_emails.csv\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURATION ===\n",
    "# FINDINGS_DIR imported from customer_retention.core.config.experiments\n",
    "\n",
    "# Find findings files (exclude multi_dataset and already-aggregated)\n",
    "findings_files = [\n",
    "    f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") \n",
    "    if \"multi_dataset\" not in f.name and \"_aggregated\" not in f.name\n",
    "]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Loaded findings for {findings.column_count} columns from {findings.source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINDINGS SUMMARY FROM 01a-01c\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udcca FROM 01a (Temporal Profiling):\n",
      "   Entity column: customer_id\n",
      "   Time column: sent_date\n",
      "   Unique entities: 4,998\n",
      "   Avg events/entity: 15.0\n",
      "   Time span: 2,825 days\n",
      "\n",
      "   \u2705 Recommended windows: ['180d', '365d', 'all_time']\n",
      "\n",
      "   \ud83d\udccb Segmentation recommendation:\n",
      "      Add lifecycle_quadrant as a categorical feature to the model\n",
      "      Heterogeneity: high\n",
      "\n",
      "   \u26a0\ufe0f Drift risk: HIGH\n",
      "      Volume drift: declining\n",
      "      Population stability: 0.66\n",
      "\n",
      "\ud83d\udccb FROM 01b (Temporal Quality):\n",
      "   Quality score: 96.3\n",
      "   Quality grade: A\n",
      "   \u26a0\ufe0f Duplicate events: 371\n",
      "\n",
      "\ud83d\udcc8 FROM 01c (Temporal Patterns):\n",
      "   Windows analyzed: ['180d', '365d', 'all_time']\n",
      "   Velocity window: 180 days\n",
      "   Momentum pairs: [[180, 365]]\n",
      "\n",
      "   Trend: stable (strength: 0.47)\n",
      "   Seasonality: weekly (7), None (21), None (14)\n",
      "   Recency: median=246 days, target_corr=0.77\n",
      "\n",
      "   \ud83c\udfaf Divergent velocity columns: ['time_to_open_hours', 'send_hour']\n",
      "   \ud83c\udfaf Divergent momentum columns: ['target', 'opened', 'clicked']\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify this is event-level data and display findings summary\n",
    "if not findings.is_time_series:\n",
    "    print(\"\u26a0\ufe0f This dataset is NOT event-level. Aggregation not needed.\")\n",
    "    print(\"   Proceed directly to 02_column_deep_dive.ipynb\")\n",
    "    raise SystemExit(\"Skipping aggregation - data is already entity-level\")\n",
    "\n",
    "ts_meta = findings.time_series_metadata\n",
    "ENTITY_COLUMN = ts_meta.entity_column\n",
    "TIME_COLUMN = ts_meta.time_column\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINDINGS SUMMARY FROM 01a-01c\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# === 01a: Time Series Metadata ===\n",
    "print(\"\\n\ud83d\udcca FROM 01a (Temporal Profiling):\")\n",
    "print(f\"   Entity column: {ENTITY_COLUMN}\")\n",
    "print(f\"   Time column: {TIME_COLUMN}\")\n",
    "if ts_meta.unique_entities:\n",
    "    print(f\"   Unique entities: {ts_meta.unique_entities:,}\")\n",
    "if ts_meta.avg_events_per_entity:\n",
    "    print(f\"   Avg events/entity: {ts_meta.avg_events_per_entity:.1f}\")\n",
    "if ts_meta.time_span_days:\n",
    "    print(f\"   Time span: {ts_meta.time_span_days:,} days\")\n",
    "\n",
    "if ts_meta.suggested_aggregations:\n",
    "    print(f\"\\n   \u2705 Recommended windows: {ts_meta.suggested_aggregations}\")\n",
    "else:\n",
    "    print(\"\\n   \u26a0\ufe0f No window recommendations - will use defaults\")\n",
    "\n",
    "if ts_meta.temporal_segmentation_recommendation:\n",
    "    print(f\"\\n   \ud83d\udccb Segmentation recommendation:\")\n",
    "    print(f\"      {ts_meta.temporal_segmentation_recommendation}\")\n",
    "    if ts_meta.heterogeneity_level:\n",
    "        print(f\"      Heterogeneity: {ts_meta.heterogeneity_level}\")\n",
    "\n",
    "if ts_meta.drift_risk_level:\n",
    "    print(f\"\\n   \u26a0\ufe0f Drift risk: {ts_meta.drift_risk_level.upper()}\")\n",
    "    if ts_meta.volume_drift_risk:\n",
    "        print(f\"      Volume drift: {ts_meta.volume_drift_risk}\")\n",
    "    if ts_meta.population_stability is not None:\n",
    "        print(f\"      Population stability: {ts_meta.population_stability:.2f}\")\n",
    "\n",
    "# === 01b: Temporal Quality ===\n",
    "quality_meta = findings.metadata.get(\"temporal_quality\", {})\n",
    "if quality_meta:\n",
    "    print(f\"\\n\ud83d\udccb FROM 01b (Temporal Quality):\")\n",
    "    if quality_meta.get(\"temporal_quality_score\"):\n",
    "        print(f\"   Quality score: {quality_meta.get('temporal_quality_score'):.1f}\")\n",
    "    if quality_meta.get(\"temporal_quality_grade\"):\n",
    "        print(f\"   Quality grade: {quality_meta.get('temporal_quality_grade')}\")\n",
    "    issues = quality_meta.get(\"issues\", {})\n",
    "    if issues.get(\"duplicate_events\", 0) > 0:\n",
    "        print(f\"   \u26a0\ufe0f Duplicate events: {issues['duplicate_events']:,}\")\n",
    "    if issues.get(\"temporal_gaps\", 0) > 0:\n",
    "        print(f\"   \u26a0\ufe0f Temporal gaps: {issues['temporal_gaps']:,}\")\n",
    "\n",
    "# === 01c: Temporal Patterns ===\n",
    "pattern_meta = findings.metadata.get(\"temporal_patterns\", {})\n",
    "if pattern_meta:\n",
    "    print(f\"\\n\ud83d\udcc8 FROM 01c (Temporal Patterns):\")\n",
    "    windows_used = pattern_meta.get(\"windows_used\", {})\n",
    "    if windows_used:\n",
    "        if windows_used.get(\"aggregation_windows\"):\n",
    "            print(f\"   Windows analyzed: {windows_used.get('aggregation_windows')}\")\n",
    "        if windows_used.get(\"velocity_window\"):\n",
    "            print(f\"   Velocity window: {windows_used.get('velocity_window')} days\")\n",
    "        if windows_used.get(\"momentum_pairs\"):\n",
    "            print(f\"   Momentum pairs: {windows_used.get('momentum_pairs')}\")\n",
    "    \n",
    "    trend = pattern_meta.get(\"trend\", {})\n",
    "    if trend and trend.get(\"direction\"):\n",
    "        print(f\"\\n   Trend: {trend.get('direction')} (strength: {trend.get('strength', 0):.2f})\")\n",
    "    \n",
    "    seasonality = pattern_meta.get(\"seasonality\", [])\n",
    "    if seasonality:\n",
    "        periods = [f\"{s.get('name', 'period')} ({s.get('period')})\" for s in seasonality[:3]]\n",
    "        print(f\"   Seasonality: {', '.join(periods)}\")\n",
    "    \n",
    "    recency = pattern_meta.get(\"recency\", {})\n",
    "    if recency and recency.get(\"median_days\"):\n",
    "        print(f\"   Recency: median={recency.get('median_days'):.0f} days, \"\n",
    "              f\"target_corr={recency.get('target_correlation', 0):.2f}\")\n",
    "    \n",
    "    # Divergent columns (important for feature prioritization)\n",
    "    velocity = pattern_meta.get(\"velocity\", {})\n",
    "    divergent_velocity = [k for k, v in velocity.items() if isinstance(v, dict) and v.get(\"divergent\")]\n",
    "    if divergent_velocity:\n",
    "        print(f\"\\n   \ud83c\udfaf Divergent velocity columns: {divergent_velocity}\")\n",
    "    \n",
    "    momentum = pattern_meta.get(\"momentum\", {})\n",
    "    divergent_momentum = momentum.get(\"_divergent_columns\", [])\n",
    "    if divergent_momentum:\n",
    "        print(f\"   \ud83c\udfaf Divergent momentum columns: {divergent_momentum}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 74,842 events x 16 columns\n",
      "Data source: snapshot\n",
      "Date range: 2015-01-01 00:00:00 to 2022-09-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n",
    "\n",
    "# Load source data (prefers snapshots over raw files)\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=str(FINDINGS_DIR))\n",
    "df[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN])\n",
    "charts = ChartBuilder()\n",
    "\n",
    "print(f\"Loaded {len(df):,} events x {len(df.columns)} columns\")\n",
    "print(f\"Data source: {data_source}\")\n",
    "print(f\"Date range: {df[TIME_COLUMN].min()} to {df[TIME_COLUMN].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1d.2 Configure Aggregation Based on Findings\n",
    "\n",
    "Apply all insights from 01a-01c to configure optimal aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AGGREGATION CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "Windows: ['180d', '365d', 'all_time']\n",
      "   Source: 01a recommendations\n",
      "\n",
      "Reference date: 2022-09-26 00:00:00\n",
      "\n",
      "Value columns (5 total):\n",
      "   Priority (divergent): ['time_to_open_hours', 'send_hour', 'opened', 'clicked']\n",
      "   Other: ['bounced']\n",
      "\n",
      "   \u26a0\ufe0f Excluded from aggregation: target (target - prevents leakage)\n",
      "\n",
      "Aggregation functions: ['sum', 'mean', 'max', 'count']\n",
      "\n",
      "Additional features:\n",
      "   Include lifecycle_quadrant: True\n",
      "   Include recency: True\n",
      "   Include tenure: True\n"
     ]
    }
   ],
   "source": [
    "# === AGGREGATION CONFIGURATION ===\n",
    "# Windows are loaded from findings (01a recommendations) with option to override\n",
    "\n",
    "# Manual override (set to None to use findings recommendations)\n",
    "WINDOW_OVERRIDE = None  # e.g., [\"7d\", \"30d\", \"90d\"] to override\n",
    "\n",
    "# Get windows from findings or use defaults\n",
    "if WINDOW_OVERRIDE:\n",
    "    WINDOWS = WINDOW_OVERRIDE\n",
    "    window_source = \"manual override\"\n",
    "elif ts_meta.suggested_aggregations:\n",
    "    WINDOWS = ts_meta.suggested_aggregations\n",
    "    window_source = \"01a recommendations\"\n",
    "else:\n",
    "    WINDOWS = [\"7d\", \"30d\", \"90d\", \"180d\", \"365d\", \"all_time\"]\n",
    "    window_source = \"defaults (no findings)\"\n",
    "\n",
    "# Reference date for window calculations\n",
    "REFERENCE_DATE = df[TIME_COLUMN].max()\n",
    "\n",
    "# Extract pattern metadata for feature prioritization\n",
    "pattern_meta = findings.metadata.get(\"temporal_patterns\", {})\n",
    "velocity_meta = pattern_meta.get(\"velocity\", {})\n",
    "momentum_meta = pattern_meta.get(\"momentum\", {})\n",
    "\n",
    "# Identify divergent columns (these are most predictive for target)\n",
    "DIVERGENT_VELOCITY_COLS = [k for k, v in velocity_meta.items() \n",
    "                           if isinstance(v, dict) and v.get(\"divergent\")]\n",
    "DIVERGENT_MOMENTUM_COLS = momentum_meta.get(\"_divergent_columns\", [])\n",
    "\n",
    "# Value columns: prioritize divergent columns, then other numerics\n",
    "# IMPORTANT: Exclude target column and temporal metadata to prevent data leakage!\n",
    "TARGET_COLUMN = findings.target_column\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = {ENTITY_COLUMN, TIME_COLUMN} | set(TEMPORAL_METADATA_COLS)\n",
    "if TARGET_COLUMN:\n",
    "    exclude_cols.add(TARGET_COLUMN)\n",
    "available_numeric = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "# Put divergent columns first (they showed predictive signal in 01c)\n",
    "priority_cols = [c for c in DIVERGENT_VELOCITY_COLS + DIVERGENT_MOMENTUM_COLS \n",
    "                 if c in available_numeric]\n",
    "other_cols = [c for c in available_numeric if c not in priority_cols]\n",
    "VALUE_COLUMNS = priority_cols + other_cols\n",
    "\n",
    "# Aggregation functions\n",
    "AGG_FUNCTIONS = [\"sum\", \"mean\", \"max\", \"count\"]\n",
    "\n",
    "# Lifecycle features (recommended by 01a segmentation analysis)\n",
    "INCLUDE_LIFECYCLE_QUADRANT = ts_meta.temporal_segmentation_recommendation is not None\n",
    "INCLUDE_RECENCY = True\n",
    "INCLUDE_TENURE = True\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"AGGREGATION CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nWindows: {WINDOWS}\")\n",
    "print(f\"   Source: {window_source}\")\n",
    "print(f\"\\nReference date: {REFERENCE_DATE}\")\n",
    "print(f\"\\nValue columns ({len(VALUE_COLUMNS)} total):\")\n",
    "if priority_cols:\n",
    "    print(f\"   Priority (divergent): {priority_cols}\")\n",
    "print(f\"   Other: {other_cols[:5]}{'...' if len(other_cols) > 5 else ''}\")\n",
    "if TARGET_COLUMN:\n",
    "    print(f\"\\n   \u26a0\ufe0f Excluded from aggregation: {TARGET_COLUMN} (target - prevents leakage)\")\n",
    "print(f\"\\nAggregation functions: {AGG_FUNCTIONS}\")\n",
    "print(f\"\\nAdditional features:\")\n",
    "print(f\"   Include lifecycle_quadrant: {INCLUDE_LIFECYCLE_QUADRANT}\")\n",
    "print(f\"   Include recency: {INCLUDE_RECENCY}\")\n",
    "print(f\"   Include tenure: {INCLUDE_TENURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 1d.3 Preview Aggregation Plan\n",
    "\n",
    "See what features will be created before executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AGGREGATION PLAN\n",
      "============================================================\n",
      "\n",
      "Entity column: customer_id\n",
      "Time column: sent_date\n",
      "Windows: ['180d', '365d', 'all_time']\n",
      "\n",
      "Features from aggregation (65):\n",
      "   - event_count_180d\n",
      "   - event_count_365d\n",
      "   - event_count_all_time\n",
      "   - time_to_open_hours_sum_180d \ud83c\udfaf\n",
      "   - time_to_open_hours_mean_180d \ud83c\udfaf\n",
      "   - time_to_open_hours_max_180d \ud83c\udfaf\n",
      "   - time_to_open_hours_count_180d \ud83c\udfaf\n",
      "   - send_hour_sum_180d \ud83c\udfaf\n",
      "   - send_hour_mean_180d \ud83c\udfaf\n",
      "   - send_hour_max_180d \ud83c\udfaf\n",
      "   - send_hour_count_180d \ud83c\udfaf\n",
      "   - opened_sum_180d \ud83c\udfaf\n",
      "   - opened_mean_180d \ud83c\udfaf\n",
      "   - opened_max_180d \ud83c\udfaf\n",
      "   - opened_count_180d \ud83c\udfaf\n",
      "   ... and 50 more\n",
      "\n",
      "Additional features:\n",
      "   - lifecycle_quadrant\n",
      "   - target (entity target)\n",
      "\n",
      "Total expected features: 68\n"
     ]
    }
   ],
   "source": [
    "# Initialize aggregator\n",
    "aggregator = TimeWindowAggregator(\n",
    "    entity_column=ENTITY_COLUMN,\n",
    "    time_column=TIME_COLUMN\n",
    ")\n",
    "\n",
    "# Generate plan\n",
    "plan = aggregator.generate_plan(\n",
    "    df=df,\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "# Count additional features we'll add\n",
    "additional_features = []\n",
    "if INCLUDE_LIFECYCLE_QUADRANT:\n",
    "    additional_features.append(\"lifecycle_quadrant\")\n",
    "if findings.target_column and findings.target_column in df.columns:\n",
    "    additional_features.append(f\"{findings.target_column} (entity target)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEntity column: {plan.entity_column}\")\n",
    "print(f\"Time column: {plan.time_column}\")\n",
    "print(f\"Windows: {[w.name for w in plan.windows]}\")\n",
    "\n",
    "print(f\"\\nFeatures from aggregation ({len(plan.feature_columns)}):\")\n",
    "for feat in plan.feature_columns[:15]:\n",
    "    # Highlight divergent column features\n",
    "    is_priority = any(dc in feat for dc in priority_cols) if priority_cols else False\n",
    "    marker = \" \ud83c\udfaf\" if is_priority else \"\"\n",
    "    print(f\"   - {feat}{marker}\")\n",
    "if len(plan.feature_columns) > 15:\n",
    "    print(f\"   ... and {len(plan.feature_columns) - 15} more\")\n",
    "\n",
    "if additional_features:\n",
    "    print(f\"\\nAdditional features:\")\n",
    "    for feat in additional_features:\n",
    "        print(f\"   - {feat}\")\n",
    "    \n",
    "print(f\"\\nTotal expected features: {len(plan.feature_columns) + len(additional_features) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 1d.4 Execute Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing aggregation...\n",
      "   Input: 74,842 events\n",
      "   Expected output: 4,998 entities\n",
      "\n",
      "   Adding lifecycle_quadrant feature...\n",
      "   Quadrant distribution:\n",
      "      Occasional & Loyal: 1,631 (32.6%)\n",
      "      Intense & Brief: 1,627 (32.6%)\n",
      "      Steady & Loyal: 873 (17.5%)\n",
      "      One-shot: 867 (17.3%)\n",
      "\n",
      "   Adding entity-level target (target)...\n",
      "      target=0: 3,025 (60.5%)\n",
      "      target=1: 1,973 (39.5%)\n",
      "\n",
      "\u2705 Aggregation complete!\n",
      "   Output: 4,998 entities x 68 features\n",
      "   Memory: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Executing aggregation...\")\n",
    "print(f\"   Input: {len(df):,} events\")\n",
    "print(f\"   Expected output: {df[ENTITY_COLUMN].nunique():,} entities\")\n",
    "\n",
    "# Step 1: Basic time window aggregation\n",
    "df_aggregated = aggregator.aggregate(\n",
    "    df,\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    reference_date=REFERENCE_DATE,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "# Step 2: Add lifecycle quadrant (from 01a recommendation)\n",
    "if INCLUDE_LIFECYCLE_QUADRANT:\n",
    "    print(\"\\n   Adding lifecycle_quadrant feature...\")\n",
    "    profiler = TimeSeriesProfiler(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN)\n",
    "    ts_profile = profiler.profile(df)\n",
    "    \n",
    "    # Rename 'entity' column to match our entity column name\n",
    "    lifecycles = ts_profile.entity_lifecycles.copy()\n",
    "    lifecycles = lifecycles.rename(columns={\"entity\": ENTITY_COLUMN})\n",
    "    \n",
    "    quadrant_result = classify_lifecycle_quadrants(lifecycles)\n",
    "    \n",
    "    # Merge lifecycle_quadrant into aggregated data\n",
    "    quadrant_map = quadrant_result.lifecycles.set_index(ENTITY_COLUMN)[\"lifecycle_quadrant\"]\n",
    "    df_aggregated[\"lifecycle_quadrant\"] = df_aggregated[ENTITY_COLUMN].map(quadrant_map)\n",
    "    \n",
    "    print(f\"   Quadrant distribution:\")\n",
    "    for quad, count in df_aggregated[\"lifecycle_quadrant\"].value_counts().items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"      {quad}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Step 3: Add entity-level target (if available)\n",
    "TARGET_COLUMN = findings.target_column\n",
    "if TARGET_COLUMN and TARGET_COLUMN in df.columns:\n",
    "    print(f\"\\n   Adding entity-level target ({TARGET_COLUMN})...\")\n",
    "    # For entity-level target, use max (if any event has target=1, entity has target=1)\n",
    "    entity_target = df.groupby(ENTITY_COLUMN)[TARGET_COLUMN].max()\n",
    "    df_aggregated[TARGET_COLUMN] = df_aggregated[ENTITY_COLUMN].map(entity_target)\n",
    "    \n",
    "    target_dist = df_aggregated[TARGET_COLUMN].value_counts()\n",
    "    for val, count in target_dist.items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"      {TARGET_COLUMN}={val}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\u2705 Aggregation complete!\")\n",
    "print(f\"   Output: {len(df_aggregated):,} entities x {len(df_aggregated.columns)} features\")\n",
    "print(f\"   Memory: {df_aggregated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>event_count_180d</th>\n",
       "      <th>event_count_365d</th>\n",
       "      <th>event_count_all_time</th>\n",
       "      <th>time_to_open_hours_sum_180d</th>\n",
       "      <th>time_to_open_hours_mean_180d</th>\n",
       "      <th>time_to_open_hours_max_180d</th>\n",
       "      <th>time_to_open_hours_count_180d</th>\n",
       "      <th>send_hour_sum_180d</th>\n",
       "      <th>send_hour_mean_180d</th>\n",
       "      <th>...</th>\n",
       "      <th>clicked_max_all_time</th>\n",
       "      <th>clicked_count_all_time</th>\n",
       "      <th>bounced_sum_all_time</th>\n",
       "      <th>bounced_mean_all_time</th>\n",
       "      <th>bounced_max_all_time</th>\n",
       "      <th>bounced_count_all_time</th>\n",
       "      <th>days_since_last_event</th>\n",
       "      <th>days_since_first_event</th>\n",
       "      <th>lifecycle_quadrant</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6A2E47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1836</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58D29E</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>477</td>\n",
       "      <td>2825</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3DA827</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>235</td>\n",
       "      <td>2825</td>\n",
       "      <td>Steady &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6897C2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2824</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACCAF7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>289</td>\n",
       "      <td>2825</td>\n",
       "      <td>Steady &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7F0800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1557</td>\n",
       "      <td>2825</td>\n",
       "      <td>One-shot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22507F</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>42</td>\n",
       "      <td>2825</td>\n",
       "      <td>Steady &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CFBB70</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>241</td>\n",
       "      <td>2825</td>\n",
       "      <td>Occasional &amp; Loyal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>307116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2597</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>168A39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2434</td>\n",
       "      <td>2825</td>\n",
       "      <td>Intense &amp; Brief</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows \u00d7 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  event_count_180d  event_count_365d  event_count_all_time  \\\n",
       "0      6A2E47                 0                 0                    32   \n",
       "1      58D29E                 0                 0                    15   \n",
       "2      3DA827                 0                 2                    21   \n",
       "3      6897C2                 0                 0                     8   \n",
       "4      ACCAF7                 0                 2                    18   \n",
       "5      7F0800                 0                 0                     7   \n",
       "6      22507F                 4                 5                    31   \n",
       "7      CFBB70                 0                 2                    14   \n",
       "8      307116                 0                 0                     4   \n",
       "9      168A39                 0                 0                     9   \n",
       "\n",
       "   time_to_open_hours_sum_180d  time_to_open_hours_mean_180d  \\\n",
       "0                          0.0                           NaN   \n",
       "1                          0.0                           NaN   \n",
       "2                          0.0                           NaN   \n",
       "3                          0.0                           NaN   \n",
       "4                          0.0                           NaN   \n",
       "5                          0.0                           NaN   \n",
       "6                          0.0                           NaN   \n",
       "7                          0.0                           NaN   \n",
       "8                          0.0                           NaN   \n",
       "9                          0.0                           NaN   \n",
       "\n",
       "   time_to_open_hours_max_180d  time_to_open_hours_count_180d  \\\n",
       "0                          NaN                              0   \n",
       "1                          NaN                              0   \n",
       "2                          NaN                              0   \n",
       "3                          NaN                              0   \n",
       "4                          NaN                              0   \n",
       "5                          NaN                              0   \n",
       "6                          NaN                              0   \n",
       "7                          NaN                              0   \n",
       "8                          NaN                              0   \n",
       "9                          NaN                              0   \n",
       "\n",
       "   send_hour_sum_180d  send_hour_mean_180d  ...  clicked_max_all_time  \\\n",
       "0                   0                  NaN  ...                     1   \n",
       "1                   0                  NaN  ...                     0   \n",
       "2                   0                  NaN  ...                     1   \n",
       "3                   0                  NaN  ...                     1   \n",
       "4                   0                  NaN  ...                     1   \n",
       "5                   0                  NaN  ...                     0   \n",
       "6                  36                  9.0  ...                     1   \n",
       "7                   0                  NaN  ...                     1   \n",
       "8                   0                  NaN  ...                     0   \n",
       "9                   0                  NaN  ...                     1   \n",
       "\n",
       "   clicked_count_all_time  bounced_sum_all_time  bounced_mean_all_time  \\\n",
       "0                      32                     1               0.031250   \n",
       "1                      15                     1               0.066667   \n",
       "2                      21                     0               0.000000   \n",
       "3                       8                     0               0.000000   \n",
       "4                      18                     1               0.055556   \n",
       "5                       7                     0               0.000000   \n",
       "6                      31                     1               0.032258   \n",
       "7                      14                     0               0.000000   \n",
       "8                       4                     0               0.000000   \n",
       "9                       9                     1               0.111111   \n",
       "\n",
       "   bounced_max_all_time  bounced_count_all_time  days_since_last_event  \\\n",
       "0                     1                      32                   1836   \n",
       "1                     1                      15                    477   \n",
       "2                     0                      21                    235   \n",
       "3                     0                       8                   2824   \n",
       "4                     1                      18                    289   \n",
       "5                     0                       7                   1557   \n",
       "6                     1                      31                     42   \n",
       "7                     0                      14                    241   \n",
       "8                     0                       4                   2597   \n",
       "9                     1                       9                   2434   \n",
       "\n",
       "   days_since_first_event  lifecycle_quadrant  target  \n",
       "0                    2825     Intense & Brief       1  \n",
       "1                    2825            One-shot       0  \n",
       "2                    2825      Steady & Loyal       0  \n",
       "3                    2825     Intense & Brief       1  \n",
       "4                    2825      Steady & Loyal       0  \n",
       "5                    2825            One-shot       1  \n",
       "6                    2825      Steady & Loyal       0  \n",
       "7                    2825  Occasional & Loyal       0  \n",
       "8                    2825     Intense & Brief       1  \n",
       "9                    2825     Intense & Brief       1  \n",
       "\n",
       "[10 rows x 68 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview aggregated data\n",
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(df_aggregated.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>event_count_180d</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.707283</td>\n",
       "      <td>1.041346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_count_365d</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>1.480592</td>\n",
       "      <td>1.732549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_count_all_time</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>14.974390</td>\n",
       "      <td>8.287083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_to_open_hours_sum_180d</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.630692</td>\n",
       "      <td>2.324767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_to_open_hours_mean_180d</th>\n",
       "      <td>684.0</td>\n",
       "      <td>3.992178</td>\n",
       "      <td>3.813826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.975</td>\n",
       "      <td>5.5</td>\n",
       "      <td>26.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bounced_max_all_time</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.268107</td>\n",
       "      <td>0.443018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bounced_count_all_time</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>14.974390</td>\n",
       "      <td>8.287083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_since_last_event</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>665.750100</td>\n",
       "      <td>803.341982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>246.500</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>2824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>days_since_first_event</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>2669.424570</td>\n",
       "      <td>158.136559</td>\n",
       "      <td>1498.0</td>\n",
       "      <td>2603.0</td>\n",
       "      <td>2719.000</td>\n",
       "      <td>2784.0</td>\n",
       "      <td>2825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>4998.0</td>\n",
       "      <td>0.394758</td>\n",
       "      <td>0.488848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows \u00d7 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count         mean         std     min     25%  \\\n",
       "event_count_180d              4998.0     0.707283    1.041346     0.0     0.0   \n",
       "event_count_365d              4998.0     1.480592    1.732549     0.0     0.0   \n",
       "event_count_all_time          4998.0    14.974390    8.287083     1.0    11.0   \n",
       "time_to_open_hours_sum_180d   4998.0     0.630692    2.324767     0.0     0.0   \n",
       "time_to_open_hours_mean_180d   684.0     3.992178    3.813826     0.0     1.3   \n",
       "...                              ...          ...         ...     ...     ...   \n",
       "bounced_max_all_time          4998.0     0.268107    0.443018     0.0     0.0   \n",
       "bounced_count_all_time        4998.0    14.974390    8.287083     1.0    11.0   \n",
       "days_since_last_event         4998.0   665.750100  803.341982     0.0    86.0   \n",
       "days_since_first_event        4998.0  2669.424570  158.136559  1498.0  2603.0   \n",
       "target                        4998.0     0.394758    0.488848     0.0     0.0   \n",
       "\n",
       "                                   50%     75%     max  \n",
       "event_count_180d                 0.000     1.0    12.0  \n",
       "event_count_365d                 1.000     2.0    25.0  \n",
       "event_count_all_time            14.000    17.0   106.0  \n",
       "time_to_open_hours_sum_180d      0.000     0.0    29.5  \n",
       "time_to_open_hours_mean_180d     2.975     5.5    26.5  \n",
       "...                                ...     ...     ...  \n",
       "bounced_max_all_time             0.000     1.0     1.0  \n",
       "bounced_count_all_time          14.000    17.0   106.0  \n",
       "days_since_last_event          246.500  1088.0  2824.0  \n",
       "days_since_first_event        2719.000  2784.0  2825.0  \n",
       "target                           0.000     1.0     1.0  \n",
       "\n",
       "[66 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nFeature Summary Statistics:\")\n",
    "display(df_aggregated.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 1d.5 Quality Check on Aggregated Data\n",
    "\n",
    "Quick validation of the aggregated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AGGREGATED DATA QUALITY CHECK\n",
      "============================================================\n",
      "\n",
      "\u26a0\ufe0f Columns with null values (22):\n",
      "   time_to_open_hours_mean_180d: 4,314 (86.3%)\n",
      "   time_to_open_hours_max_180d: 4,314 (86.3%)\n",
      "   send_hour_mean_180d: 2,884 (57.7%)\n",
      "   send_hour_max_180d: 2,884 (57.7%)\n",
      "   opened_mean_180d: 2,884 (57.7%)\n",
      "   opened_max_180d: 2,884 (57.7%)\n",
      "   clicked_mean_180d: 2,884 (57.7%)\n",
      "   clicked_max_180d: 2,884 (57.7%)\n",
      "   bounced_mean_180d: 2,884 (57.7%)\n",
      "   bounced_max_180d: 2,884 (57.7%)\n",
      "   ... and 12 more\n",
      "\n",
      "   Note: Nulls in aggregated features typically mean no events in that window.\n",
      "   Consider filling with 0 for count/sum features.\n",
      "\n",
      "\u2705 Entity count matches: 4,998\n",
      "\n",
      "\ud83d\udcca Feature Statistics:\n",
      "   Total features: 68\n",
      "   Numeric features: 65\n",
      "\n",
      "\ud83d\udcca Lifecycle Quadrant vs Target:\n",
      "   Intense & Brief: 78.2% positive\n",
      "   Occasional & Loyal: 7.7% positive\n",
      "   One-shot: 59.2% positive\n",
      "   Steady & Loyal: 7.1% positive\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AGGREGATED DATA QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for nulls\n",
    "null_counts = df_aggregated.isnull().sum()\n",
    "cols_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "if len(cols_with_nulls) > 0:\n",
    "    print(f\"\\n\u26a0\ufe0f Columns with null values ({len(cols_with_nulls)}):\")\n",
    "    for col, count in cols_with_nulls.head(10).items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
    "    if len(cols_with_nulls) > 10:\n",
    "        print(f\"   ... and {len(cols_with_nulls) - 10} more\")\n",
    "    print(\"\\n   Note: Nulls in aggregated features typically mean no events in that window.\")\n",
    "    print(\"   Consider filling with 0 for count/sum features.\")\n",
    "else:\n",
    "    print(\"\\n\u2705 No null values in aggregated data\")\n",
    "\n",
    "# Check entity count matches\n",
    "original_entities = df[ENTITY_COLUMN].nunique()\n",
    "aggregated_entities = len(df_aggregated)\n",
    "\n",
    "if original_entities == aggregated_entities:\n",
    "    print(f\"\\n\u2705 Entity count matches: {aggregated_entities:,}\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f Entity count mismatch!\")\n",
    "    print(f\"   Original: {original_entities:,}\")\n",
    "    print(f\"   Aggregated: {aggregated_entities:,}\")\n",
    "\n",
    "# Check feature statistics\n",
    "print(f\"\\n\ud83d\udcca Feature Statistics:\")\n",
    "numeric_agg_cols = df_aggregated.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if TARGET_COLUMN:\n",
    "    numeric_agg_cols = [c for c in numeric_agg_cols if c != TARGET_COLUMN]\n",
    "\n",
    "print(f\"   Total features: {len(df_aggregated.columns)}\")\n",
    "print(f\"   Numeric features: {len(numeric_agg_cols)}\")\n",
    "\n",
    "# Check for constant columns (no variance)\n",
    "const_cols = [c for c in numeric_agg_cols if df_aggregated[c].std() == 0]\n",
    "if const_cols:\n",
    "    print(f\"\\n\u26a0\ufe0f Constant columns (zero variance): {len(const_cols)}\")\n",
    "    print(f\"   {const_cols[:5]}{'...' if len(const_cols) > 5 else ''}\")\n",
    "\n",
    "# If lifecycle_quadrant was added, show its correlation with target\n",
    "if INCLUDE_LIFECYCLE_QUADRANT and TARGET_COLUMN and TARGET_COLUMN in df_aggregated.columns:\n",
    "    print(f\"\\n\ud83d\udcca Lifecycle Quadrant vs Target:\")\n",
    "    cross = pd.crosstab(df_aggregated[\"lifecycle_quadrant\"], df_aggregated[TARGET_COLUMN], normalize='index')\n",
    "    if 1 in cross.columns:\n",
    "        for quad in cross.index:\n",
    "            rate = cross.loc[quad, 1] * 100\n",
    "            print(f\"   {quad}: {rate:.1f}% positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 1d.6 Save Aggregated Data and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Aggregated data saved to: ../experiments/findings/customer_emails_408768_aggregated.parquet\n",
      "   Size: 297.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Generate output paths\n",
    "original_name = Path(findings.source_path).stem\n",
    "findings_name = Path(FINDINGS_PATH).stem.replace(\"_findings\", \"\")\n",
    "\n",
    "# Save aggregated data as parquet\n",
    "AGGREGATED_DATA_PATH = FINDINGS_DIR / f\"{findings_name}_aggregated.parquet\"\n",
    "df_aggregated.to_parquet(AGGREGATED_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"\\u2705 Aggregated data saved to: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   Size: {AGGREGATED_DATA_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating findings for aggregated data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: sans-serif; padding: 20px; color: #333;\">\n",
       "            <h2 style=\"color: #222;\">Data Exploration: ../experiments/findings/customer_emails_408768_aggregated.parquet</h2>\n",
       "            <div style=\"display: flex; gap: 20px; margin-bottom: 20px;\">\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Rows</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #222;\">4,998</span>\n",
       "                </div>\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Columns</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #222;\">68</span>\n",
       "                </div>\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Completeness</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #ff7f0e;\">83.2%</span>\n",
       "                </div>\n",
       "                <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
       "                    <h4 style=\"margin: 0 0 8px 0; color: #555;\">Memory</h4>\n",
       "                    <span style=\"font-size: 24px; font-weight: bold; color: #222;\">3.1 MB</span>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings saved to: ../experiments/findings/customer_emails_408768_aggregated_d24886_findings.yaml\n",
      "\u2705 Aggregated findings saved to: ../experiments/findings/customer_emails_408768_aggregated_d24886_findings.yaml\n"
     ]
    }
   ],
   "source": [
    "# Create new findings for aggregated data using DataExplorer\n",
    "print(\"\\nGenerating findings for aggregated data...\")\n",
    "\n",
    "explorer = DataExplorer(output_dir=str(FINDINGS_DIR))\n",
    "aggregated_findings = explorer.explore(\n",
    "    str(AGGREGATED_DATA_PATH),\n",
    "    name=f\"{findings_name}_aggregated\"\n",
    ")\n",
    "\n",
    "AGGREGATED_FINDINGS_PATH = explorer.last_findings_path\n",
    "print(f\"\u2705 Aggregated findings saved to: {AGGREGATED_FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Original findings updated with aggregation metadata: ../experiments/findings/customer_emails_408768_findings.yaml\n"
     ]
    }
   ],
   "source": [
    "# Update original findings with comprehensive aggregation metadata\n",
    "findings.time_series_metadata.aggregation_executed = True\n",
    "findings.time_series_metadata.aggregated_data_path = str(AGGREGATED_DATA_PATH)\n",
    "findings.time_series_metadata.aggregated_findings_path = str(AGGREGATED_FINDINGS_PATH)\n",
    "findings.time_series_metadata.aggregation_windows_used = WINDOWS\n",
    "findings.time_series_metadata.aggregation_timestamp = datetime.now().isoformat()\n",
    "\n",
    "# Add aggregation details to metadata\n",
    "findings.metadata[\"aggregation\"] = {\n",
    "    \"windows_used\": WINDOWS,\n",
    "    \"window_source\": window_source,\n",
    "    \"reference_date\": str(REFERENCE_DATE),\n",
    "    \"value_columns_count\": len(VALUE_COLUMNS),\n",
    "    \"priority_columns\": priority_cols,  # Divergent columns from 01c\n",
    "    \"agg_functions\": AGG_FUNCTIONS,\n",
    "    \"include_lifecycle_quadrant\": INCLUDE_LIFECYCLE_QUADRANT,\n",
    "    \"include_recency\": INCLUDE_RECENCY,\n",
    "    \"include_tenure\": INCLUDE_TENURE,\n",
    "    \"output_entities\": len(df_aggregated),\n",
    "    \"output_features\": len(df_aggregated.columns),\n",
    "    \"target_column\": TARGET_COLUMN,\n",
    "}\n",
    "\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"\u2705 Original findings updated with aggregation metadata: {FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "AGGREGATION COMPLETE - OUTPUT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "\ud83d\udcc1 Files created:\n",
      "   1. Aggregated data: ../experiments/findings/customer_emails_408768_aggregated.parquet\n",
      "   2. Aggregated findings: ../experiments/findings/customer_emails_408768_aggregated_d24886_findings.yaml\n",
      "   3. Updated original findings: ../experiments/findings/customer_emails_408768_findings.yaml\n",
      "\n",
      "\ud83d\udcca Transformation stats:\n",
      "   Input events: 74,842\n",
      "   Output entities: 4,998\n",
      "   Features created: 68\n",
      "\n",
      "\u2699\ufe0f Configuration applied:\n",
      "   Windows: ['180d', '365d', 'all_time'] (from 01a recommendations)\n",
      "   Aggregation functions: ['sum', 'mean', 'max', 'count']\n",
      "   Priority columns (from 01c divergence): ['time_to_open_hours', 'send_hour', 'opened', 'clicked']\n",
      "   Lifecycle quadrant: included (from 01a recommendation)\n",
      "\n",
      "\ud83c\udfaf Ready for modeling:\n",
      "   Entity column: customer_id\n",
      "   Target column: target\n",
      "   Target positive rate: 39.5%\n",
      "\n",
      "\u26a0\ufe0f DRIFT WARNING: High drift risk detected in 01a\n",
      "   Volume drift: declining\n",
      "   Consider: temporal validation splits, monitoring for distribution shift\n"
     ]
    }
   ],
   "source": [
    "# Summary of outputs\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATION COMPLETE - OUTPUT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Files created:\")\n",
    "print(f\"   1. Aggregated data: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   2. Aggregated findings: {AGGREGATED_FINDINGS_PATH}\")\n",
    "print(f\"   3. Updated original findings: {FINDINGS_PATH}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Transformation stats:\")\n",
    "print(f\"   Input events: {len(df):,}\")\n",
    "print(f\"   Output entities: {len(df_aggregated):,}\")\n",
    "print(f\"   Features created: {len(df_aggregated.columns)}\")\n",
    "\n",
    "print(f\"\\n\u2699\ufe0f Configuration applied:\")\n",
    "print(f\"   Windows: {WINDOWS} (from {window_source})\")\n",
    "print(f\"   Aggregation functions: {AGG_FUNCTIONS}\")\n",
    "if priority_cols:\n",
    "    print(f\"   Priority columns (from 01c divergence): {priority_cols}\")\n",
    "if INCLUDE_LIFECYCLE_QUADRANT:\n",
    "    print(f\"   Lifecycle quadrant: included (from 01a recommendation)\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Ready for modeling:\")\n",
    "print(f\"   Entity column: {ENTITY_COLUMN}\")\n",
    "if TARGET_COLUMN:\n",
    "    print(f\"   Target column: {TARGET_COLUMN}\")\n",
    "    if TARGET_COLUMN in df_aggregated.columns:\n",
    "        positive_rate = df_aggregated[TARGET_COLUMN].mean() * 100\n",
    "        print(f\"   Target positive rate: {positive_rate:.1f}%\")\n",
    "\n",
    "# Drift warning if applicable\n",
    "if ts_meta.drift_risk_level == \"high\":\n",
    "    print(f\"\\n\u26a0\ufe0f DRIFT WARNING: High drift risk detected in 01a\")\n",
    "    print(f\"   Volume drift: {ts_meta.volume_drift_risk or 'unknown'}\")\n",
    "    print(f\"   Consider: temporal validation splits, monitoring for distribution shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d.X Leakage Validation\n",
    "\n",
    "**CRITICAL CHECK:** Verify no target leakage in aggregated features before proceeding.\n",
    "\n",
    "| Check | What It Detects | Severity |\n",
    "|-------|-----------------|----------|\n",
    "| LD052 | Target column or target-derived features in feature matrix | CRITICAL |\n",
    "| LD053 | Domain patterns (churn/cancel/retain) with high correlation | CRITICAL |\n",
    "| LD001-003 | Suspiciously high feature-target correlations | HIGH |\n",
    "\n",
    "**If any CRITICAL issues are detected, do NOT proceed to modeling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage validation - MUST pass before proceeding to modeling\n",
    "from customer_retention.analysis.diagnostics import LeakageDetector\n",
    "\n",
    "if TARGET_COLUMN and TARGET_COLUMN in agg_df.columns:\n",
    "    detector = LeakageDetector()\n",
    "    \n",
    "    # Separate features and target\n",
    "    feature_cols = [c for c in agg_df.columns if c not in [ENTITY_COLUMN, TARGET_COLUMN]]\n",
    "    X = agg_df[feature_cols]\n",
    "    y = agg_df[TARGET_COLUMN]\n",
    "    \n",
    "    # Run leakage checks\n",
    "    result = detector.run_all_checks(X, y, include_pit=False)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"LEAKAGE VALIDATION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if result.passed:\n",
    "        print(\"\\n\u2705 PASSED: No critical leakage issues detected\")\n",
    "        print(f\"   Total checks run: {len(result.checks)}\")\n",
    "        print(\"\\n   You may proceed to feature engineering and modeling.\")\n",
    "    else:\n",
    "        print(\"\\n\u274c FAILED: Critical leakage issues detected!\")\n",
    "        print(f\"   Critical issues: {len(result.critical_issues)}\")\n",
    "        print(\"\\n   DO NOT proceed to modeling until issues are resolved:\\n\")\n",
    "        for issue in result.critical_issues:\n",
    "            print(f\"   [{issue.check_id}] {issue.feature}: {issue.recommendation}\")\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        raise ValueError(f\"Leakage detected: {len(result.critical_issues)} critical issues\")\n",
    "else:\n",
    "    print(\"No target column - skipping leakage validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Did\n",
    "\n",
    "In this notebook, we transformed event-level data to entity-level, applying all insights from 01a-01c:\n",
    "\n",
    "1. **Loaded findings** from prior notebooks (windows, patterns, quality)\n",
    "2. **Configured aggregation** using recommended windows from 01a\n",
    "3. **Prioritized features** based on divergent columns from 01c velocity/momentum analysis\n",
    "4. **Added lifecycle_quadrant** as recommended by 01a segmentation analysis\n",
    "5. **Added entity-level target** for downstream modeling\n",
    "6. **Saved outputs** - aggregated data, findings, and metadata\n",
    "\n",
    "## How Findings Were Applied\n",
    "\n",
    "| Finding | Source | Application |\n",
    "|---------|--------|-------------|\n",
    "| Aggregation windows | 01a | Used `suggested_aggregations` instead of defaults |\n",
    "| Lifecycle quadrant | 01a | Added as categorical feature for model |\n",
    "| Divergent columns | 01c | Prioritized in feature list (velocity/momentum signal) |\n",
    "| Drift warning | 01a | Flagged for temporal validation consideration |\n",
    "\n",
    "## Output Files\n",
    "\n",
    "| File | Purpose | Next Use |\n",
    "|------|---------|----------|\n",
    "| `*_aggregated.parquet` | Entity-level data with temporal features | Input for notebooks 02-04 |\n",
    "| `*_aggregated_findings.yaml` | Auto-profiled findings | Loaded by 02_column_deep_dive |\n",
    "| Original findings (updated) | Aggregation tracking | Reference and lineage |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Event Bronze Track complete!** Continue with the **Entity Bronze Track** on the aggregated data:\n",
    "\n",
    "1. **02_column_deep_dive.ipynb** - Profile the aggregated feature distributions\n",
    "2. **03_quality_assessment.ipynb** - Run quality checks on entity-level data  \n",
    "3. **04_relationship_analysis.ipynb** - Analyze feature correlations and target relationships\n",
    "\n",
    "The notebooks will auto-discover the aggregated findings file (most recently modified).\n",
    "\n",
    "```python\n",
    "# The aggregated findings file is now the most recent, so notebooks 02-04\n",
    "# will automatically use it via the standard discovery pattern.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}