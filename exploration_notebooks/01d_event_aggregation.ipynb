{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Chapter 1d: Event Aggregation (Event Bronze Track ‚Üí Entity Bronze Track)\n\n**Purpose:** Aggregate event-level data to entity-level, applying all insights from 01a-01c.\n\n**When to use this notebook:**\n- After completing 01a (temporal profiling), 01b (quality checks), 01c (pattern analysis)\n- Your dataset is EVENT_LEVEL granularity\n- You want to create entity-level features informed by temporal patterns\n\n**What this notebook produces:**\n- Aggregated parquet file (one row per entity)\n- New findings file for the aggregated data\n- Updated original findings with aggregation metadata\n\n**How 01a-01c findings inform aggregation:**\n\n| Source | Insight Applied |\n|--------|----------------|\n| **01a** | Recommended windows (e.g., 180d, 365d), lifecycle quadrant feature |\n| **01b** | Quality issues to handle (gaps, duplicates) |\n| **01c** | Divergent columns for velocity/momentum (prioritize these features) |\n\n---\n\n## Understanding the Shape Transformation\n\n```\nEVENT-LEVEL (input)              ENTITY-LEVEL (output)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ customer ‚îÇ date     ‚îÇ          ‚îÇ customer ‚îÇ events_180d ‚îÇ quadrant ‚îÇ ...\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚Üí     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ A        ‚îÇ Jan 1    ‚îÇ          ‚îÇ A        ‚îÇ 12          ‚îÇ Steady   ‚îÇ\n‚îÇ A        ‚îÇ Jan 5    ‚îÇ          ‚îÇ B        ‚îÇ 5           ‚îÇ Brief    ‚îÇ\n‚îÇ A        ‚îÇ Jan 10   ‚îÇ          ‚îÇ C        ‚îÇ 2           ‚îÇ Loyal    ‚îÇ\n‚îÇ B        ‚îÇ Jan 3    ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îÇ ...      ‚îÇ ...      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nMany rows per entity           One row per entity + lifecycle features\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1d.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from customer_retention.analysis.auto_explorer import ExplorationFindings, DataExplorer\nfrom customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\nfrom customer_retention.core.config.column_config import ColumnType, DatasetGranularity\nfrom customer_retention.stages.profiling import (\n    TimeWindowAggregator,\n    TimeSeriesProfiler,\n    classify_lifecycle_quadrants,\n    classify_activity_segments,\n)\nfrom datetime import datetime\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "# Find findings files (exclude multi_dataset and already-aggregated)\n",
    "findings_files = [\n",
    "    f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") \n",
    "    if \"multi_dataset\" not in f.name and \"_aggregated\" not in f.name\n",
    "]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Loaded findings for {findings.column_count} columns from {findings.source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Verify this is event-level data and display findings summary\nif not findings.is_time_series:\n    print(\"‚ö†Ô∏è This dataset is NOT event-level. Aggregation not needed.\")\n    print(\"   Proceed directly to 02_column_deep_dive.ipynb\")\n    raise SystemExit(\"Skipping aggregation - data is already entity-level\")\n\nts_meta = findings.time_series_metadata\nENTITY_COLUMN = ts_meta.entity_column\nTIME_COLUMN = ts_meta.time_column\n\nprint(\"=\" * 70)\nprint(\"FINDINGS SUMMARY FROM 01a-01c\")\nprint(\"=\" * 70)\n\n# === 01a: Time Series Metadata ===\nprint(\"\\nüìä FROM 01a (Temporal Profiling):\")\nprint(f\"   Entity column: {ENTITY_COLUMN}\")\nprint(f\"   Time column: {TIME_COLUMN}\")\nif ts_meta.unique_entities:\n    print(f\"   Unique entities: {ts_meta.unique_entities:,}\")\nif ts_meta.avg_events_per_entity:\n    print(f\"   Avg events/entity: {ts_meta.avg_events_per_entity:.1f}\")\nif ts_meta.time_span_days:\n    print(f\"   Time span: {ts_meta.time_span_days:,} days\")\n\nif ts_meta.suggested_aggregations:\n    print(f\"\\n   ‚úÖ Recommended windows: {ts_meta.suggested_aggregations}\")\nelse:\n    print(\"\\n   ‚ö†Ô∏è No window recommendations - will use defaults\")\n\nif ts_meta.temporal_segmentation_recommendation:\n    print(f\"\\n   üìã Segmentation recommendation:\")\n    print(f\"      {ts_meta.temporal_segmentation_recommendation}\")\n    if ts_meta.heterogeneity_level:\n        print(f\"      Heterogeneity: {ts_meta.heterogeneity_level}\")\n\nif ts_meta.drift_risk_level:\n    print(f\"\\n   ‚ö†Ô∏è Drift risk: {ts_meta.drift_risk_level.upper()}\")\n    if ts_meta.volume_drift_risk:\n        print(f\"      Volume drift: {ts_meta.volume_drift_risk}\")\n    if ts_meta.population_stability is not None:\n        print(f\"      Population stability: {ts_meta.population_stability:.2f}\")\n\n# === 01b: Temporal Quality ===\nquality_meta = findings.metadata.get(\"temporal_quality\", {})\nif quality_meta:\n    print(f\"\\nüìã FROM 01b (Temporal Quality):\")\n    if quality_meta.get(\"temporal_quality_score\"):\n        print(f\"   Quality score: {quality_meta.get('temporal_quality_score'):.1f}\")\n    if quality_meta.get(\"temporal_quality_grade\"):\n        print(f\"   Quality grade: {quality_meta.get('temporal_quality_grade')}\")\n    issues = quality_meta.get(\"issues\", {})\n    if issues.get(\"duplicate_events\", 0) > 0:\n        print(f\"   ‚ö†Ô∏è Duplicate events: {issues['duplicate_events']:,}\")\n    if issues.get(\"temporal_gaps\", 0) > 0:\n        print(f\"   ‚ö†Ô∏è Temporal gaps: {issues['temporal_gaps']:,}\")\n\n# === 01c: Temporal Patterns ===\npattern_meta = findings.metadata.get(\"temporal_patterns\", {})\nif pattern_meta:\n    print(f\"\\nüìà FROM 01c (Temporal Patterns):\")\n    windows_used = pattern_meta.get(\"windows_used\", {})\n    if windows_used:\n        if windows_used.get(\"aggregation_windows\"):\n            print(f\"   Windows analyzed: {windows_used.get('aggregation_windows')}\")\n        if windows_used.get(\"velocity_window\"):\n            print(f\"   Velocity window: {windows_used.get('velocity_window')} days\")\n        if windows_used.get(\"momentum_pairs\"):\n            print(f\"   Momentum pairs: {windows_used.get('momentum_pairs')}\")\n    \n    trend = pattern_meta.get(\"trend\", {})\n    if trend and trend.get(\"direction\"):\n        print(f\"\\n   Trend: {trend.get('direction')} (strength: {trend.get('strength', 0):.2f})\")\n    \n    seasonality = pattern_meta.get(\"seasonality\", [])\n    if seasonality:\n        periods = [f\"{s.get('name', 'period')} ({s.get('period')})\" for s in seasonality[:3]]\n        print(f\"   Seasonality: {', '.join(periods)}\")\n    \n    recency = pattern_meta.get(\"recency\", {})\n    if recency and recency.get(\"median_days\"):\n        print(f\"   Recency: median={recency.get('median_days'):.0f} days, \"\n              f\"target_corr={recency.get('target_correlation', 0):.2f}\")\n    \n    # Divergent columns (important for feature prioritization)\n    velocity = pattern_meta.get(\"velocity\", {})\n    divergent_velocity = [k for k, v in velocity.items() if isinstance(v, dict) and v.get(\"divergent\")]\n    if divergent_velocity:\n        print(f\"\\n   üéØ Divergent velocity columns: {divergent_velocity}\")\n    \n    momentum = pattern_meta.get(\"momentum\", {})\n    divergent_momentum = momentum.get(\"_divergent_columns\", [])\n    if divergent_momentum:\n        print(f\"   üéØ Divergent momentum columns: {divergent_momentum}\")\n\nprint(\"\\n\" + \"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n\n# Load source data (prefers snapshots over raw files)\ndf, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\ndf[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN])\ncharts = ChartBuilder()\n\nprint(f\"Loaded {len(df):,} events x {len(df.columns)} columns\")\nprint(f\"Data source: {data_source}\")\nprint(f\"Date range: {df[TIME_COLUMN].min()} to {df[TIME_COLUMN].max()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## 1d.2 Configure Aggregation Based on Findings\n\nApply all insights from 01a-01c to configure optimal aggregation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# === AGGREGATION CONFIGURATION ===\n# Windows are loaded from findings (01a recommendations) with option to override\n\n# Manual override (set to None to use findings recommendations)\nWINDOW_OVERRIDE = None  # e.g., [\"7d\", \"30d\", \"90d\"] to override\n\n# Get windows from findings or use defaults\nif WINDOW_OVERRIDE:\n    WINDOWS = WINDOW_OVERRIDE\n    window_source = \"manual override\"\nelif ts_meta.suggested_aggregations:\n    WINDOWS = ts_meta.suggested_aggregations\n    window_source = \"01a recommendations\"\nelse:\n    WINDOWS = [\"7d\", \"30d\", \"90d\", \"180d\", \"365d\", \"all_time\"]\n    window_source = \"defaults (no findings)\"\n\n# Reference date for window calculations\nREFERENCE_DATE = df[TIME_COLUMN].max()\n\n# Extract pattern metadata for feature prioritization\npattern_meta = findings.metadata.get(\"temporal_patterns\", {})\nvelocity_meta = pattern_meta.get(\"velocity\", {})\nmomentum_meta = pattern_meta.get(\"momentum\", {})\n\n# Identify divergent columns (these are most predictive for target)\nDIVERGENT_VELOCITY_COLS = [k for k, v in velocity_meta.items() \n                           if isinstance(v, dict) and v.get(\"divergent\")]\nDIVERGENT_MOMENTUM_COLS = momentum_meta.get(\"_divergent_columns\", [])\n\n# Value columns: prioritize divergent columns, then other numerics\n# IMPORTANT: Exclude target column to prevent data leakage!\nTARGET_COLUMN = findings.target_column\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nexclude_cols = {ENTITY_COLUMN, TIME_COLUMN}\nif TARGET_COLUMN:\n    exclude_cols.add(TARGET_COLUMN)\navailable_numeric = [c for c in numeric_cols if c not in exclude_cols]\n\n# Put divergent columns first (they showed predictive signal in 01c)\npriority_cols = [c for c in DIVERGENT_VELOCITY_COLS + DIVERGENT_MOMENTUM_COLS \n                 if c in available_numeric]\nother_cols = [c for c in available_numeric if c not in priority_cols]\nVALUE_COLUMNS = priority_cols + other_cols\n\n# Aggregation functions\nAGG_FUNCTIONS = [\"sum\", \"mean\", \"max\", \"count\"]\n\n# Lifecycle features (recommended by 01a segmentation analysis)\nINCLUDE_LIFECYCLE_QUADRANT = ts_meta.temporal_segmentation_recommendation is not None\nINCLUDE_RECENCY = True\nINCLUDE_TENURE = True\n\n# Print configuration\nprint(\"=\" * 70)\nprint(\"AGGREGATION CONFIGURATION\")\nprint(\"=\" * 70)\nprint(f\"\\nWindows: {WINDOWS}\")\nprint(f\"   Source: {window_source}\")\nprint(f\"\\nReference date: {REFERENCE_DATE}\")\nprint(f\"\\nValue columns ({len(VALUE_COLUMNS)} total):\")\nif priority_cols:\n    print(f\"   Priority (divergent): {priority_cols}\")\nprint(f\"   Other: {other_cols[:5]}{'...' if len(other_cols) > 5 else ''}\")\nif TARGET_COLUMN:\n    print(f\"\\n   ‚ö†Ô∏è Excluded from aggregation: {TARGET_COLUMN} (target - prevents leakage)\")\nprint(f\"\\nAggregation functions: {AGG_FUNCTIONS}\")\nprint(f\"\\nAdditional features:\")\nprint(f\"   Include lifecycle_quadrant: {INCLUDE_LIFECYCLE_QUADRANT}\")\nprint(f\"   Include recency: {INCLUDE_RECENCY}\")\nprint(f\"   Include tenure: {INCLUDE_TENURE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 1d.3 Preview Aggregation Plan\n",
    "\n",
    "See what features will be created before executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize aggregator\naggregator = TimeWindowAggregator(\n    entity_column=ENTITY_COLUMN,\n    time_column=TIME_COLUMN\n)\n\n# Generate plan\nplan = aggregator.generate_plan(\n    df=df,\n    windows=WINDOWS,\n    value_columns=VALUE_COLUMNS,\n    agg_funcs=AGG_FUNCTIONS,\n    include_event_count=True,\n    include_recency=INCLUDE_RECENCY,\n    include_tenure=INCLUDE_TENURE\n)\n\n# Count additional features we'll add\nadditional_features = []\nif INCLUDE_LIFECYCLE_QUADRANT:\n    additional_features.append(\"lifecycle_quadrant\")\nif findings.target_column and findings.target_column in df.columns:\n    additional_features.append(f\"{findings.target_column} (entity target)\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"AGGREGATION PLAN\")\nprint(\"=\"*60)\nprint(f\"\\nEntity column: {plan.entity_column}\")\nprint(f\"Time column: {plan.time_column}\")\nprint(f\"Windows: {[w.name for w in plan.windows]}\")\n\nprint(f\"\\nFeatures from aggregation ({len(plan.feature_columns)}):\")\nfor feat in plan.feature_columns[:15]:\n    # Highlight divergent column features\n    is_priority = any(dc in feat for dc in priority_cols) if priority_cols else False\n    marker = \" üéØ\" if is_priority else \"\"\n    print(f\"   - {feat}{marker}\")\nif len(plan.feature_columns) > 15:\n    print(f\"   ... and {len(plan.feature_columns) - 15} more\")\n\nif additional_features:\n    print(f\"\\nAdditional features:\")\n    for feat in additional_features:\n        print(f\"   - {feat}\")\n    \nprint(f\"\\nTotal expected features: {len(plan.feature_columns) + len(additional_features) + 1}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 1d.4 Execute Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Executing aggregation...\")\nprint(f\"   Input: {len(df):,} events\")\nprint(f\"   Expected output: {df[ENTITY_COLUMN].nunique():,} entities\")\n\n# Step 1: Basic time window aggregation\ndf_aggregated = aggregator.aggregate(\n    df,\n    windows=WINDOWS,\n    value_columns=VALUE_COLUMNS,\n    agg_funcs=AGG_FUNCTIONS,\n    reference_date=REFERENCE_DATE,\n    include_event_count=True,\n    include_recency=INCLUDE_RECENCY,\n    include_tenure=INCLUDE_TENURE\n)\n\n# Step 2: Add lifecycle quadrant (from 01a recommendation)\nif INCLUDE_LIFECYCLE_QUADRANT:\n    print(\"\\n   Adding lifecycle_quadrant feature...\")\n    profiler = TimeSeriesProfiler(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN)\n    ts_profile = profiler.profile(df)\n    \n    # Rename 'entity' column to match our entity column name\n    lifecycles = ts_profile.entity_lifecycles.copy()\n    lifecycles = lifecycles.rename(columns={\"entity\": ENTITY_COLUMN})\n    \n    quadrant_result = classify_lifecycle_quadrants(lifecycles)\n    \n    # Merge lifecycle_quadrant into aggregated data\n    quadrant_map = quadrant_result.lifecycles.set_index(ENTITY_COLUMN)[\"lifecycle_quadrant\"]\n    df_aggregated[\"lifecycle_quadrant\"] = df_aggregated[ENTITY_COLUMN].map(quadrant_map)\n    \n    print(f\"   Quadrant distribution:\")\n    for quad, count in df_aggregated[\"lifecycle_quadrant\"].value_counts().items():\n        pct = count / len(df_aggregated) * 100\n        print(f\"      {quad}: {count:,} ({pct:.1f}%)\")\n\n# Step 3: Add entity-level target (if available)\nTARGET_COLUMN = findings.target_column\nif TARGET_COLUMN and TARGET_COLUMN in df.columns:\n    print(f\"\\n   Adding entity-level target ({TARGET_COLUMN})...\")\n    # For entity-level target, use max (if any event has target=1, entity has target=1)\n    entity_target = df.groupby(ENTITY_COLUMN)[TARGET_COLUMN].max()\n    df_aggregated[TARGET_COLUMN] = df_aggregated[ENTITY_COLUMN].map(entity_target)\n    \n    target_dist = df_aggregated[TARGET_COLUMN].value_counts()\n    for val, count in target_dist.items():\n        pct = count / len(df_aggregated) * 100\n        print(f\"      {TARGET_COLUMN}={val}: {count:,} ({pct:.1f}%)\")\n\nprint(f\"\\n‚úÖ Aggregation complete!\")\nprint(f\"   Output: {len(df_aggregated):,} entities x {len(df_aggregated.columns)} features\")\nprint(f\"   Memory: {df_aggregated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview aggregated data\n",
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(df_aggregated.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nFeature Summary Statistics:\")\n",
    "display(df_aggregated.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 1d.5 Quality Check on Aggregated Data\n",
    "\n",
    "Quick validation of the aggregated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"AGGREGATED DATA QUALITY CHECK\")\nprint(\"=\"*60)\n\n# Check for nulls\nnull_counts = df_aggregated.isnull().sum()\ncols_with_nulls = null_counts[null_counts > 0]\n\nif len(cols_with_nulls) > 0:\n    print(f\"\\n‚ö†Ô∏è Columns with null values ({len(cols_with_nulls)}):\")\n    for col, count in cols_with_nulls.head(10).items():\n        pct = count / len(df_aggregated) * 100\n        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n    if len(cols_with_nulls) > 10:\n        print(f\"   ... and {len(cols_with_nulls) - 10} more\")\n    print(\"\\n   Note: Nulls in aggregated features typically mean no events in that window.\")\n    print(\"   Consider filling with 0 for count/sum features.\")\nelse:\n    print(\"\\n‚úÖ No null values in aggregated data\")\n\n# Check entity count matches\noriginal_entities = df[ENTITY_COLUMN].nunique()\naggregated_entities = len(df_aggregated)\n\nif original_entities == aggregated_entities:\n    print(f\"\\n‚úÖ Entity count matches: {aggregated_entities:,}\")\nelse:\n    print(f\"\\n‚ö†Ô∏è Entity count mismatch!\")\n    print(f\"   Original: {original_entities:,}\")\n    print(f\"   Aggregated: {aggregated_entities:,}\")\n\n# Check feature statistics\nprint(f\"\\nüìä Feature Statistics:\")\nnumeric_agg_cols = df_aggregated.select_dtypes(include=[np.number]).columns.tolist()\nif TARGET_COLUMN:\n    numeric_agg_cols = [c for c in numeric_agg_cols if c != TARGET_COLUMN]\n\nprint(f\"   Total features: {len(df_aggregated.columns)}\")\nprint(f\"   Numeric features: {len(numeric_agg_cols)}\")\n\n# Check for constant columns (no variance)\nconst_cols = [c for c in numeric_agg_cols if df_aggregated[c].std() == 0]\nif const_cols:\n    print(f\"\\n‚ö†Ô∏è Constant columns (zero variance): {len(const_cols)}\")\n    print(f\"   {const_cols[:5]}{'...' if len(const_cols) > 5 else ''}\")\n\n# If lifecycle_quadrant was added, show its correlation with target\nif INCLUDE_LIFECYCLE_QUADRANT and TARGET_COLUMN and TARGET_COLUMN in df_aggregated.columns:\n    print(f\"\\nüìä Lifecycle Quadrant vs Target:\")\n    cross = pd.crosstab(df_aggregated[\"lifecycle_quadrant\"], df_aggregated[TARGET_COLUMN], normalize='index')\n    if 1 in cross.columns:\n        for quad in cross.index:\n            rate = cross.loc[quad, 1] * 100\n            print(f\"   {quad}: {rate:.1f}% positive\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 1d.6 Save Aggregated Data and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output paths\n",
    "original_name = Path(findings.source_path).stem\n",
    "findings_name = Path(FINDINGS_PATH).stem.replace(\"_findings\", \"\")\n",
    "\n",
    "# Save aggregated data as parquet\n",
    "AGGREGATED_DATA_PATH = FINDINGS_DIR / f\"{findings_name}_aggregated.parquet\"\n",
    "df_aggregated.to_parquet(AGGREGATED_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"\\u2705 Aggregated data saved to: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   Size: {AGGREGATED_DATA_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Create new findings for aggregated data using DataExplorer\nprint(\"\\nGenerating findings for aggregated data...\")\n\nexplorer = DataExplorer(output_dir=str(FINDINGS_DIR))\naggregated_findings = explorer.explore(\n    str(AGGREGATED_DATA_PATH),\n    name=f\"{findings_name}_aggregated\"\n)\n\nAGGREGATED_FINDINGS_PATH = explorer.last_findings_path\nprint(f\"‚úÖ Aggregated findings saved to: {AGGREGATED_FINDINGS_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Update original findings with comprehensive aggregation metadata\nfindings.time_series_metadata.aggregation_executed = True\nfindings.time_series_metadata.aggregated_data_path = str(AGGREGATED_DATA_PATH)\nfindings.time_series_metadata.aggregated_findings_path = str(AGGREGATED_FINDINGS_PATH)\nfindings.time_series_metadata.aggregation_windows_used = WINDOWS\nfindings.time_series_metadata.aggregation_timestamp = datetime.now().isoformat()\n\n# Add aggregation details to metadata\nfindings.metadata[\"aggregation\"] = {\n    \"windows_used\": WINDOWS,\n    \"window_source\": window_source,\n    \"reference_date\": str(REFERENCE_DATE),\n    \"value_columns_count\": len(VALUE_COLUMNS),\n    \"priority_columns\": priority_cols,  # Divergent columns from 01c\n    \"agg_functions\": AGG_FUNCTIONS,\n    \"include_lifecycle_quadrant\": INCLUDE_LIFECYCLE_QUADRANT,\n    \"include_recency\": INCLUDE_RECENCY,\n    \"include_tenure\": INCLUDE_TENURE,\n    \"output_entities\": len(df_aggregated),\n    \"output_features\": len(df_aggregated.columns),\n    \"target_column\": TARGET_COLUMN,\n}\n\nfindings.save(FINDINGS_PATH)\nprint(f\"‚úÖ Original findings updated with aggregation metadata: {FINDINGS_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Summary of outputs\nprint(\"\\n\" + \"=\"*70)\nprint(\"AGGREGATION COMPLETE - OUTPUT SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\\nüìÅ Files created:\")\nprint(f\"   1. Aggregated data: {AGGREGATED_DATA_PATH}\")\nprint(f\"   2. Aggregated findings: {AGGREGATED_FINDINGS_PATH}\")\nprint(f\"   3. Updated original findings: {FINDINGS_PATH}\")\n\nprint(f\"\\nüìä Transformation stats:\")\nprint(f\"   Input events: {len(df):,}\")\nprint(f\"   Output entities: {len(df_aggregated):,}\")\nprint(f\"   Features created: {len(df_aggregated.columns)}\")\n\nprint(f\"\\n‚öôÔ∏è Configuration applied:\")\nprint(f\"   Windows: {WINDOWS} (from {window_source})\")\nprint(f\"   Aggregation functions: {AGG_FUNCTIONS}\")\nif priority_cols:\n    print(f\"   Priority columns (from 01c divergence): {priority_cols}\")\nif INCLUDE_LIFECYCLE_QUADRANT:\n    print(f\"   Lifecycle quadrant: included (from 01a recommendation)\")\n\nprint(f\"\\nüéØ Ready for modeling:\")\nprint(f\"   Entity column: {ENTITY_COLUMN}\")\nif TARGET_COLUMN:\n    print(f\"   Target column: {TARGET_COLUMN}\")\n    if TARGET_COLUMN in df_aggregated.columns:\n        positive_rate = df_aggregated[TARGET_COLUMN].mean() * 100\n        print(f\"   Target positive rate: {positive_rate:.1f}%\")\n\n# Drift warning if applicable\nif ts_meta.drift_risk_level == \"high\":\n    print(f\"\\n‚ö†Ô∏è DRIFT WARNING: High drift risk detected in 01a\")\n    print(f\"   Volume drift: {ts_meta.volume_drift_risk or 'unknown'}\")\n    print(f\"   Consider: temporal validation splits, monitoring for distribution shift\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": "---\n\n## Summary: What We Did\n\nIn this notebook, we transformed event-level data to entity-level, applying all insights from 01a-01c:\n\n1. **Loaded findings** from prior notebooks (windows, patterns, quality)\n2. **Configured aggregation** using recommended windows from 01a\n3. **Prioritized features** based on divergent columns from 01c velocity/momentum analysis\n4. **Added lifecycle_quadrant** as recommended by 01a segmentation analysis\n5. **Added entity-level target** for downstream modeling\n6. **Saved outputs** - aggregated data, findings, and metadata\n\n## How Findings Were Applied\n\n| Finding | Source | Application |\n|---------|--------|-------------|\n| Aggregation windows | 01a | Used `suggested_aggregations` instead of defaults |\n| Lifecycle quadrant | 01a | Added as categorical feature for model |\n| Divergent columns | 01c | Prioritized in feature list (velocity/momentum signal) |\n| Drift warning | 01a | Flagged for temporal validation consideration |\n\n## Output Files\n\n| File | Purpose | Next Use |\n|------|---------|----------|\n| `*_aggregated.parquet` | Entity-level data with temporal features | Input for notebooks 02-04 |\n| `*_aggregated_findings.yaml` | Auto-profiled findings | Loaded by 02_column_deep_dive |\n| Original findings (updated) | Aggregation tracking | Reference and lineage |\n\n---\n\n## Next Steps\n\n**Event Bronze Track complete!** Continue with the **Entity Bronze Track** on the aggregated data:\n\n1. **02_column_deep_dive.ipynb** - Profile the aggregated feature distributions\n2. **03_quality_assessment.ipynb** - Run quality checks on entity-level data  \n3. **04_relationship_analysis.ipynb** - Analyze feature correlations and target relationships\n\nThe notebooks will auto-discover the aggregated findings file (most recently modified).\n\n```python\n# The aggregated findings file is now the most recent, so notebooks 02-04\n# will automatically use it via the standard discovery pattern.\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}