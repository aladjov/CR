{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Chapter 1d: Event Aggregation (Event Bronze Track \u2192 Entity Bronze Track)\n",
    "\n",
    "**Purpose:** Aggregate event-level data to entity-level, producing a dataset ready for standard exploration.\n",
    "\n",
    "**When to use this notebook:**\n",
    "- After completing 01a, 01b, 01c (temporal exploration)\n",
    "- Your dataset is EVENT_LEVEL granularity\n",
    "- You want to create entity-level features from events\n",
    "\n",
    "**What this notebook produces:**\n",
    "- Aggregated parquet file (one row per entity)\n",
    "- New findings file for the aggregated data\n",
    "- Updated original findings with aggregation metadata\n",
    "\n",
    "**Aggregation Strategy:**\n",
    "\n",
    "| Feature Type | Examples | Purpose |\n",
    "|--------------|----------|--------|\n",
    "| **Event Counts** | event_count_7d, event_count_30d | Activity level |\n",
    "| **Value Aggregations** | amount_sum_30d, clicks_mean_7d | Behavior magnitude |\n",
    "| **Recency** | days_since_last_event | Recent engagement |\n",
    "| **Tenure** | days_since_first_event | Customer lifecycle |\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding the Shape Transformation\n",
    "\n",
    "```\n",
    "EVENT-LEVEL (input)              ENTITY-LEVEL (output)\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 customer \u2502 date     \u2502          \u2502 customer \u2502 events_7d \u2502 events_30d \u2502 ...\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2192     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 A        \u2502 Jan 1    \u2502          \u2502 A        \u2502 3         \u2502 12         \u2502\n",
    "\u2502 A        \u2502 Jan 5    \u2502          \u2502 B        \u2502 1         \u2502 5          \u2502\n",
    "\u2502 A        \u2502 Jan 10   \u2502          \u2502 C        \u2502 0         \u2502 2          \u2502\n",
    "\u2502 B        \u2502 Jan 3    \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u2502 ...      \u2502 ...      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "Many rows per entity           One row per entity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1d.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customer_retention.analysis.auto_explorer import ExplorationFindings, DataExplorer\n",
    "from customer_retention.analysis.visualization import ChartBuilder, display_figure, display_table\n",
    "from customer_retention.core.config.column_config import ColumnType, DatasetGranularity\n",
    "from customer_retention.stages.profiling import TimeWindowAggregator\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "\n",
    "# Find findings files (exclude multi_dataset and already-aggregated)\n",
    "findings_files = [\n",
    "    f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") \n",
    "    if \"multi_dataset\" not in f.name and \"_aggregated\" not in f.name\n",
    "]\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings files found in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "findings_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Loaded findings for {findings.column_count} columns from {findings.source_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify this is event-level data\n",
    "if not findings.is_time_series:\n",
    "    print(\"\\u26a0\\ufe0f This dataset is NOT event-level. Aggregation not needed.\")\n",
    "    print(\"   Proceed directly to 02_column_deep_dive.ipynb\")\n",
    "    raise SystemExit(\"Skipping aggregation - data is already entity-level\")\n",
    "\n",
    "ts_meta = findings.time_series_metadata\n",
    "ENTITY_COLUMN = ts_meta.entity_column\n",
    "TIME_COLUMN = ts_meta.time_column\n",
    "\n",
    "print(f\"\\u2705 Dataset confirmed as EVENT-LEVEL\")\n",
    "print(f\"   Entity column: {ENTITY_COLUMN}\")\n",
    "print(f\"   Time column: {TIME_COLUMN}\")\n",
    "print(f\"   Unique entities: {ts_meta.unique_entities:,}\")\n",
    "print(f\"   Avg events/entity: {ts_meta.avg_events_per_entity:.1f}\")\n",
    "\n",
    "if ts_meta.temporal_segmentation_recommendation:\n",
    "    print(f\"\\n   Segmentation recommendation: {ts_meta.temporal_segmentation_recommendation}\")\n",
    "    print(f\"   Heterogeneity: {ts_meta.heterogeneity_level} \"\n",
    "          f\"(eta\\u00b2: intensity={ts_meta.eta_squared_intensity:.3f}, \"\n",
    "          f\"count={ts_meta.eta_squared_event_count:.3f})\")\n",
    "\n",
    "if ts_meta.drift_risk_level:\n",
    "    print(f\"\\n   Drift risk: {ts_meta.drift_risk_level.upper()}\")\n",
    "    print(f\"   Volume drift: {ts_meta.volume_drift_risk or 'none'}\")\n",
    "    if ts_meta.population_stability is not None:\n",
    "        print(f\"   Population stability: {ts_meta.population_stability:.2f}\")\n",
    "    if ts_meta.regime_count and ts_meta.regime_count > 1:\n",
    "        print(f\"   Data regimes: {ts_meta.regime_count}\")\n",
    "        if ts_meta.recommended_training_start:\n",
    "            print(f\"   Recommended training start: {ts_meta.recommended_training_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "from customer_retention.stages.temporal import load_data_with_snapshot_preference, TEMPORAL_METADATA_COLS\n\n# Load source data (prefers snapshots over raw files)\ndf, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\ndf[TIME_COLUMN] = pd.to_datetime(df[TIME_COLUMN])\ncharts = ChartBuilder()\n\nprint(f\"Loaded {len(df):,} events x {len(df.columns)} columns\")\nprint(f\"Data source: {data_source}\")\nprint(f\"Date range: {df[TIME_COLUMN].min()} to {df[TIME_COLUMN].max()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1d.2 Configure Aggregation\n",
    "\n",
    "Configure the time windows and aggregation functions to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# === AGGREGATION CONFIGURATION ===\n\n# Time windows (from findings or defaults)\nDEFAULT_WINDOWS = [\"7d\", \"30d\", \"90d\", \"180d\", \"365d\", \"all_time\"]\nWINDOWS = ts_meta.suggested_aggregations if ts_meta.suggested_aggregations else DEFAULT_WINDOWS\n\n# Reference date for window calculations\n# Options: use max date in data, or a fixed date\nREFERENCE_DATE = df[TIME_COLUMN].max()\n# REFERENCE_DATE = pd.Timestamp(\"2024-01-01\")  # Uncomment to use fixed date\n\n# Value columns to aggregate (numeric columns excluding entity/time)\nnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\nVALUE_COLUMNS = [c for c in numeric_cols if c not in [ENTITY_COLUMN]]\n\n# Aggregation functions\nAGG_FUNCTIONS = [\"sum\", \"mean\", \"max\", \"count\"]\n\n# Include recency and tenure features\nINCLUDE_RECENCY = True\nINCLUDE_TENURE = True\n\nprint(\"Aggregation Configuration:\")\nprint(f\"   Windows: {WINDOWS}\")\nprint(f\"   Reference date: {REFERENCE_DATE}\")\nprint(f\"   Value columns: {VALUE_COLUMNS[:5]}{'...' if len(VALUE_COLUMNS) > 5 else ''}\")\nprint(f\"   Aggregation functions: {AGG_FUNCTIONS}\")\nprint(f\"   Include recency: {INCLUDE_RECENCY}\")\nprint(f\"   Include tenure: {INCLUDE_TENURE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 1d.3 Preview Aggregation Plan\n",
    "\n",
    "See what features will be created before executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aggregator\n",
    "aggregator = TimeWindowAggregator(\n",
    "    entity_column=ENTITY_COLUMN,\n",
    "    time_column=TIME_COLUMN\n",
    ")\n",
    "\n",
    "# Generate plan\n",
    "plan = aggregator.generate_plan(\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEntity column: {plan.entity_column}\")\n",
    "print(f\"Time column: {plan.time_column}\")\n",
    "print(f\"Windows: {[w.name for w in plan.windows]}\")\n",
    "print(f\"\\nFeatures to be created ({len(plan.feature_columns)}):\")\n",
    "for feat in plan.feature_columns[:20]:\n",
    "    print(f\"   - {feat}\")\n",
    "if len(plan.feature_columns) > 20:\n",
    "    print(f\"   ... and {len(plan.feature_columns) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 1d.4 Execute Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Executing aggregation...\")\n",
    "print(f\"   Input: {len(df):,} events\")\n",
    "print(f\"   Expected output: {df[ENTITY_COLUMN].nunique():,} entities\")\n",
    "\n",
    "df_aggregated = aggregator.aggregate(\n",
    "    df,\n",
    "    windows=WINDOWS,\n",
    "    value_columns=VALUE_COLUMNS,\n",
    "    agg_funcs=AGG_FUNCTIONS,\n",
    "    reference_date=REFERENCE_DATE,\n",
    "    include_event_count=True,\n",
    "    include_recency=INCLUDE_RECENCY,\n",
    "    include_tenure=INCLUDE_TENURE\n",
    ")\n",
    "\n",
    "print(f\"\\n\\u2705 Aggregation complete!\")\n",
    "print(f\"   Output: {len(df_aggregated):,} entities x {len(df_aggregated.columns)} features\")\n",
    "print(f\"   Memory: {df_aggregated.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview aggregated data\n",
    "print(\"\\nAggregated Data Preview:\")\n",
    "display(df_aggregated.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nFeature Summary Statistics:\")\n",
    "display(df_aggregated.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 1d.5 Quality Check on Aggregated Data\n",
    "\n",
    "Quick validation of the aggregated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AGGREGATED DATA QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for nulls\n",
    "null_counts = df_aggregated.isnull().sum()\n",
    "cols_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "if len(cols_with_nulls) > 0:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Columns with null values ({len(cols_with_nulls)}):\")\n",
    "    for col, count in cols_with_nulls.items():\n",
    "        pct = count / len(df_aggregated) * 100\n",
    "        print(f\"   {col}: {count:,} ({pct:.1f}%)\")\n",
    "    print(\"\\n   Note: Nulls in aggregated features typically mean no events in that window.\")\n",
    "    print(\"   Consider filling with 0 for count/sum features.\")\n",
    "else:\n",
    "    print(\"\\n\\u2705 No null values in aggregated data\")\n",
    "\n",
    "# Check entity count matches\n",
    "original_entities = df[ENTITY_COLUMN].nunique()\n",
    "aggregated_entities = len(df_aggregated)\n",
    "\n",
    "if original_entities == aggregated_entities:\n",
    "    print(f\"\\n\\u2705 Entity count matches: {aggregated_entities:,}\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0\\ufe0f Entity count mismatch!\")\n",
    "    print(f\"   Original: {original_entities:,}\")\n",
    "    print(f\"   Aggregated: {aggregated_entities:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 1d.6 Save Aggregated Data and Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output paths\n",
    "original_name = Path(findings.source_path).stem\n",
    "findings_name = Path(FINDINGS_PATH).stem.replace(\"_findings\", \"\")\n",
    "\n",
    "# Save aggregated data as parquet\n",
    "AGGREGATED_DATA_PATH = FINDINGS_DIR / f\"{findings_name}_aggregated.parquet\"\n",
    "df_aggregated.to_parquet(AGGREGATED_DATA_PATH, index=False)\n",
    "\n",
    "print(f\"\\u2705 Aggregated data saved to: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   Size: {AGGREGATED_DATA_PATH.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new findings for aggregated data using DataExplorer\n",
    "print(\"\\nGenerating findings for aggregated data...\")\n",
    "\n",
    "explorer = DataExplorer(output_dir=str(FINDINGS_DIR))\n",
    "aggregated_findings = explorer.explore(\n",
    "    str(AGGREGATED_DATA_PATH),\n",
    "    dataset_name=f\"{findings_name}_aggregated\"\n",
    ")\n",
    "\n",
    "AGGREGATED_FINDINGS_PATH = explorer.last_findings_path\n",
    "print(f\"\\u2705 Aggregated findings saved to: {AGGREGATED_FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update original findings with aggregation metadata\n",
    "findings.time_series_metadata.aggregation_executed = True\n",
    "findings.time_series_metadata.aggregated_data_path = str(AGGREGATED_DATA_PATH)\n",
    "findings.time_series_metadata.aggregated_findings_path = str(AGGREGATED_FINDINGS_PATH)\n",
    "findings.time_series_metadata.aggregation_windows_used = WINDOWS\n",
    "findings.time_series_metadata.aggregation_timestamp = datetime.now().isoformat()\n",
    "\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"\\u2705 Original findings updated with aggregation metadata: {FINDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATION COMPLETE - OUTPUT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n\\U0001f4c1 Files created:\")\n",
    "print(f\"   1. Aggregated data: {AGGREGATED_DATA_PATH}\")\n",
    "print(f\"   2. Aggregated findings: {AGGREGATED_FINDINGS_PATH}\")\n",
    "print(f\"   3. Updated original findings: {FINDINGS_PATH}\")\n",
    "\n",
    "print(f\"\\n\\U0001f4ca Aggregation stats:\")\n",
    "print(f\"   Input events: {len(df):,}\")\n",
    "print(f\"   Output entities: {len(df_aggregated):,}\")\n",
    "print(f\"   Features created: {len(df_aggregated.columns)}\")\n",
    "print(f\"   Windows used: {WINDOWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Did\n",
    "\n",
    "In this notebook, we transformed event-level data to entity-level:\n",
    "\n",
    "1. **Loaded event data** with entity and time columns\n",
    "2. **Configured aggregation** windows and functions\n",
    "3. **Executed aggregation** using TimeWindowAggregator\n",
    "4. **Quality checked** the aggregated output\n",
    "5. **Saved outputs** - data file, findings, and metadata\n",
    "\n",
    "## Output Files\n",
    "\n",
    "| File | Purpose | Next Use |\n",
    "|------|---------|----------|\n",
    "| `*_aggregated.parquet` | Entity-level data | Input for notebooks 02-04 |\n",
    "| `*_aggregated_findings.yaml` | Auto-profiled findings | Loaded by 02_column_deep_dive |\n",
    "| Original findings (updated) | Aggregation tracking | Reference |\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Event Bronze Track complete!** Continue with the **Entity Bronze Track** on the aggregated data:\n",
    "\n",
    "1. **02_column_deep_dive.ipynb** - Profile the aggregated feature distributions\n",
    "2. **03_quality_assessment.ipynb** - Run quality checks on entity-level data\n",
    "3. **04_relationship_analysis.ipynb** - Analyze feature correlations and target relationships\n",
    "\n",
    "The notebooks will auto-discover the aggregated findings file (most recently modified).\n",
    "\n",
    "```python\n",
    "# The aggregated findings file is now the most recent, so notebooks 02-04\n",
    "# will automatically use it via the standard discovery pattern.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}