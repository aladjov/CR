{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Chapter 1b: Temporal Quality Assessment (Event Bronze Track)\n\n**Purpose:** Run quality checks specific to event-level datasets to identify data issues before feature engineering.\n\n**When to use this notebook:**\n- After completing 01a_temporal_deep_dive.ipynb\n- Your dataset is EVENT_LEVEL granularity\n- You want to validate temporal data integrity before aggregation\n\n| Check | What It Detects | Why It Matters for ML |\n|-------|-----------------|----------------------|\n| **TQ001** | Duplicate events (same entity + timestamp) | Inflates counts, skews aggregations, creates artificial sequence patterns |\n| **TQ002** | Unexpected temporal gaps | Rolling features become misleading; \"events in last 30d\" drops during gaps |\n| **TQ003** | Future dates | Data leakage ‚Äî model sees future during training |\n| **TQ004** | Ambiguous event ordering | Sequence features undefined when multiple events share timestamp |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1b.1 Load Findings and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom customer_retention.analysis.auto_explorer import ExplorationFindings, RecommendationEngine\nfrom customer_retention.analysis.visualization import ChartBuilder, display_figure\nfrom customer_retention.core.config.column_config import ColumnType\nfrom customer_retention.stages.profiling import (\n    DuplicateEventCheck, TemporalGapCheck, FutureDateCheck, EventOrderCheck,\n    TemporalQualityReporter, SegmentAwareOutlierAnalyzer\n)\nfrom customer_retention.stages.temporal import load_data_with_snapshot_preference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINDINGS_DIR = Path(\"../experiments/findings\")\n",
    "findings_files = sorted(\n",
    "    [f for f in FINDINGS_DIR.glob(\"*_findings.yaml\") if \"multi_dataset\" not in f.name],\n",
    "    key=lambda f: f.stat().st_mtime, reverse=True\n",
    ")\n",
    "if not findings_files:\n",
    "    raise FileNotFoundError(f\"No findings in {FINDINGS_DIR}. Run notebook 01 first.\")\n",
    "\n",
    "FINDINGS_PATH = str(findings_files[0])\n",
    "findings = ExplorationFindings.load(FINDINGS_PATH)\n",
    "print(f\"Using: {FINDINGS_PATH}\")\n",
    "\n",
    "ts_meta = findings.time_series_metadata\n",
    "ENTITY_COLUMN, TIME_COLUMN = ts_meta.entity_column, ts_meta.time_column\n",
    "print(f\"Entity: {ENTITY_COLUMN}, Time: {TIME_COLUMN}\")\n",
    "\n",
    "df, data_source = load_data_with_snapshot_preference(findings, output_dir=\"../experiments/findings\")\n",
    "charts = ChartBuilder()\n",
    "print(f\"Loaded {len(df):,} rows ({data_source})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1b.2 Configure Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_DATE = pd.Timestamp.now()  # or pd.Timestamp(\"2024-01-01\")\n",
    "EXPECTED_FREQUENCY = \"D\"  # D=daily, W=weekly, M=monthly, H=hourly\n",
    "MAX_GAP_MULTIPLE = 3.0\n",
    "\n",
    "print(f\"Reference: {REFERENCE_DATE.date()}, Frequency: {EXPECTED_FREQUENCY}, Gap threshold: {MAX_GAP_MULTIPLE}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "## 1b.3 Run Temporal Quality Checks\n\n| Issue Type | ML Impact | Mitigation |\n|------------|-----------|------------|\n| Duplicates | Sum/count features inflated; artificial patterns in sequences | Deduplicate or add sequence index |\n| Gaps | Rolling aggregations drop; recency features spike | Document gaps; add gap indicator feature |\n| Future dates | Model trains on leaked future info | Filter to reference date; check timezone handling |\n| Ordering | \"Previous event\" features undefined | Add tiebreaker column; use stable sort |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checks = [\n",
    "    DuplicateEventCheck(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN),\n",
    "    TemporalGapCheck(time_column=TIME_COLUMN, expected_frequency=EXPECTED_FREQUENCY, max_gap_multiple=MAX_GAP_MULTIPLE),\n",
    "    FutureDateCheck(time_column=TIME_COLUMN, reference_date=REFERENCE_DATE),\n",
    "    EventOrderCheck(entity_column=ENTITY_COLUMN, time_column=TIME_COLUMN),\n",
    "]\n",
    "results = [check.run(df) for check in checks]\n",
    "reporter = TemporalQualityReporter(results, len(df))\n",
    "reporter.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "## 1b.4 Quality Score\n\n| Component | Weight | Scoring Logic |\n|-----------|--------|---------------|\n| Each check | 25% | 100 if no issues; deductions proportional to % affected |\n| Grade A | 90-100 | Proceed with confidence |\n| Grade B | 75-89 | Document issues, proceed with caution |\n| Grade C | 60-74 | Address issues before feature engineering |\n| Grade D | <60 | Investigation required |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter.print_score()\n",
    "quality_score, grade, passed = reporter.quality_score, reporter.grade, reporter.passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## 1b.5 Event Volume Analysis\n\n| What to Look For | Indicates | Action |\n|-----------------|-----------|--------|\n| Missing bars | Data gaps (TQ002) | Document; add gap indicator |\n| Declining trend | Population shrinkage or data cutoff | Check if intentional |\n| Spikes | Campaigns, seasonality, or data issues | Investigate cause |\n| Flat periods | Possible logging outages | Verify with data source |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.copy()\n",
    "df_temp[TIME_COLUMN] = pd.to_datetime(df_temp[TIME_COLUMN])\n",
    "time_span = (df_temp[TIME_COLUMN].max() - df_temp[TIME_COLUMN].min()).days\n",
    "\n",
    "freq, label = (\"D\", \"Daily\") if time_span <= 90 else (\"W\", \"Weekly\") if time_span <= 365 else (\"ME\", \"Monthly\")\n",
    "counts = df_temp.groupby(pd.Grouper(key=TIME_COLUMN, freq=freq)).size()\n",
    "\n",
    "fig = go.Figure(go.Bar(x=counts.index, y=counts.values, marker_color=\"#4682B4\"))\n",
    "fig.update_layout(title=f\"{label} Event Volume (gaps = missing bars)\", height=300, template=\"plotly_white\")\n",
    "display_figure(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "## 1b.6 Target Distribution (Event Level)\n\n| Imbalance Level | Ratio | Note |\n|-----------------|-------|------|\n| Mild | <3:1 | Standard methods work |\n| Moderate | 3-10:1 | Use stratified splits |\n| Severe | >10:1 | Resampling needed (see notebook 07) |\n\n**Note:** This shows event-level distribution. Entity-level distribution (after aggregation) is what matters for modeling ‚Äî analyzed in notebook 07."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "if findings.target_column and findings.target_column in df.columns:\n    target_counts = df[findings.target_column].value_counts().sort_index()\n    labels = [f\"{'Retained' if v == 1 else 'Churned'} ({v})\" for v in target_counts.index]\n    \n    fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]])\n    fig.add_trace(go.Pie(labels=labels, values=target_counts.values, hole=0.4,\n        marker_colors=[\"#e74c3c\", \"#2ecc71\"], textinfo=\"percent\"), row=1, col=1)\n    fig.add_trace(go.Bar(x=labels, y=target_counts.values, marker_color=[\"#e74c3c\", \"#2ecc71\"],\n        text=[f\"{c:,}\" for c in target_counts.values], textposition=\"inside\", showlegend=False), row=1, col=2)\n    fig.update_layout(height=350, template=\"plotly_white\", title=\"Target Distribution (Event Level)\")\n    display_figure(fig)\n    \n    if len(target_counts) == 2:\n        ratio = target_counts.max() / target_counts.min()\n        print(f\"\\nEvent-level imbalance: {ratio:.1f}:1 (minority: {target_counts.idxmin()})\")\n        print(\"‚ÑπÔ∏è Note: Resampling strategies apply after aggregation - see notebook 07\")\nelse:\n    print(\"No target column detected.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "## 1b.7 Outlier Analysis\n\n| Approach | When to Use | Why It Matters |\n|----------|-------------|----------------|\n| Global detection | Homogeneous data | Simple threshold works |\n| Segment-aware | Data has natural groups | Avoids false positives when segments have different scales |\n\nSegment-aware detection clusters entities by target (or other segment) and detects outliers within each group separately."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [n for n, c in findings.columns.items()\n",
    "    if c.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]\n",
    "    and n not in [ENTITY_COLUMN, TIME_COLUMN]]\n",
    "\n",
    "if numeric_cols:\n",
    "    analyzer = SegmentAwareOutlierAnalyzer(max_segments=5)\n",
    "    result = analyzer.analyze(df, feature_cols=numeric_cols, segment_col=None, target_col=findings.target_column)\n",
    "    \n",
    "    print(f\"Segments detected: {result.n_segments}\")\n",
    "    if result.n_segments > 1:\n",
    "        data = [{\"Feature\": c, \"Global\": result.global_analysis[c].outliers_detected,\n",
    "            \"Segment\": sum(s[c].outliers_detected for s in result.segment_analysis.values() if c in s)}\n",
    "            for c in numeric_cols]\n",
    "        display(pd.DataFrame(data))\n",
    "        if result.segmentation_recommended:\n",
    "            print(\"\\nüí° Segment-specific outlier treatment recommended\")\n",
    "    else:\n",
    "        print(\"Data appears homogeneous - using global outlier detection\")\n",
    "else:\n",
    "    print(\"No numeric columns for outlier analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "## 1b.8 Data Validation\n\n| Check | Issue | Impact |\n|-------|-------|--------|\n| Binary fields | Values outside {0, 1} | Model crashes or silent errors |\n| String consistency | Case/spacing variants (\"Yes\" vs \"yes\") | Inflated cardinality; split categories |\n| Missing patterns | Systematic missingness | Bias in imputation |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary field validation\n",
    "binary_cols = [n for n, c in findings.columns.items() if c.inferred_type == ColumnType.BINARY]\n",
    "for col in binary_cols:\n",
    "    c0, c1 = (df[col] == 0).sum(), (df[col] == 1).sum()\n",
    "    print(f\"‚úì {col}: 0={c0:,} ({c0/(c0+c1)*100:.1f}%), 1={c1:,} ({c1/(c0+c1)*100:.1f}%)\")\n",
    "\n",
    "# Consistency check\n",
    "issues = []\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col in [ENTITY_COLUMN, TIME_COLUMN]: continue\n",
    "    variants = {}\n",
    "    for v in df[col].dropna().unique():\n",
    "        key = str(v).lower().strip()\n",
    "        variants.setdefault(key, []).append(v)\n",
    "    issues.extend([{\"Column\": col, \"Variants\": vs} for vs in variants.values() if len(vs) > 1])\n",
    "\n",
    "print(f\"\\n{'‚ö†Ô∏è Consistency issues: ' + str(len(issues)) if issues else '‚úÖ No consistency issues'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## 1b.9 Recommendations\n\nFramework-generated recommendations based on column-level issues detected during exploration."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "rec_engine = RecommendationEngine()\nrecs = rec_engine.recommend_cleaning(findings)\n\nif recs:\n    for r in sorted(recs, key=lambda x: {\"high\": 0, \"medium\": 1, \"low\": 2}.get(x.severity, 3)):\n        icon = {\"high\": \"üî¥\", \"medium\": \"üü°\", \"low\": \"üü¢\"}.get(r.severity, \"‚ö™\")\n        print(f\"{icon} [{r.severity.upper()}] {r.column_name}: {r.description}\")\n        label = r.strategy_label if r.strategy_label else r.strategy.replace(\"_\", \" \").title()\n        print(f\"   Strategy: {label}\")\nelse:\n    print(\"‚úÖ No critical cleaning recommendations\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 1b.10 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not findings.metadata:\n",
    "    findings.metadata = {}\n",
    "findings.metadata[\"temporal_quality\"] = reporter.to_dict()\n",
    "findings.save(FINDINGS_PATH)\n",
    "print(f\"Saved to: {FINDINGS_PATH}\")\n",
    "print(f\"Score: {quality_score:.0f}/100 (Grade {grade})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "---\n\n## Summary: What We Learned\n\nIn this notebook, we validated temporal data quality:\n\n1. **Temporal Quality Checks** ‚Äî Detected duplicates, gaps, future dates, ordering issues\n2. **Quality Score** ‚Äî Quantified overall data health with pass/fail grading\n3. **Event Volume** ‚Äî Visualized data coverage over time\n4. **Target Distribution** ‚Äî Checked event-level class balance\n5. **Outlier Analysis** ‚Äî Compared global vs segment-aware detection\n6. **Data Validation** ‚Äî Verified binary fields and string consistency\n\n## Quality Score Interpretation\n\n| Grade | Score | Meaning | Action |\n|-------|-------|---------|--------|\n| A | 90-100 | Excellent | Proceed with confidence |\n| B | 75-89 | Good | Document issues, proceed |\n| C | 60-74 | Fair | Address issues before aggregation |\n| D | <60 | Poor | Investigation required |\n\n---\n\n## Next Steps\n\nContinue with the **Event Bronze Track**:\n\n1. **01c_temporal_patterns.ipynb** ‚Äî Detect trends, seasonality, cohort effects\n2. **01d_event_aggregation.ipynb** ‚Äî Aggregate events to entity-level features\n\nAfter 01d, continue with **Entity Bronze Track** (02 ‚Üí 03 ‚Üí 04) on aggregated data."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}