{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Pipeline Generation\n",
    "\n",
    "Generate production-ready pipeline code from exploration findings.\n",
    "\n",
    "**Generation Targets:**\n",
    "1. **Local (Feast + MLFlow)** - Local feature store and experiment tracking\n",
    "2. **Databricks (FS + MLFlow)** - Unity Catalog, DLT, Feature Store, MLFlow\n",
    "3. **LLM Documentation** - Markdown files for AI-assisted development\n",
    "\n",
    "**Output Formats:**\n",
    "- Python files (`.py`)\n",
    "- Jupyter notebooks (`.ipynb`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "\n",
    "class GenerationTarget(Enum):\n",
    "    LOCAL_FEAST_MLFLOW = \"local\"\n",
    "    DATABRICKS = \"databricks\"\n",
    "    LLM_DOCS = \"llm_docs\"\n",
    "\n",
    "class OutputFormat(Enum):\n",
    "    PYTHON = \"py\"\n",
    "    NOTEBOOK = \"ipynb\"\n",
    "\n",
    "# === USER CONFIGURATION ===\n",
    "PIPELINE_NAME = \"customer_churn\"\n",
    "GENERATION_TARGET = GenerationTarget.LOCAL_FEAST_MLFLOW\n",
    "OUTPUT_FORMAT = OutputFormat.PYTHON\n",
    "\n",
    "# Paths\n",
    "# FINDINGS_DIR imported from customer_retention.core.config.experiments\n",
    "OUTPUT_BASE_DIR = Path(\"../generated_pipelines\")\n",
    "\n",
    "# Databricks settings (only used when GENERATION_TARGET == DATABRICKS)\n",
    "DATABRICKS_CATALOG = \"main\"\n",
    "DATABRICKS_SCHEMA = \"ml_features\"\n",
    "\n",
    "print(f\"Pipeline: {PIPELINE_NAME}\")\n",
    "print(f\"Target: {GENERATION_TARGET.value}\")\n",
    "print(f\"Format: {OUTPUT_FORMAT.value}\")\n",
    "from customer_retention.stages.temporal import TEMPORAL_METADATA_COLS\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Load Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from customer_retention.analysis.auto_explorer import ExplorationFindings\n",
    "from customer_retention.analysis.auto_explorer.layered_recommendations import RecommendationRegistry\n",
    "from customer_retention.core.config.experiments import FINDINGS_DIR, EXPERIMENTS_DIR, OUTPUT_DIR, setup_experiments_structure\n",
    "\n",
    "def load_findings_and_recommendations(findings_dir: Path):\n",
    "    findings_files = sorted(\n",
    "        [f for f in findings_dir.glob(\"*_findings.yaml\") if \"multi_dataset\" not in f.name],\n",
    "        key=lambda f: f.stat().st_mtime, reverse=True\n",
    "    )\n",
    "    if not findings_files:\n",
    "        raise FileNotFoundError(f\"No findings in {findings_dir}. Run exploration notebooks first.\")\n",
    "    \n",
    "    findings = ExplorationFindings.load(str(findings_files[0]))\n",
    "    \n",
    "    # Look for recommendations file matching the findings file pattern\n",
    "    # Step 06 saves as: {name}_recommendations.yaml (matching {name}_findings.yaml)\n",
    "    findings_name = findings_files[0].stem.replace(\"_findings\", \"\")\n",
    "    recommendations_path = findings_dir / f\"{findings_name}_recommendations.yaml\"\n",
    "    \n",
    "    # Fallback to generic recommendations.yaml if not found\n",
    "    if not recommendations_path.exists():\n",
    "        recommendations_path = findings_dir / \"recommendations.yaml\"\n",
    "    \n",
    "    # Final fallback: find any *_recommendations.yaml\n",
    "    if not recommendations_path.exists():\n",
    "        rec_files = sorted(findings_dir.glob(\"*_recommendations.yaml\"), \n",
    "                          key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "        if rec_files:\n",
    "            recommendations_path = rec_files[0]\n",
    "    \n",
    "    registry = None\n",
    "    if recommendations_path.exists():\n",
    "        with open(recommendations_path) as f:\n",
    "            registry = RecommendationRegistry.from_dict(yaml.safe_load(f))\n",
    "        print(f\"Loaded recommendations from: {recommendations_path.name}\")\n",
    "    \n",
    "    multi_dataset_path = findings_dir / \"multi_dataset_findings.yaml\"\n",
    "    multi_dataset = None\n",
    "    if multi_dataset_path.exists():\n",
    "        with open(multi_dataset_path) as f:\n",
    "            multi_dataset = yaml.safe_load(f)\n",
    "    \n",
    "    return findings, registry, multi_dataset\n",
    "\n",
    "findings, registry, multi_dataset = load_findings_and_recommendations(FINDINGS_DIR)\n",
    "\n",
    "print(f\"Loaded: {findings.source_path}\")\n",
    "print(f\"Rows: {findings.row_count:,} | Columns: {findings.column_count}\")\n",
    "print(f\"Target: {findings.target_column}\")\n",
    "print(f\"Recommendations: {'Loaded' if registry else 'Not found'}\")\n",
    "print(f\"Multi-dataset: {'Loaded' if multi_dataset else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Review Layered Recommendations\n",
    "\n",
    "Recommendations are organized by medallion layer:\n",
    "- **Bronze**: null_handling, outlier_handling, type_conversions, deduplication, filtering, text_processing\n",
    "- **Silver**: joins, aggregations, derived_columns\n",
    "- **Gold**: encoding, scaling, feature_selection, transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendations(registry: RecommendationRegistry):\n",
    "    if not registry:\n",
    "        print(\"No recommendations loaded. Run notebooks 02-07 first.\")\n",
    "        return\n",
    "    \n",
    "    for layer in [\"bronze\", \"silver\", \"gold\"]:\n",
    "        recs = registry.get_by_layer(layer)\n",
    "        print(f\"\\n{layer.upper()} ({len(recs)} recommendations):\")\n",
    "        print(\"-\" * 50)\n",
    "        for rec in recs[:5]:\n",
    "            print(f\"  [{rec.category}] {rec.target_column}: {rec.action}\")\n",
    "        if len(recs) > 5:\n",
    "            print(f\"  ... and {len(recs) - 5} more\")\n",
    "\n",
    "display_recommendations(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.4 Generate Pipeline\n",
    "\n",
    "Select generation based on configured target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = OUTPUT_BASE_DIR / GENERATION_TARGET.value / PIPELINE_NAME\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Local (Feast + MLFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATION_TARGET == GenerationTarget.LOCAL_FEAST_MLFLOW:\n",
    "    from customer_retention.generators.spec_generator import MLflowPipelineGenerator, MLflowConfig\n",
    "    from customer_retention.generators.pipeline_generator import PipelineGenerator\n",
    "    \n",
    "    mlflow_config = MLflowConfig(\n",
    "        tracking_uri=\"./mlruns\",\n",
    "        experiment_name=PIPELINE_NAME,\n",
    "        log_data_quality=True,\n",
    "        nested_runs=True\n",
    "    )\n",
    "    \n",
    "    mlflow_gen = MLflowPipelineGenerator(mlflow_config=mlflow_config, output_dir=str(output_dir))\n",
    "    \n",
    "    if OUTPUT_FORMAT == OutputFormat.PYTHON:\n",
    "        saved = mlflow_gen.save_all(findings)\n",
    "        print(\"Generated MLflow pipeline files:\")\n",
    "        for f in saved:\n",
    "            print(f\"  {f}\")\n",
    "    \n",
    "    if multi_dataset:\n",
    "        pipeline_gen = PipelineGenerator(\n",
    "            findings_dir=str(FINDINGS_DIR),\n",
    "            output_dir=str(output_dir),\n",
    "            pipeline_name=PIPELINE_NAME\n",
    "        )\n",
    "        orch_files = pipeline_gen.generate()\n",
    "        print(\"\\nGenerated pipeline files (Bronze/Silver/Gold/Training):\")\n",
    "        for f in orch_files:\n",
    "            print(f\"  {f}\")\n",
    "else:\n",
    "    print(f\"Skipping Local generation (target is {GENERATION_TARGET.value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Databricks (FS + MLFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATION_TARGET == GenerationTarget.DATABRICKS:\n",
    "    from customer_retention.generators.spec_generator import DatabricksSpecGenerator, PipelineSpec, SourceSpec\n",
    "    \n",
    "    spec = PipelineSpec(\n",
    "        name=PIPELINE_NAME,\n",
    "        version=\"1.0.0\",\n",
    "        sources=[SourceSpec(\n",
    "            name=findings.source_path.split(\"/\")[-1].replace(\".csv\", \"\"),\n",
    "            path=findings.source_path,\n",
    "            format=findings.source_format\n",
    "        )]\n",
    "    )\n",
    "    \n",
    "    if findings.target_column:\n",
    "        from customer_retention.generators.spec_generator import ModelSpec\n",
    "        spec.model_config = ModelSpec(\n",
    "            name=f\"{PIPELINE_NAME}_model\",\n",
    "            model_type=\"gradient_boosting\",\n",
    "            target_column=findings.target_column\n",
    "        )\n",
    "    \n",
    "    db_gen = DatabricksSpecGenerator(\n",
    "        catalog=DATABRICKS_CATALOG,\n",
    "        schema=DATABRICKS_SCHEMA,\n",
    "        output_dir=str(output_dir)\n",
    "    )\n",
    "    \n",
    "    saved = db_gen.save_all(spec)\n",
    "    print(\"Generated Databricks artifacts:\")\n",
    "    for f in saved:\n",
    "        print(f\"  {f}\")\n",
    "else:\n",
    "    print(f\"Skipping Databricks generation (target is {GENERATION_TARGET.value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: LLM Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENERATION_TARGET == GenerationTarget.LLM_DOCS:\n",
    "    from customer_retention.analysis.auto_explorer import RecommendationEngine\n",
    "    \n",
    "    recommender = RecommendationEngine()\n",
    "    target_rec = recommender.recommend_target(findings)\n",
    "    feature_recs = recommender.recommend_features(findings)\n",
    "    cleaning_recs = recommender.recommend_cleaning(findings)\n",
    "    \n",
    "    docs_dir = output_dir / \"docs\"\n",
    "    docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Overview\n",
    "    overview = f\"\"\"# {PIPELINE_NAME} Pipeline Overview\n",
    "\n",
    "## Data Source\n",
    "- **Path**: {findings.source_path}\n",
    "- **Format**: {findings.source_format}\n",
    "- **Rows**: {findings.row_count:,}\n",
    "- **Columns**: {findings.column_count}\n",
    "- **Quality Score**: {findings.overall_quality_score:.1f}/100\n",
    "\n",
    "## Target Variable\n",
    "- **Column**: {target_rec.column_name}\n",
    "- **Type**: {target_rec.target_type}\n",
    "- **Rationale**: {target_rec.rationale}\n",
    "\n",
    "## Column Types\n",
    "| Column | Type | Nulls | Unique |\n",
    "|--------|------|-------|--------|\n",
    "\"\"\"\n",
    "    for name, col in list(findings.columns.items())[:20]:\n",
    "        overview += f\"| {name} | {col.inferred_type.value} | {col.null_percentage:.1f}% | {col.unique_count} |\\n\"\n",
    "    (docs_dir / \"01_overview.md\").write_text(overview)\n",
    "    \n",
    "    # 2. Bronze layer - separate file per source\n",
    "    if registry and registry.sources:\n",
    "        for source_name, bronze_recs in registry.sources.items():\n",
    "            bronze_doc = f\"\"\"# Bronze Layer - {source_name}\n",
    "\n",
    "## Source File\n",
    "`{bronze_recs.source_file}`\n",
    "\n",
    "## Null Handling\n",
    "\"\"\"\n",
    "            for rec in bronze_recs.null_handling:\n",
    "                bronze_doc += f\"- `{rec.target_column}`: {rec.action} ({rec.parameters.get('strategy', '')}) - {rec.rationale}\\n\"\n",
    "            \n",
    "            bronze_doc += \"\\n## Outlier Handling\\n\"\n",
    "            for rec in bronze_recs.outlier_handling:\n",
    "                bronze_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.rationale}\\n\"\n",
    "            \n",
    "            bronze_doc += \"\\n## Type Conversions\\n\"\n",
    "            for rec in bronze_recs.type_conversions:\n",
    "                bronze_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.rationale}\\n\"\n",
    "            \n",
    "            bronze_doc += \"\\n## Deduplication\\n\"\n",
    "            for rec in bronze_recs.deduplication:\n",
    "                bronze_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.rationale}\\n\"\n",
    "            \n",
    "            bronze_doc += \"\\n## Filtering\\n\"\n",
    "            for rec in bronze_recs.filtering:\n",
    "                bronze_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.rationale}\\n\"\n",
    "            \n",
    "            bronze_doc += \"\\n## Text Processing\\n\"\n",
    "            for rec in bronze_recs.text_processing:\n",
    "                bronze_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.rationale}\\n\"\n",
    "            \n",
    "            safe_name = source_name.replace(\" \", \"_\").lower()\n",
    "            (docs_dir / f\"02_bronze_cleaning_{safe_name}.md\").write_text(bronze_doc)\n",
    "    else:\n",
    "        bronze_doc = f\"\"\"# Bronze Layer - Data Cleaning\n",
    "\n",
    "## Cleaning Recommendations\n",
    "\"\"\"\n",
    "        for rec in cleaning_recs:\n",
    "            bronze_doc += f\"\\n### {rec.column_name}\\n- **Strategy**: {rec.strategy}\\n- **Severity**: {rec.severity}\\n- **Rationale**: {rec.rationale}\\n\"\n",
    "        (docs_dir / \"02_bronze_cleaning.md\").write_text(bronze_doc)\n",
    "    \n",
    "    # 3. Silver layer\n",
    "    silver_doc = \"\"\"# Silver Layer - Feature Engineering\n",
    "\n",
    "## Aggregations and Joins\n",
    "\"\"\"\n",
    "    if registry and registry.silver:\n",
    "        silver_doc += \"\\n### Joins\\n\"\n",
    "        for rec in registry.silver.joins:\n",
    "            silver_doc += f\"- {rec.parameters.get('left_source', '')} \u27f7 {rec.parameters.get('right_source', '')} on `{rec.parameters.get('join_keys', [])}`\\n\"\n",
    "        \n",
    "        silver_doc += \"\\n### Aggregations\\n\"\n",
    "        for rec in registry.silver.aggregations:\n",
    "            silver_doc += f\"- `{rec.target_column}`: {rec.action} - windows: {rec.parameters.get('windows', [])}\\n\"\n",
    "        \n",
    "        silver_doc += \"\\n### Derived Columns\\n\"\n",
    "        for rec in registry.silver.derived_columns:\n",
    "            silver_doc += f\"- `{rec.target_column}`: {rec.parameters.get('expression', rec.action)}\\n\"\n",
    "    else:\n",
    "        silver_doc += \"\\nNo silver-layer recommendations found.\\n\"\n",
    "    (docs_dir / \"03_silver_features.md\").write_text(silver_doc)\n",
    "    \n",
    "    # 4. Gold layer\n",
    "    gold_doc = \"\"\"# Gold Layer - ML Features\n",
    "\n",
    "## Feature Recommendations\n",
    "\"\"\"\n",
    "    for rec in feature_recs[:15]:\n",
    "        gold_doc += f\"\\n### {rec.feature_name}\\n- **Source**: {rec.source_column}\\n- **Type**: {rec.feature_type}\\n- **Description**: {rec.description}\\n\"\n",
    "    \n",
    "    if registry and registry.gold:\n",
    "        gold_doc += \"\\n## Encoding\\n\"\n",
    "        for rec in registry.gold.encoding:\n",
    "            gold_doc += f\"- `{rec.target_column}`: {rec.parameters.get('method', rec.action)}\\n\"\n",
    "        \n",
    "        gold_doc += \"\\n## Scaling\\n\"\n",
    "        for rec in registry.gold.scaling:\n",
    "            gold_doc += f\"- `{rec.target_column}`: {rec.parameters.get('method', rec.action)}\\n\"\n",
    "        \n",
    "        gold_doc += \"\\n## Feature Selection\\n\"\n",
    "        for rec in registry.gold.feature_selection:\n",
    "            gold_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.rationale}\\n\"\n",
    "        \n",
    "        gold_doc += \"\\n## Transformations\\n\"\n",
    "        for rec in registry.gold.transformations:\n",
    "            gold_doc += f\"- `{rec.target_column}`: {rec.action} - {rec.parameters}\\n\"\n",
    "    (docs_dir / \"04_gold_ml_features.md\").write_text(gold_doc)\n",
    "    \n",
    "    # 5. Training\n",
    "    training_doc = f\"\"\"# Model Training\n",
    "\n",
    "## Target\n",
    "- **Column**: {target_rec.column_name}\n",
    "- **Type**: {target_rec.target_type}\n",
    "\n",
    "## Recommended Models\n",
    "1. **Gradient Boosting** - Good for tabular data with mixed types\n",
    "2. **Random Forest** - Robust baseline, handles missing values\n",
    "3. **Logistic Regression** - Interpretable, good for imbalanced data\n",
    "\n",
    "## Evaluation Metrics\n",
    "- ROC-AUC (primary)\n",
    "- Precision/Recall at threshold\n",
    "- F1 Score\n",
    "\"\"\"\n",
    "    (docs_dir / \"05_training.md\").write_text(training_doc)\n",
    "    \n",
    "    print(\"Generated LLM documentation:\")\n",
    "    for f in sorted(docs_dir.glob(\"*.md\")):\n",
    "        print(f\"  {f.name}\")\n",
    "else:\n",
    "    print(f\"Skipping LLM docs generation (target is {GENERATION_TARGET.value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.5 Convert to Notebooks (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def py_to_notebook(py_path: Path):\n",
    "    content = py_path.read_text()\n",
    "    cells = []\n",
    "    current_lines = []\n",
    "    \n",
    "    for line in content.split(\"\\n\"):\n",
    "        if line.startswith(\"# %% \") or line.startswith(\"# %%\\n\"):\n",
    "            if current_lines:\n",
    "                cells.append({\"cell_type\": \"code\", \"metadata\": {}, \"source\": current_lines, \"outputs\": [], \"execution_count\": None})\n",
    "                current_lines = []\n",
    "            title = line.replace(\"# %% \", \"\").strip()\n",
    "            if title:\n",
    "                cells.append({\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [f\"## {title}\"]})\n",
    "        else:\n",
    "            current_lines.append(line + \"\\n\")\n",
    "    \n",
    "    if current_lines:\n",
    "        cells.append({\"cell_type\": \"code\", \"metadata\": {}, \"source\": current_lines, \"outputs\": [], \"execution_count\": None})\n",
    "    \n",
    "    notebook = {\n",
    "        \"cells\": cells,\n",
    "        \"metadata\": {\"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"}},\n",
    "        \"nbformat\": 4, \"nbformat_minor\": 4\n",
    "    }\n",
    "    \n",
    "    out_path = py_path.with_suffix(\".ipynb\")\n",
    "    out_path.write_text(json.dumps(notebook, indent=1))\n",
    "    return out_path\n",
    "\n",
    "if OUTPUT_FORMAT == OutputFormat.NOTEBOOK:\n",
    "    print(\"Converting Python files to notebooks...\")\n",
    "    for py_file in output_dir.rglob(\"*.py\"):\n",
    "        if py_file.name != \"__init__.py\":\n",
    "            nb_path = py_to_notebook(py_file)\n",
    "            print(f\"  {py_file.name} -> {nb_path.name}\")\n",
    "else:\n",
    "    print(\"Output format is Python. Set OUTPUT_FORMAT = OutputFormat.NOTEBOOK to convert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10.6 Run Pipeline\n",
    "\n",
    "Single command runs everything: Bronze (parallel) \u2192 Silver \u2192 Gold \u2192 Training \u2192 MLflow UI (auto-opens browser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Uncomment below to run the pipeline after generation\n# RUN_PIPELINE = True\n\nRUN_PIPELINE = True\n\nrun_all_path = output_dir / \"run_all.py\"\n\nif RUN_PIPELINE and GENERATION_TARGET == GenerationTarget.LOCAL_FEAST_MLFLOW:\n    import subprocess\n    if run_all_path.exists():\n        print(f\"Running: python {run_all_path}\")\n        print(\"Pipeline will run and MLflow UI will open automatically...\")\n        subprocess.run([\"python\", \"run_all.py\"], cwd=str(run_all_path.parent.resolve()))\n    else:\n        print(f\"run_all.py not found. Generate first by running cells above.\")\nelse:\n    print(\"To run the complete pipeline:\")\n    print(f\"\\n  cd {output_dir}\")\n    print(f\"  python run_all.py\")\n    print(f\"\\nThis will:\")\n    print(\"  1. Run Bronze layers (parallel)\")\n    print(\"  2. Run Silver merge\")\n    print(\"  3. Run Gold features\")\n    print(\"  4. Train models with MLflow\")\n    print(\"  5. Auto-start MLflow UI and open browser\")\n    print(\"  6. Press Ctrl+C to stop MLflow UI when done\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10.7 Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Artifacts Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pipeline: {PIPELINE_NAME}\")\n",
    "print(f\"Target: {GENERATION_TARGET.value}\")\n",
    "print(f\"Format: {OUTPUT_FORMAT.value}\")\n",
    "print(f\"Output: {output_dir}\")\n",
    "print()\n",
    "\n",
    "def show_tree(path: Path, prefix: str = \"\"):\n",
    "    items = sorted(path.iterdir(), key=lambda p: (p.is_file(), p.name))\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        connector = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n",
    "        if item.is_file():\n",
    "            size = item.stat().st_size\n",
    "            print(f\"{prefix}{connector}{item.name} ({size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"{prefix}{connector}{item.name}/\")\n",
    "            show_tree(item, prefix + (\"    \" if is_last else \"\u2502   \"))\n",
    "\n",
    "if output_dir.exists():\n",
    "    show_tree(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10.8 Recommendations Hash\n\nThe recommendations hash is a unique identifier for the gold layer feature engineering configuration. It enables experiment tracking and reproducibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if registry:\n",
    "    recommendations_hash = registry.compute_recommendations_hash()\n",
    "    print(\"Recommendations Hash\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Hash: {recommendations_hash}\")\n",
    "    print(f\"Full version tag: v1.0.0_{recommendations_hash}\")\n",
    "    print()\n",
    "    print(\"This hash uniquely identifies the gold layer configuration:\")\n",
    "    print(f\"  - Encodings: {len(registry.gold.encoding) if registry.gold else 0}\")\n",
    "    print(f\"  - Scalings: {len(registry.gold.scaling) if registry.gold else 0}\")\n",
    "    print(f\"  - Transformations: {len(registry.gold.transformations) if registry.gold else 0}\")\n",
    "    print(f\"  - Feature selections: {len(registry.gold.feature_selection) if registry.gold else 0}\")\n",
    "    \n",
    "    # Show what's in each layer for debugging\n",
    "    print()\n",
    "    print(\"Recommendations by layer:\")\n",
    "    for layer in [\"bronze\", \"silver\", \"gold\"]:\n",
    "        recs = registry.get_by_layer(layer)\n",
    "        print(f\"  {layer.upper()}: {len(recs)} recommendations\")\n",
    "        if recs and layer == \"gold\":\n",
    "            for rec in recs[:3]:\n",
    "                print(f\"    - [{rec.category}] {rec.target_column}: {rec.action}\")\n",
    "            if len(recs) > 3:\n",
    "                print(f\"    ... and {len(recs) - 3} more\")\n",
    "    \n",
    "    # Check if gold layer exists but is empty\n",
    "    if registry.gold:\n",
    "        print(f\"\\n\u2713 Gold layer initialized (target: {registry.gold.target_column})\")\n",
    "    else:\n",
    "        print(\"\\n\u26a0 Gold layer not initialized - run step 06 first\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Use this hash to:\")\n",
    "    print(\"  - Track MLflow experiments (tag: recommendations_hash)\")\n",
    "    print(\"  - Version Feast feature views (tag in feature_store)\")\n",
    "    print(\"  - Return to a specific feature engineering configuration\")\n",
    "else:\n",
    "    print(\"No recommendations loaded - hash not available\")\n",
    "    print(\"Run notebooks 02-07 first, then re-run this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10.9 Feast Feature Store Validation\n\nCheck what's registered in Feast after running the pipeline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Feast Feature Store contents\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"feast\")\n",
    "\n",
    "feast_repo_path = output_dir / \"feature_repo\"\n",
    "\n",
    "if feast_repo_path.exists() and (feast_repo_path / \"feature_store.yaml\").exists():\n",
    "    try:\n",
    "        from feast import FeatureStore\n",
    "        store = FeatureStore(repo_path=str(feast_repo_path))\n",
    "        \n",
    "        print(\"Feast Feature Store Contents\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # List entities\n",
    "        entities = store.list_entities()\n",
    "        feature_views = store.list_feature_views()\n",
    "        data_sources = store.list_data_sources()\n",
    "        \n",
    "        # Check if registry is empty (feast apply not run yet)\n",
    "        if not entities and not feature_views:\n",
    "            print(\"\\n\u26a0\ufe0f  Feature store registry is empty.\")\n",
    "            print(\"   The feature definitions exist but haven't been applied yet.\")\n",
    "            print(\"\\n   To register features, run:\")\n",
    "            print(f\"     cd {feast_repo_path}\")\n",
    "            print(\"     feast apply\")\n",
    "            print(\"\\n   Or run the full pipeline:\")\n",
    "            print(f\"     cd {output_dir}\")\n",
    "            print(\"     python run_all.py\")\n",
    "        else:\n",
    "            print(f\"\\n\ud83d\udce6 Entities ({len(entities)}):\")\n",
    "            for entity in entities:\n",
    "                print(f\"   - {entity.name} (join_key: {entity.join_keys})\")\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcca Feature Views ({len(feature_views)}):\")\n",
    "            for fv in feature_views:\n",
    "                print(f\"   - {fv.name}: {len(fv.features)} features\")\n",
    "                for feat in fv.features[:5]:\n",
    "                    print(f\"      \u2022 {feat.name} ({feat.dtype})\")\n",
    "                if len(fv.features) > 5:\n",
    "                    print(f\"      ... and {len(fv.features) - 5} more\")\n",
    "            \n",
    "            print(f\"\\n\ud83d\udcbe Data Sources ({len(data_sources)}):\")\n",
    "            for ds in data_sources:\n",
    "                print(f\"   - {ds.name}\")\n",
    "        \n",
    "        # Try to show sample data from parquet files\n",
    "        print(f\"\\n\ud83d\udcc4 Sample Feature Data:\")\n",
    "        data_dir = feast_repo_path / \"data\"\n",
    "        if data_dir.exists():\n",
    "            parquet_files = list(data_dir.glob(\"*.parquet\"))\n",
    "            if parquet_files:\n",
    "                sample_df = pd.read_parquet(parquet_files[0])\n",
    "                print(f\"   Source: {parquet_files[0].name}\")\n",
    "                print(f\"   Shape: {sample_df.shape[0]:,} rows x {sample_df.shape[1]} columns\")\n",
    "                print(f\"\\n   Head (first 5 rows):\")\n",
    "                display(sample_df.head())\n",
    "            else:\n",
    "                print(\"   No parquet files found yet in data/ directory.\")\n",
    "                print(\"   Features will be materialized when you run the pipeline.\")\n",
    "        else:\n",
    "            print(\"   Data directory not created yet.\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"Feast not installed. Install with: pip install feast\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not connect to Feast: {e}\")\n",
    "        print(\"\\nTo manually inspect, run:\")\n",
    "        print(f\"  cd {feast_repo_path}\")\n",
    "        print(\"  feast apply\")\n",
    "        print(\"  feast feature-views list\")\n",
    "else:\n",
    "    print(f\"Feature repo not found at: {feast_repo_path}\")\n",
    "    print(\"Generate the pipeline first by running cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 10.10 Next Steps\n\n### Run Pipeline (Single Command)\n```bash\ncd ../generated_pipelines/local/customer_churn\npython run_all.py\n```\n\nThis single command:\n1. Runs Bronze layers in **parallel**\n2. Runs Silver merge\n3. Runs Gold features  \n4. Trains models with MLflow tracking\n5. **Auto-starts MLflow UI** and opens browser\n6. Press `Ctrl+C` to stop when done\n\n### Generated Structure\n```\ngenerated_pipelines/local/{pipeline}/\n\u251c\u2500\u2500 run_all.py          # Single entry point\n\u251c\u2500\u2500 config.py           # Configuration (includes RECOMMENDATIONS_HASH)\n\u251c\u2500\u2500 bronze/\n\u2502   \u2514\u2500\u2500 bronze_*.py     # Parallel execution\n\u251c\u2500\u2500 silver/\n\u2502   \u2514\u2500\u2500 silver_merge.py\n\u251c\u2500\u2500 gold/\n\u2502   \u2514\u2500\u2500 gold_features.py  # Includes feature version tag\n\u251c\u2500\u2500 training/\n\u2502   \u2514\u2500\u2500 ml_experiment.py  # MLflow tags with recommendations_hash\n\u251c\u2500\u2500 pipeline.py         # Standalone pipeline script\n\u2514\u2500\u2500 requirements.txt\n```\n\n### Tracking Your Experiment\nAfter running, you can find your experiment by:\n- **MLflow UI**: Filter by tag `recommendations_hash = <your_hash>`\n- **Feast**: Check feature view tags for `recommendations_hash`\n- **Return to config**: The hash uniquely identifies the gold layer settings\n\n---\n\n## Complete!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}