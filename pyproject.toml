[project]
name = "customer-retention"
version = "1.0.0"
description = "Production-ready ML framework for customer churn prediction and retention optimization"
readme = "README.md"
license = {text = "Apache-2.0"}
authors = [
    {name = "Customer Retention Contributors"}
]
keywords = ["machine-learning", "churn-prediction", "customer-retention", "mlops", "databricks"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: Apache Software License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
requires-python = ">=3.10"
dependencies = [
    "pydantic>=2.0.0",
    "pandas>=2.0.0",
    "pyarrow>=12.0.0",
    "fsspec>=2023.0.0",
    "s3fs>=2023.0.0",
    "scipy>=1.10.0",
    "statsmodels>=0.14.0",
    # Visualization (for exploration notebooks)
    "matplotlib>=3.7.0",
    "seaborn>=0.12.0",
    "plotly>=5.15.0",
    # Rich display
    "rich>=13.0.0",
    "tabulate>=0.9.0",
    # YAML for exploration result serialization
    "pyyaml>=6.0.0",
    # Orchestration
    "papermill>=2.4.0",
    # Static image export for Plotly
    "kaleido>=0.2.1",
    "ipykernel>=7.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-timeout>=2.2.0",
    "ruff>=0.1.0",
    "nbconvert>=7.0.0",
    "jinja2>=3.0.0",
    "pre-commit>=3.7.0",
]

# ML dependencies for local/server environments (not needed on Databricks ML runtime)
ml = [
    "scikit-learn>=1.3.0",
    "xgboost>=2.0.0",
    "lightgbm>=4.0.0",
    "mlflow>=2.10.0",
    "imbalanced-learn>=0.12.0",
    "feast>=0.40.0",
]

# SHAP for model interpretation (macOS ARM64 and Linux only)
# - numba 0.63+ dropped Intel Mac (x86_64) support
# - Pre-built wheels available for: macOS ARM64, Linux x86_64, Linux ARM64
# - On Databricks ML runtime: shap is pre-installed (no installation needed)
# - Intel Mac users: use conda or build from source
ml-shap = [
    "customer-retention[ml]",
    "numba>=0.63.0",
    "llvmlite>=0.46.0",
    "shap>=0.50.0",
]

# SHAP for Intel Macs (x86_64) - uses older numba with pre-built wheels
# Use this if you're on an Intel Mac or x86_64 Python through Rosetta
ml-shap-intel = [
    "customer-retention[ml]",
    "numba>=0.59.0,<0.63.0",
    "llvmlite>=0.43.0,<0.46.0",
    "shap>=0.44.0,<0.50.0",
]

# PyTorch CPU - for Mac/Linux without GPU
ml-cpu = [
    "customer-retention[ml]",
    "torch>=2.2.0",
    "torchvision>=0.17.0",
]

# PyTorch CUDA 12.8 - for Linux servers with NVIDIA GPU
ml-cuda = [
    "customer-retention[ml]",
    "torch>=2.2.0",
    "torchvision>=0.17.0",
]

# Text processing - for embedding and dimensionality reduction of TEXT columns
# Requires >=2.7.0 for full Qwen3 model support
text = [
    "sentence-transformers>=2.7.0",
]

# All optional dependencies (without SHAP - smaller footprint)
all = [
    "customer-retention[ml,text]",
]

# Full ML with SHAP (pre-built wheels available for macOS ARM64 and Linux)
all-shap = [
    "customer-retention[ml-shap,text]",
]

[project.scripts]
# CLI command to bootstrap new projects
customer-retention-init = "customer_retention.cli:init_project"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/customer_retention"]

[tool.hatch.build.targets.wheel.shared-data]
"exploration_notebooks" = "share/customer-retention/exploration_notebooks"

[tool.hatch.build.targets.sdist]
include = [
    "src/",
    "exploration_notebooks/",
    "scripts/",
    "constraints/",
]

[tool.ruff]
line-length = 120
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W"]
ignore = [
    "E501",   # Line too long (handled by formatter)
    "N803",   # Argument name should be lowercase (ML convention: X, X_train)
    "N806",   # Variable name should be lowercase (ML convention: X, X_train)
    "N817",   # CamelCase imported as acronym (CT, FE aliases)
    "E402",   # Module level import not at top (needed for conditional imports)
    "E712",   # Comparison to True/False (style preference)
    "E721",   # Type comparison using == (sometimes intentional)
]

[tool.ruff.lint.per-file-ignores]
# Tests often have unused variables for setup/assertions
"tests/**/*.py" = ["F841", "N801"]
# Availability check imports are intentional
"src/customer_retention/core/compat/*.py" = ["F401"]
"tests/integrations/adapters/*.py" = ["F401"]
"tests/core/compat/*.py" = ["F401"]
# Storage adapters check for optional dependencies
"src/customer_retention/integrations/adapters/storage/*.py" = ["F401"]

[tool.uv]
# PyTorch CUDA index - used when installing ml-cuda extras
[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

# Use CUDA wheels for torch when installing ml-cuda
[tool.uv.sources]
torch = [
    { index = "pytorch-cu128", extra = "ml-cuda" },
]
torchvision = [
    { index = "pytorch-cu128", extra = "ml-cuda" },
]

# Packages managed by Databricks Runtime ML - do not override
# These will be read by helper scripts to generate constraints
# no-install:
#   - pyspark, delta-spark, databricks-sdk, py4j (Core Spark/Databricks)
#   - mlflow, mlflow-skinny (MLflow)
#   - numpy, pandas, pyarrow, scipy (Data processing)
#   - scikit-learn, xgboost, lightgbm, catboost, hyperopt, shap (ML libraries)
#   - torch, torchvision, tensorflow (Deep learning)
#   - matplotlib, seaborn, plotly (Visualization)

[tool.databricks]
# Target Databricks Runtime version (ML variant includes scikit-learn, xgboost, etc.)
runtime = "17.3.x-cpu-ml-scala2.12"
# Path where constraints file will be stored/read (relative to project root)
constraints-path = "constraints/dbr-17.3-ml-lts.txt"
